[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Vertikal Willis",
    "section": "",
    "text": "Hello! Thanks for visiting my website. I’m Vertikal Willis, 25 years old, and single. I’m from Medan and now living in Jakarta. I’ve got total 5 years of work experience (I’ve been working since my first semester of college), mostly as a buyer/procurement professional. I decided to switch careers to data science because I feel it suits me better. Processing complex data in creative ways to find strong insights and solutions gives me real joy. I think a data scientist is like an artist, be able to make something out of nothing by using data – it’s an art form!\nTo see my complete CV please email to vertikalwillis@gmail.com or contact me through LinkedIn!\n\nFind me\n\n\n\n\n\n Email\n\n\n\n\n\n\n LinkedIn\n\n\n\n\n\n\n GitHub\n\n\n\n\n\n\n Instagram\n\n\n\n\n\n\nEducation\n\n\n\nDibimbing DS Bootcamp \n\n\n2023\n\n\nDS Batch 19\n\n\nMost valuable person\n2nd best of final project\n3 x student of the month\n\n\n\n\nUniversitas Prima Indonesia \n\n\n2016-2020\n\n\nBachelor’s degree in Industrial Engineering \n\n\nGPA : 3.68 / 4\n\n\n\n\nCertificates\n\nDibimbing DS  MVP  Report Card  Completion \nHackerRank SQL  Advanced SQL queries \n\n\nSkills\n\n\n\n\n\nPython\n\n\n\n\n82%\n\n\n\n\n\n\n\n\nSQL\n\n\n\n\n78%\n\n\n\n\n\n\n\n\n\n\nPower BI (DAX)\n\n\n\n\n79%\n\n\n\n\n\n\n\n\nExcel VBA (Macro)\n\n\n\n\n73%\n\n\n\n\n\n\n\n\n\n\nTableau\n\n\n\n\n75%\n\n\n\n\n\n\n\n\nGoogling & ChatGPT\n\n\n\n\n83%\n\n\n\n\n\n\n\n\nTools\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrior work experiences\n\n\n\nPalm oil industry company (JAKARTA)\n\n\n2023\n\n\nPurchasing officer\n\n\nResearch potential vendors.\nCompare and evaluate offers from vendors.\nNegotiate contract of terms of agreement and pricing.\nTrack orders and ensure timely delivery.\nReview quality of purchased products.\nMaintain updated records of purchased, products, delivery information and invoice.\nPrepare reports on purchases.\n\n\n\n\n\nCoconut industry company (STABAT)\n\n\n2022\n\n\nMT Production\n\n\nLearn the process of making desiccated coconut from raw coconut.\nLearn about the production machinery (technical and maintain).\nEnsure employee to work as Standard Operating Procedure (SOP).\nDo continuous improvement and brainstorm to increase machine efficiency and effectiveness.\n\n\n\n\n\nCrusher trading company (MEDAN)\n\n\n2020-2022\n\n\nPurchasing Officer\n\n\nResearch potential vendors.\nCompare and evaluate offers from vendors.\nNegotiate contract of terms of agreement and pricing.\nTrack orders and ensure timely delivery.\nReview quality of purchased products.\nEnter order details (e.g. Vendor, prices) into internal database.\nMaintain updated records of purchased, products, delivery information and invoice.\nPrepare reports on purchases.\nMonitor stock levels and place orders as needed.\nCoordinate with warehouse staff to ensure proper storage.\nCoordinate with warehouse about delivery goods to customers.\nPrepare and check paper work for logistic/shipment.\n\n\n\n\n\nTransportation company (MEDAN)\n\n\n2018-2020\n\n\nWorkshop Analyst\n\n\nMonitor and follow up on breakdown vehicles to user (branch workshop supervisor).\nAnalyze mechanic’s daily job by report that sent by user every day.\nAnalyze and approve vehicle’s spare parts request.\nAnalyze and approve vehicle’s repair request.\nCoordinate and follow up with the purchasing department about outstanding spare parts purchase requests.\nCoordinate and negotiate with external workshops for heavy repair work that can’t be done in the internal workshop.\nAnalyze vehicles for an unreasonable breakdown.\nAnalyze and evaluate the mechanic’s monthly performance based on the user’s and the mechanic’s daily reports.\nPrepare a daily, weekly, and monthly report.\nHelped supervisor make annual budgets.\n\n\n\n\n\nTransportation company (MEDAN)\n\n\n2017-2018\n\n\nPurchasing Officer\n\n\nResearch potential vendors.\nCompare and evaluate offers from vendors.\nNegotiate contract of terms of agreement and pricing.\nTrack orders and ensure timely delivery.\nReview quality of purchased products.\nEnter order details (e.g. Vendor, prices) into internal database.\nMaintain updated records of purchased, products, delivery information and invoice.\nPrepare reports on purchases.\nMonitor stock levels and place orders as needed.\nCoordinate with warehouse staff to ensure proper storage."
  },
  {
    "objectID": "portfolios/airflow_docker/index.html",
    "href": "portfolios/airflow_docker/index.html",
    "title": "Building a data automation pipeline with Apache Airflow",
    "section": "",
    "text": "I’ve come to realize that a data scientist’s role includes building data pipelines, monitoring machine learning models, and creating/maintaining visualization dashboards. In this mini-project, I’ve implemented a comprehensive end-to-end data automation using Apache Airflow, encompassing data scraping, model monitoring, and updating power BI visualization dashboard."
  },
  {
    "objectID": "portfolios/airflow_docker/index.html#overview",
    "href": "portfolios/airflow_docker/index.html#overview",
    "title": "Building a data automation pipeline with Apache Airflow",
    "section": "Overview",
    "text": "Overview\nTake a look at diagram below.\n \nThis DAG consists of a total of eight tasks. It starts with fresh housing data being scraped as input, which is then fed into a ML model (predicting houses prices) . The model performance will be evaluated on the new data using Evidently.ai. Additionally, the newly scraped data is also updated to a Google Sheet-based Power BI data source, updating the Power BI visualization dashboard.\nIn summary: Input : New data (scrapped data) Output1 : updated model monitoring result. Output2 : updated power BI online dashboard.\nThe data is about houses information in Tangerang City, with more than 40 sub-districts.\nTo prepare for this project, there are some requirements:  WSL2 (Windows sub-system for linux) Given that I’m using Windows 11, I’ll use WSL2. Airflow with Docker Airflow needs to be installed within a Docker environment to create an isolated environment for workflow orchestration. Power BI Pro Account & Microsoft Azure Account These accounts are essential for utilizing the Power BI REST API, which is necessary to refresh the Power BI visualization dashboard. ML Model Prior to commencing the project, I’ve conducted data exploration and experimented with various ML models to ensure the best model is ready. Scraping Application In this project, I’ll be using Scrapy for web scraping tasks."
  },
  {
    "objectID": "portfolios/airflow_docker/index.html#task-1-2-3",
    "href": "portfolios/airflow_docker/index.html#task-1-2-3",
    "title": "Building a data automation pipeline with Apache Airflow",
    "section": "Task 1, 2, & 3",
    "text": "Task 1, 2, & 3\nBelow is the diagram of task 1, 2 & 3 \n \nFlow of task 1:\n\nEstablish connection to host (WSL2) from docker container via SSH Connection.\nExecute shell script to run scrappy.\nThe scrapped file will be in csv format and uploaded to shared folder.\n\nFlow of task 2:\n\nGet the scrapped file from shared folder.\nTransform or change the data dimension into proper format.\nUpload the new transformed file into the shared folder.\n\nFlow of task 3:\n\nConnect to PostgreSQL database.\nGet the transformed data from shared folder.\nUpload it into PostgreSQL database."
  },
  {
    "objectID": "portfolios/airflow_docker/index.html#task-4-5-6",
    "href": "portfolios/airflow_docker/index.html#task-4-5-6",
    "title": "Building a data automation pipeline with Apache Airflow",
    "section": "Task 4, 5, & 6",
    "text": "Task 4, 5, & 6\nBelow is the diagram of task 4, 5 & 6 \n \nFlow of task 4:\n\nGet data from database.\nClean the data for the purpose of data visualization.\nUpload it to shared folder.\n\nFlow of task 5:\n\nConnect to host machine via SSH connection.\nExecute python script to get the cleaned data from shared folder and upload it to google sheet via google spreadsheet API.\n\nFlow of task 6:\n\nConnect to power BI via power BI API.\nRefresh the power bi visualization dashboard.\nPower BI will detect changes in dataset (google sheet) which already updated on task 5.\nPower BI will update the visualization based on the new dataset.\n\nBelow is the Power BI visualization dashboard :"
  },
  {
    "objectID": "portfolios/airflow_docker/index.html#task-4a-5a",
    "href": "portfolios/airflow_docker/index.html#task-4a-5a",
    "title": "Building a data automation pipeline with Apache Airflow",
    "section": "Task 4a & 5a",
    "text": "Task 4a & 5a\nBelow is the diagram of task 4a & 5a \n \nFlow of task 4a:\n\nGet data from database.\nClean the data for the purpose of Machine Learning.\nUpload it to shared folder.\n\nFlow of task 5a:\n\nConnect to host machine via SSH connection.\nExecute python script to get the cleaned data from shared folder and run Evidently.ai to generate model report.\nGenerate the report as html file.\n\nBelow is the preview of the report:"
  },
  {
    "objectID": "portfolios/airflow_docker/index.html#demonstration",
    "href": "portfolios/airflow_docker/index.html#demonstration",
    "title": "Building a data automation pipeline with Apache Airflow",
    "section": "Demonstration",
    "text": "Demonstration\nOn this video below, I’m going to demonstrate how to run the Airflow DAG and illustrate the expected results."
  },
  {
    "objectID": "portfolios/houses_tangerang/index.html",
    "href": "portfolios/houses_tangerang/index.html",
    "title": "Tangerang house price prediction",
    "section": "",
    "text": "This is my first end-to-end project with a scraped dataset. I used Scrapy as my scraping tool to collect data from one of the largest house listing websites in Indonesia. The most challenging part of this project was cleaning the data, as it had many missing values that required careful imputation.\nOne of the most important lessons I learned in this project is the significance of sample selection. When training a machine learning model, we typically split the data into a training and test set. If the model performs well on both sets, we may consider it to be a good model, which could be true. However, when we apply the model to new test data from different samples, the results may not meet our expectations. This issue doesn’t stem from the machine learning algorithm itself but rather from the initial sample selection, which may not accurately represent the population.\nFor scrapy installation and setup for this project please visit :"
  },
  {
    "objectID": "portfolios/houses_tangerang/index.html#table-of-contents",
    "href": "portfolios/houses_tangerang/index.html#table-of-contents",
    "title": "Tangerang house price prediction",
    "section": "Table Of Contents",
    "text": "Table Of Contents\n• Introduction  • Data Preparation • Basic exploratory data analysis      • Descriptive statistic      • Univariate analysis      • Multivariate analysis  • Deep-dive exploratory data analysis  • Modelling     • Model building combination 1      • Model building combination 2      • Model building combination 3      • Model building combination 4      • Model building combination 5      • Model building combination 6  • Choosing the best model combination  • Model evaluation and interpretation      • Residuals plot      • PDP & ICE plots  • Conclusion"
  },
  {
    "objectID": "portfolios/houses_tangerang/index.html#introduction",
    "href": "portfolios/houses_tangerang/index.html#introduction",
    "title": "Tangerang house price prediction",
    "section": "Introduction",
    "text": "Introduction\nThis project revolves around predicting house prices in Tangerang City, which encompasses more than 40 regions. The data for this project was gathered by scraping information from one of Indonesia’s largest online real estate listing platforms. After conducting this exploratory phase, I will deploy an application publicly to assist people in estimating house prices."
  },
  {
    "objectID": "portfolios/houses_tangerang/index.html#data-preparation",
    "href": "portfolios/houses_tangerang/index.html#data-preparation",
    "title": "Tangerang house price prediction",
    "section": "Data Preparation",
    "text": "Data Preparation\nImporting libraries, cleaning data and choosing features.\n\n\nCode\n#import libraries\nimport numpy as np\nimport pandas as pd\nimport ast\nimport re\nfrom sklearn.impute import KNNImputer\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\nimport warnings\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\nfrom sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\nfrom xgboost import XGBRegressor\nimport xgboost as xgb\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\nimport plotly.express as px\nimport matplotlib.patches as mpatches\nfrom pdpbox import pdp\n\n#set some library settings\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_columns', None)\nsns.set()\nsns.set_style('darkgrid')\nmpl.rcParams['figure.dpi'] = 150 \nplt.rc('legend',**{'fontsize':10})\n\n\n\n\nCode\n#read csv\ndf = pd.read_csv('tangerang.csv')\n\n\n\n\nCode\n#first look of data\ndf.head()\n\n\n\n\n\n\n\n\n\nharga\nalamat\nfasilitas\nspesifikasi\n\n\n\n\n0\nRp 2,6 Miliar\nBSD City, Tangerang\nTempat Jemuran,Keamanan 24 jam,Playground,Wast...\n[['Kamar Tidur', '3'], ['Kamar Mandi', '2'], [...\n\n\n1\nRp 20 Miliar\nBSD City, Tangerang\nNaN\n[['Kamar Tidur', '5'], ['Kamar Mandi', '5'], [...\n\n\n2\nRp 1,68 Miliar\nBSD City, Tangerang\nTaman,Tempat Jemuran,Keamanan 24 jam\n[['Kamar Tidur', '2'], ['Kamar Mandi', '2'], [...\n\n\n3\nRp 1,61 Miliar\nBSD City, Tangerang\nTaman,Tempat Jemuran,Keamanan 24 jam\n[['Kamar Tidur', '4'], ['Kamar Mandi', '3'], [...\n\n\n4\nRp 2,14 Miliar\nBSD City, Tangerang\nTaman,Tempat Jemuran,Keamanan 24 jam\n[['Kamar Tidur', '3'], ['Kamar Mandi', '3'], [...\n\n\n\n\n\n\n\n\n\nCode\n#change 'spesifikasi' to type list and explode it to become columns\ndf['spesifikasi'] = df.spesifikasi.apply(ast.literal_eval)\ndf = df.explode('spesifikasi')\n\n\n\n\nCode\n#split into 'keterangan' and 'qty'\ndf['keterangan'] = df.spesifikasi.str[0]\ndf['qty'] = df.spesifikasi.str[1]\n\n\n\n\nCode\ndf_columns = df[['harga','alamat','fasilitas']].reset_index().drop_duplicates('index').set_index('index')\ndf_pivot = df.pivot(columns='keterangan', values='qty').rename_axis(None, axis=1)\ndf = pd.concat([df_columns, df_pivot], axis = 1)\n\n\n\nCode above is about to transform ‘keterangan’ into several columns.\n\n\n\nCode\n#new transformed dataframe\ndf.head()\n\n\n\n\n\n\n\n\n\nharga\nalamat\nfasilitas\nCarport\nDapur\nDaya Listrik\nGarasi\nHadap\nHook\nID Iklan\nJumlah Lantai\nKamar Mandi\nKamar Mandi Pembantu\nKamar Pembantu\nKamar Tidur\nKondisi Perabotan\nKondisi Properti\nKonsep dan Gaya Rumah\nLebar Jalan\nLuas Bangunan\nLuas Tanah\nMaterial Bangunan\nMaterial Lantai\nNomor Lantai\nPemandangan\nPeriode Sewa\nRuang Makan\nRuang Tamu\nSertifikat\nSumber Air\nTahun Di Renovasi\nTahun Dibangun\nTerjangkau Internet\nTipe Properti\n\n\n\n\n0\nRp 2,6 Miliar\nBSD City, Tangerang\nTempat Jemuran,Keamanan 24 jam,Playground,Wast...\n1\nNaN\nNaN\nNaN\nNaN\nTidak\nhos14814642\n1\n2\n2\n1\n3\nNaN\nBagus\nMinimalis Modern\n3 Mobil\n91 m²\n91 m²\nBatako\nGranit\nNaN\nTaman Kota\nNaN\nYa\nYa\nLainnya (PPJB,Girik,Adat,dll)\nPAM atau PDAM\nNaN\nNaN\nYa\nRumah\n\n\n1\nRp 20 Miliar\nBSD City, Tangerang\nNaN\nNaN\n2\n7700 Watt\nNaN\nNaN\nTidak\nhos13101318\n2\n5\n1\n1\n5\nUnfurnished\nBagus\nNaN\nNaN\n465 m²\n396 m²\nNaN\nNaN\nNaN\nNaN\nNaN\nYa\nYa\nLainnya (PPJB,Girik,Adat,dll)\nPAM atau PDAM\nNaN\nNaN\nTidak\nRumah\n\n\n2\nRp 1,68 Miliar\nBSD City, Tangerang\nTaman,Tempat Jemuran,Keamanan 24 jam\n1\n1\n1300 Watt\nNaN\nUtara\nTidak\nhos14850708\n1\n2\nNaN\nNaN\n2\nUnfurnished\nBagus\nNaN\nNaN\n70 m²\n105 m²\nNaN\nNaN\nNaN\nNaN\nNaN\nYa\nYa\nHGB - Hak Guna Bangunan\nPAM atau PDAM\nNaN\nNaN\nTidak\nRumah\n\n\n3\nRp 1,61 Miliar\nBSD City, Tangerang\nTaman,Tempat Jemuran,Keamanan 24 jam\n1\n1\n2200 Watt\nNaN\nTimur Laut\nTidak\nhos14850789\n2\n3\nNaN\nNaN\n4\nUnfurnished\nBagus\nNaN\nNaN\n174 m²\n144 m²\nNaN\nNaN\nNaN\nNaN\nNaN\nYa\nYa\nSHM - Sertifikat Hak Milik\nPAM atau PDAM\nNaN\nNaN\nTidak\nRumah\n\n\n4\nRp 2,14 Miliar\nBSD City, Tangerang\nTaman,Tempat Jemuran,Keamanan 24 jam\n1\n1\n5500 Watt\nNaN\nUtara\nTidak\nhos14003997\n3\n3\nNaN\n1\n3\nUnfurnished\nBagus\nNaN\nNaN\n170 m²\n105 m²\nNaN\nNaN\nNaN\nNaN\nNaN\nYa\nYa\nSHM - Sertifikat Hak Milik\nPAM atau PDAM\nNaN\nNaN\nTidak\nRumah\n\n\n\n\n\n\n\n\n\nCode\n#remove duplicate data / 'iklan' (ads)\ndf = df.drop_duplicates('ID Iklan').reset_index(drop=True)\n\n\n\n\nCode\ndf.insert(3, 'komplek', df[['fasilitas']].fillna('none').apply(lambda z: 'ya' if any([x in z.fasilitas.lower() for x in ['lapangan','gym','jogging','playground','one gate system']]) else 'tidak', axis = 1))\ndf.insert(1, 'Kecamatan', df.alamat.str.split(',').str[0])\ndf.insert(2, 'Kota', df.alamat.str.split(',').str[1])\ndf.drop(columns=['alamat','fasilitas'], inplace=True)\n\n\n\nCreate 3 new columns: 1.’komplek’ = check if the house is in a ‘komplek’ or not. 2.’Kecamatan’ = ‘kecamatan’ of the address. 3.’Kota’ = ‘kota’ of the address.\n\n\n\nCode\n#ensure the selected city is Tangerang\ndf = df[df.Kota == ' Tangerang']\ndf.drop(columns = ['Kota'], inplace=True)\n\n\n\n\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 22667 entries, 0 to 22673\nData columns (total 34 columns):\n #   Column                 Non-Null Count  Dtype \n---  ------                 --------------  ----- \n 0   harga                  22667 non-null  object\n 1   Kecamatan              22667 non-null  object\n 2   komplek                22667 non-null  object\n 3   Carport                14104 non-null  object\n 4   Dapur                  12157 non-null  object\n 5   Daya Listrik           18287 non-null  object\n 6   Garasi                 6450 non-null   object\n 7   Hadap                  8528 non-null   object\n 8   Hook                   18764 non-null  object\n 9   ID Iklan               22667 non-null  object\n 10  Jumlah Lantai          20569 non-null  object\n 11  Kamar Mandi            21871 non-null  object\n 12  Kamar Mandi Pembantu   8945 non-null   object\n 13  Kamar Pembantu         9597 non-null   object\n 14  Kamar Tidur            21864 non-null  object\n 15  Kondisi Perabotan      16139 non-null  object\n 16  Kondisi Properti       19797 non-null  object\n 17  Konsep dan Gaya Rumah  9493 non-null   object\n 18  Lebar Jalan            12045 non-null  object\n 19  Luas Bangunan          22624 non-null  object\n 20  Luas Tanah             22658 non-null  object\n 21  Material Bangunan      7709 non-null   object\n 22  Material Lantai        8380 non-null   object\n 23  Nomor Lantai           0 non-null      object\n 24  Pemandangan            9987 non-null   object\n 25  Periode Sewa           1 non-null      object\n 26  Ruang Makan            11288 non-null  object\n 27  Ruang Tamu             18765 non-null  object\n 28  Sertifikat             22489 non-null  object\n 29  Sumber Air             13236 non-null  object\n 30  Tahun Di Renovasi      2856 non-null   object\n 31  Tahun Dibangun         8878 non-null   object\n 32  Terjangkau Internet    18761 non-null  object\n 33  Tipe Properti          22667 non-null  object\ndtypes: object(34)\nmemory usage: 6.1+ MB\n\n\n\n\nThere are 29 missing columns. After carefully reviewing each house one by one in the context, I categorized the missing columns into: \n\n\n1.Missing Completely At Random This means that the values are not input by the users due to accidents, forgetfulness, or oversight. These columns are:  Daya Listrik, Kamar Mandi, Kamar Tidur, Luas Bangunan, Luas Tanah, Sertifikat, Sumber Air, Hook\n\n\n2.Missing At Random This means that the values are not input by the users because these values can be obtained from other sources, such as photos or other information. These columns are:  Carport, Dapur, Garasi, Kamar Mandi Pembantu, Kamar Pembantu, Kondisi Perabotan, Kondisi Properti, Jumlah Lantai, Ruang Makan, Ruang Tamu\n\n\n3.Missing Not At Random This means that the values are not input by the users because they either do not know the value or they perceive it as unimportant to input the value. These columns are:  Konsep dan Gaya Rumah, Hadap, Lebar Jalan, Material Bangunan, Material Lantai, Nomor Lantai, Pemandangan, Periode Sewa, Tahun Di Renovasi, Tahun Dibangun, Terjangkau Internet ***\n\n\n\nCode\n#create a function to categorize multiple 'perabot' conditions into more general category\ndef perabot(kondisi):\n    kondisi = str(kondisi)\n    if kondisi.lower() in ['unfurnished','butuh renovasi']:\n        return 'Unfurnished'\n    elif kondisi.lower() in ['furnished','bagus','bagus sekali','baru','sudah renovasi','semi furnished']:\n        return 'Furnished'\n    else:\n        return np.nan\n\ndf['Kondisi Perabotan'] = df[['Kondisi Perabotan']].apply(lambda x: perabot(x['Kondisi Perabotan']), axis = 1)\n\n\n\n\nCode\n#filtered only houses and not other property\ndf = df[df['Tipe Properti'] == 'Rumah']\n\n\n\n\nCode\n#extract prices from column 'harga' into integer\ndef price_extract(price):\n    if \"Triliun\" in price:\n        numbers = re.findall(r'\\d+\\.\\d+|\\d+', price)\n        numbers = float(\".\".join(numbers))\n        return int(numbers * 1000000)\n    elif \"Miliar\" in price:\n        numbers = re.findall(r'\\d+\\.\\d+|\\d+', price)\n        numbers = float(\".\".join(numbers))\n        return int(numbers * 1000)\n    elif \"Juta\" in price:\n        numbers = re.findall(r'\\d+\\.\\d+|\\d+', price)\n        numbers = float(\".\".join(numbers))\n        return int(numbers * 1)      \n\ndf.insert(1, 'price',  df[['harga']].apply(lambda x: price_extract(x.harga), axis = 1))\ndf.drop(columns = ['harga'], inplace = True)\n\n\n\nHere, I converted the ‘price’ column into integers and divided it by one million to make it easier to view and analyze.\n\n\n\nCode\n#move 'ID Iklan' column to left\ndf.insert(3, 'ID',  df['ID Iklan'])\n\n\n\n\nCode\n#filter Sertifikat to SHM only\ndf = df[df.Sertifikat == 'SHM - Sertifikat Hak Milik']\n\n\n\n\nCode\n#drop useless columns\ndf.drop(columns=['Kondisi Properti','Sertifikat','Hook','ID Iklan','Lebar Jalan','Dapur','Tipe Properti','Terjangkau Internet','Sumber Air','Ruang Tamu','Ruang Makan','Carport','Garasi','Tahun Di Renovasi','Tahun Dibangun','Hadap','Konsep dan Gaya Rumah','Material Bangunan','Material Lantai','Nomor Lantai','Pemandangan','Periode Sewa'], inplace = True, errors='ignore')\n\n\n\n\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 17746 entries, 3 to 22673\nData columns (total 13 columns):\n #   Column                Non-Null Count  Dtype \n---  ------                --------------  ----- \n 0   price                 17746 non-null  int64 \n 1   Kecamatan             17746 non-null  object\n 2   komplek               17746 non-null  object\n 3   ID                    17746 non-null  object\n 4   Daya Listrik          14640 non-null  object\n 5   Jumlah Lantai         16312 non-null  object\n 6   Kamar Mandi           17110 non-null  object\n 7   Kamar Mandi Pembantu  6714 non-null   object\n 8   Kamar Pembantu        7296 non-null   object\n 9   Kamar Tidur           17103 non-null  object\n 10  Kondisi Perabotan     12942 non-null  object\n 11  Luas Bangunan         17719 non-null  object\n 12  Luas Tanah            17743 non-null  object\ndtypes: int64(1), object(12)\nmemory usage: 1.9+ MB\n\n\n\n\nCode\ndf = df[~df['Luas Tanah'].isna()].reset_index(drop=True)\ndf = df[~((df['Kamar Mandi'].isna()) & (df['Kamar Mandi Pembantu'].isna()) & (df['Kamar Pembantu'].isna()) & (df['Kamar Tidur'].isna()))].reset_index(drop=True)\n\n\n\nI removed rows that lacked critical information, such as ‘Luas Tanah’ and the total number of ‘Kamar’.\n\n\n\nCode\nfor kol in ['Kamar Mandi','Kamar Tidur']:\n    df[kol] = df[kol].fillna(1)\n\ndf['Luas Bangunan'] = df['Luas Bangunan'].fillna(df['Luas Tanah'])\ndf['Kamar Mandi Pembantu'] = df['Kamar Mandi Pembantu'].fillna(0)\ndf['Kamar Pembantu'] = df['Kamar Mandi Pembantu'].fillna(0)\n\n\n\nThe columns mentioned above have a small number of missing values. I filled in the missing values for ‘Kamar Mandi’ and ‘Kamar Tidur’ with ‘1’ because there is at least one of each of them in a house. For ‘Kamar & Kamar Mandi Pembantu,’ I filled in ‘0’.\n\n\n\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 17149 entries, 0 to 17148\nData columns (total 13 columns):\n #   Column                Non-Null Count  Dtype \n---  ------                --------------  ----- \n 0   price                 17149 non-null  int64 \n 1   Kecamatan             17149 non-null  object\n 2   komplek               17149 non-null  object\n 3   ID                    17149 non-null  object\n 4   Daya Listrik          14208 non-null  object\n 5   Jumlah Lantai         15731 non-null  object\n 6   Kamar Mandi           17149 non-null  object\n 7   Kamar Mandi Pembantu  17149 non-null  object\n 8   Kamar Pembantu        17149 non-null  object\n 9   Kamar Tidur           17149 non-null  object\n 10  Kondisi Perabotan     12890 non-null  object\n 11  Luas Bangunan         17149 non-null  object\n 12  Luas Tanah            17149 non-null  object\ndtypes: int64(1), object(12)\nmemory usage: 1.7+ MB\n\n\n\n\nCode\n#If a LB is bigger than LT, I assume that the houses has more than 1 floor.\ndef conditions(x):\n    if pd.isnull(x['Jumlah Lantai']):\n        if x['Luas Bangunan'] &gt; x['Luas Tanah']:\n            return '2'\n        else:\n            return '1'\n    else:\n        return x['Jumlah Lantai']\n\ndf['Jumlah Lantai'] = df[['Jumlah Lantai','Luas Bangunan','Luas Tanah']].apply(lambda x: conditions(x) , axis = 1 )\n\n\n\n\nCode\n#I replaced certain ambiguous values in the 'Daya Listrik' column with 'NaN' so it can be imputed with KNN Imputer\ncondition = ((df['Daya Listrik'] == 'Lainnya Watt') | (df['Daya Listrik'] == 'lainnya Watt'))\ndf.loc[condition, 'Daya Listrik'] = np.nan\n\n\n\n\nCode\n#change data types\ndf['Daya Listrik'] = df['Daya Listrik'].str.replace('Watt','').astype('Float64')\ndf['Luas Bangunan'] = df['Luas Bangunan'].str.replace('m²','').astype(int)\ndf['Luas Tanah'] = df['Luas Tanah'].str.replace('m²','').astype(int)\n\n\n\n\nCode\n#impute with KNN\ndf['avg_bangunan'] = df.groupby('Daya Listrik')['Luas Bangunan'].transform('median').fillna(df['Luas Bangunan'])\nlistrik_impute = pd.DataFrame(KNNImputer(n_neighbors=1).fit_transform(df[['Daya Listrik','avg_bangunan']]))\ndf['Daya Listrik'] = listrik_impute[0]\ndf.drop(columns=['avg_bangunan'], inplace=True)\n\n\n\n\nCode\n#I selected only the 'Kecamatan' with more than 100 samples to ensure a reliable result\ndf = df[ df.groupby('Kecamatan')['price'].transform('count') &gt; 100]\n\n\n\n\nCode\ndf['price/m'] = df.price / (df['Luas Tanah'] + df['Luas Bangunan'])\n\n\n\n\nCode\n#create dictionary to map 'Kondisi Perabotan'\nvalue_mapping = {\n    'Unfurnished': 1,\n    'Furnished' : 2,\n}\n\nreverse_mapping = {\n    1.0 : 'Unfurnished',\n    2.0 : 'Furnished',\n}\n\n\n\n\nCode\ndf['Kondisi Perabotan'] = df['Kondisi Perabotan'].map(value_mapping)\n\n\n\n\nCode\nfor kec in df.Kecamatan.unique() :\n    df1 = df[df.Kecamatan == kec][['Kondisi Perabotan','price/m']].copy().reset_index(drop=True)\n    df1['price/m'] = df1.groupby(['Kondisi Perabotan'])['price/m'].transform('median').fillna(df1['price/m'])\n\n    imputer = KNNImputer(n_neighbors = 1)\n    imputer.fit(df1[['Kondisi Perabotan','price/m']])\n    missing_values = df.loc[df[df.Kecamatan == kec].index,['Kondisi Perabotan','price/m']]\n    df.loc[df[df.Kecamatan == kec].index,['Kondisi Perabotan','price/m']] = imputer.transform(missing_values)\n        \ndf.reset_index(drop=True, inplace=True)\ndf['Kondisi Perabotan'] = df['Kondisi Perabotan'].map(reverse_mapping)\n\n\n\nTo impute missing values for ‘Kondisi Perabotan’, first gather the ‘price/m’ data for each ‘Kecamatan’. Once we have this data, we can observe that ‘Kondisi Perabotan’ prices vary significantly, with furnished properties being considerably more expensive than unfurnished ones. Then, we will use the KNN Imputer to fill in the missing values with the nearest neighbors’ values.\n\n\n\nCode\ndf.drop(columns = ['price/m'], inplace = True)\n\n\n\n\nCode\n#change data types\ndf = df.convert_dtypes(convert_string = False)\nfor column in ['Kamar Mandi','Kamar Mandi Pembantu','Kamar Pembantu','Kamar Tidur','price','Daya Listrik','Luas Bangunan','Luas Tanah']:\n    df[column] = df[column].astype(int)\n\n\n\n\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 17050 entries, 0 to 17049\nData columns (total 13 columns):\n #   Column                Non-Null Count  Dtype \n---  ------                --------------  ----- \n 0   price                 17050 non-null  int64 \n 1   Kecamatan             17050 non-null  object\n 2   komplek               17050 non-null  object\n 3   ID                    17050 non-null  object\n 4   Daya Listrik          17050 non-null  int64 \n 5   Jumlah Lantai         17050 non-null  object\n 6   Kamar Mandi           17050 non-null  int64 \n 7   Kamar Mandi Pembantu  17050 non-null  int64 \n 8   Kamar Pembantu        17050 non-null  int64 \n 9   Kamar Tidur           17050 non-null  int64 \n 10  Kondisi Perabotan     17050 non-null  object\n 11  Luas Bangunan         17050 non-null  int64 \n 12  Luas Tanah            17050 non-null  int64 \ndtypes: int64(8), object(5)\nmemory usage: 1.7+ MB\n\n\n\n\nCode\ndf.drop(columns = 'ID', inplace = True)\ndf.duplicated().sum()\n\n\n2877\n\n\n\nAfter getting rid of the ‘ID’ column, you’ll notice there are still lots of duplicates left. It seems like the same ads are being posted multiple times. Let’s go ahead and clean those up.\n\n\n\nCode\ndf = df.drop_duplicates()\n\n\n\n\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 14173 entries, 0 to 17049\nData columns (total 12 columns):\n #   Column                Non-Null Count  Dtype \n---  ------                --------------  ----- \n 0   price                 14173 non-null  int64 \n 1   Kecamatan             14173 non-null  object\n 2   komplek               14173 non-null  object\n 3   Daya Listrik          14173 non-null  int64 \n 4   Jumlah Lantai         14173 non-null  object\n 5   Kamar Mandi           14173 non-null  int64 \n 6   Kamar Mandi Pembantu  14173 non-null  int64 \n 7   Kamar Pembantu        14173 non-null  int64 \n 8   Kamar Tidur           14173 non-null  int64 \n 9   Kondisi Perabotan     14173 non-null  object\n 10  Luas Bangunan         14173 non-null  int64 \n 11  Luas Tanah            14173 non-null  int64 \ndtypes: int64(8), object(4)\nmemory usage: 1.4+ MB\n\n\n\n\nCode\ndf = df[df['Jumlah Lantai'].astype(int) &lt;= 4]\n\n\n\n\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 14161 entries, 0 to 17049\nData columns (total 12 columns):\n #   Column                Non-Null Count  Dtype \n---  ------                --------------  ----- \n 0   price                 14161 non-null  int64 \n 1   Kecamatan             14161 non-null  object\n 2   komplek               14161 non-null  object\n 3   Daya Listrik          14161 non-null  int64 \n 4   Jumlah Lantai         14161 non-null  object\n 5   Kamar Mandi           14161 non-null  int64 \n 6   Kamar Mandi Pembantu  14161 non-null  int64 \n 7   Kamar Pembantu        14161 non-null  int64 \n 8   Kamar Tidur           14161 non-null  int64 \n 9   Kondisi Perabotan     14161 non-null  object\n 10  Luas Bangunan         14161 non-null  int64 \n 11  Luas Tanah            14161 non-null  int64 \ndtypes: int64(8), object(4)\nmemory usage: 1.4+ MB\n\n\n\n\nCode\ndf5 = df.copy()\n\n\n\n\nCode\n#rename columns\ndf.rename(columns={'Daya Listrik': 'Listrik', \n                   'Jumlah Lantai': 'Lantai',\n                   'Kamar Mandi': 'KM',\n                   'Kamar Mandi Pembantu': 'KMP',\n                   'Kamar Pembantu': 'KP',\n                   'Kamar Tidur': 'KT',\n                   'Kondisi Perabotan': 'Kondisi',\n                   'Luas Bangunan': 'LB',\n                   'Luas Tanah': 'LT'}, inplace=True)\n\n\n\n\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 14161 entries, 0 to 17049\nData columns (total 12 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   price      14161 non-null  int64 \n 1   Kecamatan  14161 non-null  object\n 2   komplek    14161 non-null  object\n 3   Listrik    14161 non-null  int64 \n 4   Lantai     14161 non-null  object\n 5   KM         14161 non-null  int64 \n 6   KMP        14161 non-null  int64 \n 7   KP         14161 non-null  int64 \n 8   KT         14161 non-null  int64 \n 9   Kondisi    14161 non-null  object\n 10  LB         14161 non-null  int64 \n 11  LT         14161 non-null  int64 \ndtypes: int64(8), object(4)\nmemory usage: 1.4+ MB\n\n\n\n\nCode\ndf.Kecamatan.unique()\n\n\narray(['BSD City', 'Poris', 'Karang Tengah', 'Pondok Benda',\n       'Tangerang Kota', 'BSD Delatinos', 'Serua', 'Karawaci', 'Jombang',\n       'Cikupa', 'Curug', 'Suradita', 'Panongan', 'BSD Residence One',\n       'Kelapa Dua', 'Pondok Ranji', 'Ciledug', 'Pondok Cabe',\n       'Gading Serpong', 'Pagedangan', 'Legok', 'Pinang', 'Cikokol',\n       'Dadap', 'BSD Foresta', 'BSD Green Wich', 'Cireundeu',\n       'BSD Anggrek Loka', 'Cipondoh', 'BSD Nusaloka', 'BSD Puspita Loka',\n       'Gading Serpong Pondok Hijau Golf', 'BSD The Green', 'Cipadu',\n       'Larangan', 'BSD Giri Loka', 'Cikupa Citra Raya', 'Modernland',\n       'Benda', 'Rempoa', 'Alam Sutera', 'BSD', 'Cisauk', 'Graha Raya',\n       'Lippo Karawaci'], dtype=object)"
  },
  {
    "objectID": "portfolios/houses_tangerang/index.html#basic-exploratory-data-analysis",
    "href": "portfolios/houses_tangerang/index.html#basic-exploratory-data-analysis",
    "title": "Tangerang house price prediction",
    "section": "Basic Exploratory Data Analysis",
    "text": "Basic Exploratory Data Analysis\n\n\nCode\n#Let's split the categorical and numerical columns into separate dataframes for easier analysis.\ndfnum = df._get_numeric_data()\ndfcat = df.drop(columns = dfnum.columns)\n\n\n\nDescriptive Statistic\n\n\nCode\n#numerical columns describe.\ndfnum.describe()\n\n\n\n\n\n\n\n\n\nprice\nListrik\nKM\nKMP\nKP\nKT\nLB\nLT\n\n\n\n\ncount\n1.416100e+04\n14161.000000\n14161.000000\n14161.000000\n14161.000000\n14161.000000\n14161.000000\n1.416100e+04\n\n\nmean\n4.525861e+03\n3892.131205\n2.717958\n0.419603\n0.419603\n3.521715\n187.472848\n4.820822e+04\n\n\nstd\n1.518706e+05\n6346.367405\n2.124905\n0.535776\n0.535776\n2.272806\n209.443483\n5.714284e+06\n\n\nmin\n7.000000e+00\n-1300.000000\n1.000000\n0.000000\n0.000000\n1.000000\n1.000000\n7.000000e+00\n\n\n25%\n1.200000e+03\n2200.000000\n2.000000\n0.000000\n0.000000\n3.000000\n81.000000\n8.300000e+01\n\n\n50%\n2.000000e+03\n2200.000000\n2.000000\n0.000000\n0.000000\n3.000000\n135.000000\n1.260000e+02\n\n\n75%\n3.500000e+03\n4400.000000\n3.000000\n1.000000\n1.000000\n4.000000\n225.000000\n2.050000e+02\n\n\nmax\n1.800000e+07\n95000.000000\n99.000000\n10.000000\n10.000000\n99.000000\n8032.000000\n6.800000e+08\n\n\n\n\n\n\n\n\nAll columns above have outliers. ‘Listrik’ column has negative value which is caused by incorrect user input\n\n\n\nCode\ndf['Listrik'] = abs(df['Listrik'])\n\n\n\n\nCode\ndfcat.describe()\n\n\n\n\n\n\n\n\n\nKecamatan\nkomplek\nLantai\nKondisi\n\n\n\n\ncount\n14161\n14161\n14161\n14161\n\n\nunique\n45\n2\n4\n2\n\n\ntop\nPoris\ntidak\n2\nUnfurnished\n\n\nfreq\n503\n7852\n8818\n8014\n\n\n\n\n\n\n\n\nThere are 45 ‘Kecamatan’ used on this samples.\n\n\n\nUnivariate Analysis\n\n\nCode\nfig, axarr = plt.subplots(2,4, figsize=(11, 8))\nfor col in dfnum.columns:\n    loc = dfnum.columns.get_loc(col)\n    y = 0 if loc &lt; 4 else 1\n    x = loc if loc &lt; 4 else loc - 4\n    sns.boxplot(y = dfnum[col], ax = axarr[y,x])\n    axarr[y, x].set_ylabel(None)\n    axarr[y, x].set_xlabel(col)\nplt.suptitle('Outliers checking on numeric columns', weight='bold')\nfig.tight_layout(pad=1)\nplt.show()\n\n\n\n\n\n\n\nCode\n#removing extreme values\ndf = df[(df.price &lt; 10000)]\ndf = df[(df['Listrik'] &lt; 20000)]\ndf = df[(df['KM'] &lt; 10)]\ndf = df[(df['KMP'] &lt;= 2)]\ndf = df[(df['KP'] &lt;= 2)]\ndf = df[(df['KT'] &lt;= 20)]\ndf = df[(df['LB'] &lt;= 1000)]\ndf = df[(df['LT'] &lt;= 1000)]\n\n\n\n\nCode\ndfnum = df._get_numeric_data()\ndfcat = df.drop(columns = dfnum.columns)\n\n\n\n\nCode\nfig, axarr = plt.subplots(2,4, figsize=(11, 8))\nfor col in dfnum.columns:\n    loc = dfnum.columns.get_loc(col)\n    y = 0 if loc &lt; 4 else 1\n    x = loc if loc &lt; 4 else loc - 4\n    sns.histplot(data=dfnum[col], \n                 color='skyblue', \n                 kde=True, \n                 edgecolor='none', \n                 ax=axarr[y, x])\nplt.suptitle('Distribution checking on numeric columns', weight='bold')\nfig.tight_layout(pad=1)\nplt.show()\n\n\n\n\n\n\nWe can clearly see that this data is right skewed.\n\n\n\nCode\nfig, axarr = plt.subplots(2,2, figsize=(11, 8))\nfor col in dfcat.columns:\n    loc = dfcat.columns.get_loc(col)\n    y = 0 if loc &lt; 2 else 1\n    x = loc if loc &lt; 2 else loc - 2\n    sns.countplot(x=df[col], color='green', ax = axarr[y,x], order=df[col].value_counts().iloc[:4].index)\n\nplt.suptitle('Countplot of categorical columns', weight='bold')\nfig.tight_layout(pad=1)\nplt.show()\n\n\n\n\n\n\n\nMultivariate Analysis\n\n\nCode\nplt.figure(figsize=(10,8))\nsns.heatmap(dfnum.corr(), annot=True, fmt='.2f')\nplt.title('Correlation plot', weight ='bold')\nplt.xticks(rotation = 0)\nplt.show()\n\n\n\n\n\n\nprice, LB, and LT, exhibit a strong positive correlation. Although this correlation is quite strong, it does not lead to multicollinearity issues."
  },
  {
    "objectID": "portfolios/houses_tangerang/index.html#deep-dive-exploratory-data-analysis",
    "href": "portfolios/houses_tangerang/index.html#deep-dive-exploratory-data-analysis",
    "title": "Tangerang house price prediction",
    "section": "Deep-Dive Exploratory Data Analysis",
    "text": "Deep-Dive Exploratory Data Analysis\n\n\nCode\ndf['KMR'] = df.KP + df.KT\ndf['KMD'] = df.KM + df.KMP\ndf['price/m'] = df.price / (df.LT + df.LB) \ndf['Lantai'] = df.Lantai.astype(int)\ndf.drop(columns=['KM','KMP','KP','KT'], inplace=True)\n\n\n\nI’ve developed a new feature named ‘price/m’ by dividing the price by the sum of LT and LB. This feature is more suitable for analysis compared to using just the price alone.\n\n\n\nCode\nsns.histplot(df.groupby('Kecamatan').size(), alpha=0.8)\nplt.xlabel('Kecamatan Frequency')\nplt.title('Kecamatan distribution count', weight='bold')\nplt.show()\n\n\n\n\n\n\nThe plot above illustrates the distribution of samples collected from each Kecamatan. The sample size varies, with the lowest being approximately 100 and the highest reaching 500. In cases of insufficient sample size, potential issues may arise, and further analysis is necessary to determine whether these samples are adequate.\n\n\n\nCode\nfig, axarr = plt.subplots(1,2, figsize=(10, 4))\ndata = df.groupby('Kecamatan').size().reset_index(drop=False).sort_values(0, ascending=False)\ndata.columns = ['Kecamatan','count']\nsns.barplot(data = data.head(15), x='count', y='Kecamatan', ax = axarr[0])\nsns.barplot(data = data.tail(15), x='count', y='Kecamatan', ax = axarr[1], palette='RdBu')\naxarr[0].set_title('Top 15 highest kecamatan count', weight='bold')\naxarr[1].set_title('Top 15 lowest kecamatan count', weight='bold')\naxarr[0].set_position([0.1, 0.1, 0.3, 0.8])\naxarr[1].set_position([0.6, 0.1, 0.3, 0.8])\n\n\n\n\n\n\n\nCode\ndf['Kecamatan'] = df.Kecamatan.str.replace('Gading Serpong Pondok Hijau Golf' , 'Gading Serpong PHG')\n\n\n\n\nCode\nfig, axarr = plt.subplots(1,2, figsize=(10, 4))\ndata = df.groupby('Kecamatan')[['price']].agg('median').reset_index().sort_values('price', ascending=False)\nsns.barplot(data = data.head(15), x='price', y='Kecamatan', ax = axarr[0], palette='plasma')\nsns.barplot(data = data.tail(15), x='price', y='Kecamatan', ax = axarr[1], palette='coolwarm')\nplt.title('harga per kecamatan')\naxarr[0].set_title('Top 15 highest kecamatan price', weight='bold')\naxarr[1].set_title('Top 15 lowest kecamatan price', weight='bold')\naxarr[0].set_position([0.1, 0.1, 0.3, 0.8])\naxarr[1].set_position([0.6, 0.1, 0.3, 0.8])\n\n\n\n\n\n\nBSD is one of the most expensive region in Tangerang\n\n\n\nCode\nfig, axarr = plt.subplots(1,2, figsize=(10, 4))\ndata = df.groupby('Kecamatan')[['price/m']].agg('median').reset_index().sort_values('price/m', ascending=False)\nsns.barplot(data = data.head(15), x='price/m', y='Kecamatan', ax = axarr[0], palette = 'YlGn')\nsns.barplot(data = data.tail(15), x='price/m', y='Kecamatan', ax = axarr[1], palette = 'YlGnBu')\nplt.title('harga meteran per kecamatan')\naxarr[0].set_title('Top 15 highest kecamatan price/m', weight='bold')\naxarr[1].set_title('Top 15 lowest kecamatan price/m', weight='bold')\naxarr[0].set_position([0.1, 0.1, 0.3, 0.8])\naxarr[1].set_position([0.6, 0.1, 0.3, 0.8])\n\n\n\n\n\n\nfrom the price/m, bsd is still one of highest price/m\n\n\n\nCode\ndf.Kondisi = df.Kondisi.map(value_mapping)\n\n\n\n\nCode\ndf['TK'] = df.KMR + df.KMD\ndf.drop(columns=['KMR','KMD'], inplace=True)\n\n\n\n\nCode\ndf['avg_price'] = df.groupby(['Kecamatan','Kondisi'])['price/m'].transform('median')\n\n\n\n\nCode\ndf['rstd'] = df.groupby(['Kecamatan','Kondisi'])['price/m'].transform(lambda x: x.std() * 100 / x.mean())\n\n\n\n\nCode\ndf1 = df[df.Kondisi == 1]\ndf2 = df[df.Kondisi == 2]\n\nfig, axarr = plt.subplots(1,2, figsize=(10, 4))\ndata = df1[['Kecamatan','rstd']].drop_duplicates().sort_values('rstd', ascending=False).head()\nsns.barplot(data = data, x='rstd', y='Kecamatan', ax = axarr[0])\nsns.histplot(data = df1.rstd.drop_duplicates(), ax = axarr[1])\nsns.histplot(data = df2.rstd.drop_duplicates(), ax = axarr[1], alpha = 0.5, legend='a')\naxarr[0].set_title('Top 5 highest kecamatan price variability', weight='bold')\naxarr[1].set_title('r-std.dev price/m per kecamatan', weight='bold')\naxarr[0].yaxis.set_tick_params(labelsize=10)\n\nhand2 = [mpatches.Rectangle((0, 0), 1, 1, color='blue', label='Unfurnished')]\nhand1 = [mpatches.Rectangle((0, 0), 1, 1, color='orange', label='Furnished')]\nlegend1 = plt.legend(handles = hand2, loc='upper right', labelcolor='blue')\nlegend2 = plt.legend(handles = hand1, loc='upper left', labelcolor='orange')\nplt.gca().add_artist(legend1)\nplt.show()\n\n\n\n\n\n\nR-std, which stands for relative standard deviation, is a valuable tool for assessing variability and making comparisons between datasets. In the displayed plot, we observe that the r-std is distributed within a substantial 30% range. When the price exhibits a high r-std, it typically indicates an insufficient number of samples or a failure to account for critical factors influencing price fluctuations.\n\n\n\nCode\n#create a variable to contain avg_price information from the data to be used by test data\navg = df[['Kecamatan','Kondisi','avg_price']].drop_duplicates()\n\n\n\n\nCode\n#remove columns that are no longer needed\ndf9 = df.copy()\ndf = df.drop(columns=['Kecamatan','price/m','rstd'])\n\n\n\n\nCode\n#encoding\ndf['komplek'] =  df[['komplek']].apply(lambda x: 1 if x.komplek == 'ya' else 0, axis = 1)\n\n\n\n\nCode\n#create a variable to contain result from all combination\nfinal_result = pd.DataFrame(index = ['mae','mape','rmse','r2'])"
  },
  {
    "objectID": "portfolios/houses_tangerang/index.html#modelling-combination-1",
    "href": "portfolios/houses_tangerang/index.html#modelling-combination-1",
    "title": "Tangerang house price prediction",
    "section": "Modelling Combination 1",
    "text": "Modelling Combination 1\nFor the first combination, I will utilize all available features and apply linear regression.\n\n\nCode\ndfc1 = df.copy().reset_index(drop=True)\n\n\n\n\nCode\ndfc1.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 13428 entries, 0 to 13427\nData columns (total 9 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   price      13428 non-null  int64  \n 1   komplek    13428 non-null  int64  \n 2   Listrik    13428 non-null  int64  \n 3   Lantai     13428 non-null  int64  \n 4   Kondisi    13428 non-null  int64  \n 5   LB         13428 non-null  int64  \n 6   LT         13428 non-null  int64  \n 7   TK         13428 non-null  int64  \n 8   avg_price  13428 non-null  float64\ndtypes: float64(1), int64(8)\nmemory usage: 944.3 KB\n\n\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(dfc1.drop(columns='price'), dfc1.price.to_numpy(), test_size = 0.2, random_state=129)\nX_train = X_train.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)\n\n\n\n\nCode\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n\nCode\ny_pred_test = model.predict(X_test)\ny_pred_train = model.predict(X_train)\n\n\n\n\nCode\nresult = pd.DataFrame()\nmae_test = mean_absolute_error(y_test, y_pred_test)\nmape_test = mean_absolute_percentage_error(y_test, y_pred_test)\nrmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\nr2_test = r2_score(y_test, y_pred_test)\n\nmae_train = mean_absolute_error(y_train, y_pred_train)\nmape_train = mean_absolute_percentage_error(y_train, y_pred_train)\nrmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\nr2_train = r2_score(y_train, y_pred_train)\n\nresult['test_OLS'] = [mae_test, mape_test, rmse_test, r2_test]\nresult['train_OLS'] = [mae_train, mape_train, rmse_train, r2_train]\nresult.index = ['mae','mape','rmse','r2']\n\n\n\n\nCode\nresult\n\n\n\n\n\n\n\n\n\ntest_OLS\ntrain_OLS\n\n\n\n\nmae\n532.715149\n541.468771\n\n\nmape\n0.278840\n0.329897\n\n\nrmse\n800.262693\n805.231411\n\n\nr2\n0.797440\n0.794544\n\n\n\n\n\n\n\n\nAbove is the result of the base model.\n\n\n\nCode\nfinal_result = pd.concat([final_result, result], axis = 1)"
  },
  {
    "objectID": "portfolios/houses_tangerang/index.html#modelling-combination-2",
    "href": "portfolios/houses_tangerang/index.html#modelling-combination-2",
    "title": "Tangerang house price prediction",
    "section": "Modelling Combination 2",
    "text": "Modelling Combination 2\nIn this second combination, I intend to enhance model complexity by incorporating polynomial features. If overfitting concerns arise, I will address them by applying ridge regression.\n\n\nCode\ndfc2 = df.copy().reset_index(drop=True) \n\n\n\n\nCode\ndfc2.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 13428 entries, 0 to 13427\nData columns (total 9 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   price      13428 non-null  int64  \n 1   komplek    13428 non-null  int64  \n 2   Listrik    13428 non-null  int64  \n 3   Lantai     13428 non-null  int64  \n 4   Kondisi    13428 non-null  int64  \n 5   LB         13428 non-null  int64  \n 6   LT         13428 non-null  int64  \n 7   TK         13428 non-null  int64  \n 8   avg_price  13428 non-null  float64\ndtypes: float64(1), int64(8)\nmemory usage: 944.3 KB\n\n\n\n\nCode\nscaler = MinMaxScaler()\ndf1 = scaler.fit_transform(dfc2.drop(columns=['price']))\ndf1 = pd.DataFrame(df1, columns = dfc2.drop(columns=['price']).columns)\ndf1['price'] = dfc2.price\ndfc2 = df1.copy()\n\n\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(dfc2.drop(columns='price'), dfc2.price.to_numpy(), test_size=0.2, random_state=129)\n\n\n\n\nCode\nresult = pd.DataFrame()\nfor x in [2,3,4]:\n    poly_features = PolynomialFeatures(degree=x)\n    X_poly = poly_features.fit_transform(X_train)\n    X_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(X_poly, y_train, test_size=0.2, random_state=129)\n\n    model = LinearRegression()\n    model.fit(X_train_poly, y_train_poly)\n\n    y_pred_test = model.predict(X_test_poly)\n    y_pred_train = model.predict(X_train_poly)\n\n    mae_test = mean_absolute_error(y_test_poly, y_pred_test)\n    rmse_test = np.sqrt(mean_squared_error(y_test_poly, y_pred_test))\n    r2_test = r2_score(y_test_poly, y_pred_test)\n\n    mae_train = mean_absolute_error(y_train_poly, y_pred_train)\n    rmse_train = np.sqrt(mean_squared_error(y_train_poly, y_pred_train))\n    r2_train = r2_score(y_train_poly, y_pred_train)\n\n    result[f'test_{x}'] = [mae_test, rmse_test, r2_test]\n    result[f'train_{x}'] = [mae_train, rmse_train, r2_train]\n\nresult.index = ['mae','rmse','r2']\nresult\n\n\n\n\n\n\n\n\n\ntest_2\ntrain_2\ntest_3\ntrain_3\ntest_4\ntrain_4\n\n\n\n\nmae\n472.915901\n481.615889\n472.519472\n464.954008\n501.537211\n442.182946\n\n\nrmse\n712.046550\n738.563833\n719.862027\n705.295298\n834.318214\n660.588687\n\n\nr2\n0.837082\n0.827754\n0.833486\n0.842922\n0.776326\n0.862204\n\n\n\n\n\n\n\n\nI tested a polynomial degree of 3, and the results indicate a minor degree of overfitting. However, when I increased it to degree 4, the model exhibited a severe overfitting issue.\n\n\n\nCode\nresult = pd.DataFrame()\nfor x in [3,4]:\n    poly_features = PolynomialFeatures(degree=x)\n    X_poly = poly_features.fit_transform(X_train)\n    X_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(X_poly, y_train, test_size=0.2, random_state=129)\n\n    for alpha in [0.01, 0.1, 1., 10]:\n        model = Ridge(alpha=alpha)\n        model.fit(X_train_poly, y_train_poly)\n\n        y_pred_test = model.predict(X_test_poly)\n        y_pred_train = model.predict(X_train_poly)\n\n        mae_test = mean_absolute_error(y_test_poly, y_pred_test)\n        rmse_test = np.sqrt(mean_squared_error(y_test_poly, y_pred_test))\n        r2_test = r2_score(y_test_poly, y_pred_test)\n\n        mae_train = mean_absolute_error(y_train_poly, y_pred_train)\n        rmse_train = np.sqrt(mean_squared_error(y_train_poly, y_pred_train))\n        r2_train = r2_score(y_train_poly, y_pred_train)\n\n        result[f'test_{x}_{alpha}'] = [mae_test, rmse_test, r2_test]\n        result[f'train_{x}_{alpha}'] = [mae_train, rmse_train, r2_train]\n\nresult.index = ['mae','rmse','r2']\nresult\n\n\n\n\n\n\n\n\n\ntest_3_0.01\ntrain_3_0.01\ntest_3_0.1\ntrain_3_0.1\ntest_3_1.0\ntrain_3_1.0\ntest_3_10\ntrain_3_10\ntest_4_0.01\ntrain_4_0.01\ntest_4_0.1\ntrain_4_0.1\ntest_4_1.0\ntrain_4_1.0\ntest_4_10\ntrain_4_10\n\n\n\n\nmae\n467.342704\n466.439950\n464.299064\n470.035509\n470.187015\n478.290378\n484.975821\n490.051383\n464.840133\n456.548113\n462.897803\n463.601116\n466.596826\n473.665648\n481.134291\n487.347859\n\n\nrmse\n704.461373\n708.507262\n698.403249\n717.934342\n707.437585\n731.792609\n726.156035\n749.558649\n706.885343\n690.605679\n695.774749\n704.905074\n700.618287\n723.246246\n720.228905\n743.215049\n\n\nr2\n0.840534\n0.841488\n0.843265\n0.837242\n0.839184\n0.830898\n0.830561\n0.822587\n0.839435\n0.849397\n0.844443\n0.843096\n0.842270\n0.834824\n0.833316\n0.825578\n\n\n\n\n\n\n\n\nI attempted to use ridge regression to tackle overfitting on highly polynomial features, but the outcome wasn’t superior to simply using a polynomial degree of 2 without any regularization.\n\n\n\nCode\npoly_features = PolynomialFeatures(degree=2)\nX_train = poly_features.fit_transform(X_train)\nX_test = poly_features.fit_transform(X_test)\n\n\n\n\nCode\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n\nCode\ny_pred_test = model.predict(X_test)\ny_pred_train = model.predict(X_train)\n\n\n\n\nCode\nresult = pd.DataFrame()\nmae_test = mean_absolute_error(y_test, y_pred_test)\nmape_test = mean_absolute_percentage_error(y_test, y_pred_test)\nrmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\nr2_test = r2_score(y_test, y_pred_test)\n\nmae_train = mean_absolute_error(y_train, y_pred_train)\nmape_train = mean_absolute_percentage_error(y_train, y_pred_train)\nrmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\nr2_train = r2_score(y_train, y_pred_train)\n\nresult['test_PolyRidgeOLS'] = [mae_test, mape_test, rmse_test, r2_test]\nresult['train_PolyRidgeOLS'] = [mae_train, mape_train, rmse_train, r2_train]\nresult.index = ['mae','mape','rmse','r2']\n\n\n\n\nCode\nresult\n\n\n\n\n\n\n\n\n\ntest_PolyRidgeOLS\ntrain_PolyRidgeOLS\n\n\n\n\nmae\n471.090238\n478.917020\n\n\nmape\n0.227019\n0.276395\n\n\nrmse\n729.230240\n732.438213\n\n\nr2\n0.831804\n0.830011\n\n\n\n\n\n\n\n\nThe model is not overfitting and the result is better than the base model\n\n\n\nCode\nfinal_result = pd.concat([final_result, result], axis = 1)"
  },
  {
    "objectID": "portfolios/houses_tangerang/index.html#modelling-combination-3",
    "href": "portfolios/houses_tangerang/index.html#modelling-combination-3",
    "title": "Tangerang house price prediction",
    "section": "Modelling Combination 3",
    "text": "Modelling Combination 3\nLet’s try to log transform skewed features and use linear regression.\n\n\nCode\ndfc3 = df.copy().reset_index(drop=True)\n\n\n\n\nCode\ncolumn = ['LB','LT','TK']\nfor col in column:\n    dfc3[col] = np.log(dfc3[col])\n\n\n\n\nCode\ndfc3.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 13428 entries, 0 to 13427\nData columns (total 9 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   price      13428 non-null  int64  \n 1   komplek    13428 non-null  int64  \n 2   Listrik    13428 non-null  int64  \n 3   Lantai     13428 non-null  int64  \n 4   Kondisi    13428 non-null  int64  \n 5   LB         13428 non-null  float64\n 6   LT         13428 non-null  float64\n 7   TK         13428 non-null  float64\n 8   avg_price  13428 non-null  float64\ndtypes: float64(4), int64(5)\nmemory usage: 944.3 KB\n\n\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(dfc3.drop(columns='price'), dfc3.price.to_numpy(), test_size = 0.2, random_state=129)\nX_train = X_train.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)\n\n\n\n\nCode\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n\nCode\ny_pred_test = model.predict(X_test)\ny_pred_train = model.predict(X_train)\n\n\n\n\nCode\nresult = pd.DataFrame()\nmae_test = mean_absolute_error(y_test, y_pred_test)\nmape_test = mean_absolute_percentage_error(y_test, y_pred_test)\nrmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\nr2_test = r2_score(y_test, y_pred_test)\n\nmae_train = mean_absolute_error(y_train, y_pred_train)\nmape_train = mean_absolute_percentage_error(y_train, y_pred_train)\nrmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\nr2_train = r2_score(y_train, y_pred_train)\n\nresult['test_LogOLS'] = [mae_test, mape_test, rmse_test, r2_test]\nresult['train_LogOLS'] = [mae_train, mape_train, rmse_train, r2_train]\nresult.index = ['mae','mape','rmse','r2']\n\n\n\n\nCode\nresult\n\n\n\n\n\n\n\n\n\ntest_LogOLS\ntrain_LogOLS\n\n\n\n\nmae\n583.179251\n595.413840\n\n\nmape\n0.329967\n0.394927\n\n\nrmse\n835.080845\n841.121663\n\n\nr2\n0.779431\n0.775821\n\n\n\n\n\n\n\n\nThe results show a performance decline compared to using Linear Regression on the original, untransformed features.\n\n\n\nCode\nfinal_result = pd.concat([final_result, result], axis = 1)"
  },
  {
    "objectID": "portfolios/houses_tangerang/index.html#modelling-combination-4",
    "href": "portfolios/houses_tangerang/index.html#modelling-combination-4",
    "title": "Tangerang house price prediction",
    "section": "Modelling Combination 4",
    "text": "Modelling Combination 4\nFor this combination, let’s try XGBRegressor\n\n\nCode\ndfc4 = df.copy()\n\n\n\n\nCode\ndfc4.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 13428 entries, 0 to 17049\nData columns (total 9 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   price      13428 non-null  int64  \n 1   komplek    13428 non-null  int64  \n 2   Listrik    13428 non-null  int64  \n 3   Lantai     13428 non-null  int64  \n 4   Kondisi    13428 non-null  int64  \n 5   LB         13428 non-null  int64  \n 6   LT         13428 non-null  int64  \n 7   TK         13428 non-null  int64  \n 8   avg_price  13428 non-null  float64\ndtypes: float64(1), int64(8)\nmemory usage: 1.0 MB\n\n\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(dfc4.drop(columns='price'), dfc4.price.to_numpy(), test_size = 0.2, random_state=129)\nX_train = X_train.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)\n\n\n\n\nCode\nxgb_regressor = XGBRegressor(\n    objective='reg:squarederror',  \n    eval_metric='rmse',           \n    random_state=42               \n)\n\n\n\n\nCode\nparam_grid = {  \n    'learning_rate': [0.001, 0.01],  \n    'n_estimators': [100, 400, 900, 1000],  \n    'max_depth': [3, 4],  \n    'eval_metric': ['rmse'],  \n}\n\n\n\n\nCode\ngrid_search = GridSearchCV(\n    estimator=xgb_regressor,\n    param_grid=param_grid,\n    scoring='neg_mean_squared_error',  \n    cv=5,                              \n    verbose=1,                         \n    n_jobs=-1                           \n)\n\n\n\n\nCode\ngrid_search.fit(X_train, y_train)\n\n\nFitting 5 folds for each of 16 candidates, totalling 80 fits\n\n\nGridSearchCV(cv=5,\n             estimator=XGBRegressor(base_score=None, booster=None,\n                                    callbacks=None, colsample_bylevel=None,\n                                    colsample_bynode=None,\n                                    colsample_bytree=None, device=None,\n                                    early_stopping_rounds=None,\n                                    enable_categorical=False,\n                                    eval_metric='rmse', feature_types=None,\n                                    gamma=None, grow_policy=None,\n                                    importance_type=None,\n                                    interaction_constraints=None,\n                                    learning_rate=None...\n                                    max_depth=None, max_leaves=None,\n                                    min_child_weight=None, missing=nan,\n                                    monotone_constraints=None,\n                                    multi_strategy=None, n_estimators=None,\n                                    n_jobs=None, num_parallel_tree=None,\n                                    random_state=42, ...),\n             n_jobs=-1,\n             param_grid={'eval_metric': ['rmse'],\n                         'learning_rate': [0.001, 0.01], 'max_depth': [3, 4],\n                         'n_estimators': [100, 400, 900, 1000]},\n             scoring='neg_mean_squared_error', verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5,\n             estimator=XGBRegressor(base_score=None, booster=None,\n                                    callbacks=None, colsample_bylevel=None,\n                                    colsample_bynode=None,\n                                    colsample_bytree=None, device=None,\n                                    early_stopping_rounds=None,\n                                    enable_categorical=False,\n                                    eval_metric='rmse', feature_types=None,\n                                    gamma=None, grow_policy=None,\n                                    importance_type=None,\n                                    interaction_constraints=None,\n                                    learning_rate=None...\n                                    max_depth=None, max_leaves=None,\n                                    min_child_weight=None, missing=nan,\n                                    monotone_constraints=None,\n                                    multi_strategy=None, n_estimators=None,\n                                    n_jobs=None, num_parallel_tree=None,\n                                    random_state=42, ...),\n             n_jobs=-1,\n             param_grid={'eval_metric': ['rmse'],\n                         'learning_rate': [0.001, 0.01], 'max_depth': [3, 4],\n                         'n_estimators': [100, 400, 900, 1000]},\n             scoring='neg_mean_squared_error', verbose=1)estimator: XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric='rmse', feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=None, n_jobs=None,\n             num_parallel_tree=None, random_state=42, ...)XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric='rmse', feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=None, n_jobs=None,\n             num_parallel_tree=None, random_state=42, ...)\n\n\n\n\nCode\nbest_params_xgb = grid_search.best_params_\n\n\n\n\nCode\nbest_params_xgb\n\n\n{'eval_metric': 'rmse',\n 'learning_rate': 0.01,\n 'max_depth': 4,\n 'n_estimators': 1000}\n\n\n\n\nCode\nxgboost = XGBRegressor(**best_params_xgb, random_state=42)\n\n\n\n\nCode\nxgboost.fit(X_train, y_train)\n\n\nXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric='rmse', feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=4, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=1000, n_jobs=None,\n             num_parallel_tree=None, random_state=42, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric='rmse', feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=4, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=1000, n_jobs=None,\n             num_parallel_tree=None, random_state=42, ...)\n\n\n\n\nCode\ny_pred_test = xgboost.predict(X_test)\ny_pred_train = xgboost.predict(X_train)\n\n\n\n\nCode\nresult = pd.DataFrame()\nmae_test = mean_absolute_error(y_test, y_pred_test)\nmape_test = mean_absolute_percentage_error(y_test, y_pred_test)\nrmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\nr2_test = r2_score(y_test, y_pred_test)\n\nmae_train = mean_absolute_error(y_train, y_pred_train)\nmape_train = mean_absolute_percentage_error(y_train, y_pred_train)\nrmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\nr2_train = r2_score(y_train, y_pred_train)\n\nresult['test_XGB'] = [mae_test, mape_test, rmse_test, r2_test]\nresult['train_XGB'] = [mae_train, mape_train, rmse_train, r2_train]\nresult.index = ['mae','mape','rmse','r2']\n\n\n\n\nCode\nresult\n\n\n\n\n\n\n\n\n\ntest_XGB\ntrain_XGB\n\n\n\n\nmae\n438.290716\n419.337764\n\n\nmape\n0.213093\n0.253128\n\n\nrmse\n678.516114\n626.639712\n\n\nr2\n0.854384\n0.875573\n\n\n\n\n\n\n\n\nThe model is little overfit. But this is the best model so far\n\n\n\nCode\nfinal_result = pd.concat([final_result, result], axis = 1)\n\n\n\n\nCode\nxgb.plot_importance(xgboost)  # You can also use 'gain' or 'cover' for importance_type\nplt.show()"
  },
  {
    "objectID": "portfolios/houses_tangerang/index.html#modelling-combination-5",
    "href": "portfolios/houses_tangerang/index.html#modelling-combination-5",
    "title": "Tangerang house price prediction",
    "section": "Modelling Combination 5",
    "text": "Modelling Combination 5\nI will use RandomForest on this combination. But using RandomForest model on this case is very computational expensive because of many continuous features.\n\n\nCode\ndfc5 = df.copy().reset_index(drop=True) #df[['LT','avg_price','distance','LB','Listrik','price']]\n\n\n\n\nCode\ndfc5.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 13428 entries, 0 to 13427\nData columns (total 9 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   price      13428 non-null  int64  \n 1   komplek    13428 non-null  int64  \n 2   Listrik    13428 non-null  int64  \n 3   Lantai     13428 non-null  int64  \n 4   Kondisi    13428 non-null  int64  \n 5   LB         13428 non-null  int64  \n 6   LT         13428 non-null  int64  \n 7   TK         13428 non-null  int64  \n 8   avg_price  13428 non-null  float64\ndtypes: float64(1), int64(8)\nmemory usage: 944.3 KB\n\n\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(dfc5.drop(columns='price'), dfc5.price.to_numpy(), test_size = 0.2, random_state=129)\nX_train = X_train.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)\n\n\n\n\nCode\nrandomforest = RandomForestRegressor(random_state=42)\n\n\n\n\nCode\nparam_grid = {\n    'n_estimators': [600],\n    'max_depth': [8, 7],\n    'min_samples_split': [10, 50, 100],\n}\n\n\n\n\nCode\ngrid_search = GridSearchCV(estimator=randomforest, param_grid=param_grid, cv=5, n_jobs=-1)\n\n\n\n\nCode\ngrid_search.fit(X_train, y_train)\n\n\nGridSearchCV(cv=5, estimator=RandomForestRegressor(random_state=42), n_jobs=-1,\n             param_grid={'max_depth': [8, 7],\n                         'min_samples_split': [10, 50, 100],\n                         'n_estimators': [600]})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5, estimator=RandomForestRegressor(random_state=42), n_jobs=-1,\n             param_grid={'max_depth': [8, 7],\n                         'min_samples_split': [10, 50, 100],\n                         'n_estimators': [600]})estimator: RandomForestRegressorRandomForestRegressor(random_state=42)RandomForestRegressorRandomForestRegressor(random_state=42)\n\n\n\n\nCode\nbest_params_rf = grid_search.best_params_\n\n\n\n\nCode\nbest_params_rf\n\n\n{'max_depth': 8, 'min_samples_split': 10, 'n_estimators': 600}\n\n\n\n\nCode\nrf = RandomForestRegressor(**best_params_rf, random_state=42)\n\n\n\n\nCode\nrf.fit(X_train, y_train)\n\n\nRandomForestRegressor(max_depth=8, min_samples_split=10, n_estimators=600,\n                      random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressorRandomForestRegressor(max_depth=8, min_samples_split=10, n_estimators=600,\n                      random_state=42)\n\n\n\n\nCode\ny_pred_test = rf.predict(X_test)\ny_pred_train = rf.predict(X_train)\n\n\n\n\nCode\nresult = pd.DataFrame()\nmae_test = mean_absolute_error(y_test, y_pred_test)\nmape_test = mean_absolute_percentage_error(y_test, y_pred_test)\nrmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\nr2_test = r2_score(y_test, y_pred_test)\n\nmae_train = mean_absolute_error(y_train, y_pred_train)\nmape_train = mean_absolute_percentage_error(y_train, y_pred_train)\nrmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\nr2_train = r2_score(y_train, y_pred_train)\n\nresult['test_RF'] = [mae_test, mape_test, rmse_test, r2_test]\nresult['train_RF'] = [mae_train, mape_train, rmse_train, r2_train]\nresult.index = ['mae','mape','rmse','r2']\n\n\n\n\nCode\nresult\n\n\n\n\n\n\n\n\n\ntest_RF\ntrain_RF\n\n\n\n\nmae\n438.359894\n399.306010\n\n\nmape\n0.216506\n0.243069\n\n\nrmse\n687.059730\n599.451504\n\n\nr2\n0.850694\n0.886136\n\n\n\n\n\n\n\n\nThe result is more or less the same with XGBoost model.\n\n\n\nCode\nfinal_result = pd.concat([final_result, result], axis = 1)"
  },
  {
    "objectID": "portfolios/houses_tangerang/index.html#modelling-combination-6",
    "href": "portfolios/houses_tangerang/index.html#modelling-combination-6",
    "title": "Tangerang house price prediction",
    "section": "Modelling Combination 6",
    "text": "Modelling Combination 6\nFor this last combination, let’s see how AdaBoost perform.\n\n\nCode\ndfc6 = df.copy().reset_index(drop=True) #df[['LT','avg_price','distance','LB','Listrik','price']]\n\n\n\n\nCode\ndfc6.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 13428 entries, 0 to 13427\nData columns (total 9 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   price      13428 non-null  int64  \n 1   komplek    13428 non-null  int64  \n 2   Listrik    13428 non-null  int64  \n 3   Lantai     13428 non-null  int64  \n 4   Kondisi    13428 non-null  int64  \n 5   LB         13428 non-null  int64  \n 6   LT         13428 non-null  int64  \n 7   TK         13428 non-null  int64  \n 8   avg_price  13428 non-null  float64\ndtypes: float64(1), int64(8)\nmemory usage: 944.3 KB\n\n\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(dfc5.drop(columns='price'), dfc5.price.to_numpy(), test_size = 0.2, random_state=129)\nX_train = X_train.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)\n\n\n\n\nCode\nadaboost = AdaBoostRegressor(random_state = 42)\n\n\n\n\nCode\nparam_grid = {\n    'n_estimators': [50, 150, 250],\n    'learning_rate': [0.01, 0.1, 1, 0.001]\n}\n\n\n\n\nCode\ngrid_search = GridSearchCV(estimator=adaboost, param_grid=param_grid, cv=5, n_jobs=-1)\n\n\n\n\nCode\ngrid_search.fit(X_train, y_train)\n\n\nGridSearchCV(cv=5, estimator=AdaBoostRegressor(random_state=42), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 1, 0.001],\n                         'n_estimators': [50, 150, 250]})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5, estimator=AdaBoostRegressor(random_state=42), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 1, 0.001],\n                         'n_estimators': [50, 150, 250]})estimator: AdaBoostRegressorAdaBoostRegressor(random_state=42)AdaBoostRegressorAdaBoostRegressor(random_state=42)\n\n\n\n\nCode\nbest_params_ada = grid_search.best_params_\n\n\n\n\nCode\nbest_params_ada\n\n\n{'learning_rate': 0.1, 'n_estimators': 50}\n\n\n\n\nCode\nada = AdaBoostRegressor(**best_params_ada, random_state=42)\n\n\n\n\nCode\nada.fit(X_train, y_train)\n\n\nAdaBoostRegressor(learning_rate=0.1, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.AdaBoostRegressorAdaBoostRegressor(learning_rate=0.1, random_state=42)\n\n\n\n\nCode\ny_pred_test = ada.predict(X_test)\ny_pred_train = ada.predict(X_train)\n\n\n\n\nCode\nresult = pd.DataFrame()\nmae_test = mean_absolute_error(y_test, y_pred_test)\nmape_test = mean_absolute_percentage_error(y_test, y_pred_test)\nrmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\nr2_test = r2_score(y_test, y_pred_test)\n\nmae_train = mean_absolute_error(y_train, y_pred_train)\nmape_train = mean_absolute_percentage_error(y_train, y_pred_train)\nrmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\nr2_train = r2_score(y_train, y_pred_train)\n\nresult['test_ADA'] = [mae_test, mape_test, rmse_test, r2_test]\nresult['train_ADA'] = [mae_train, mape_train, rmse_train, r2_train]\nresult.index = ['mae','mape','rmse','r2']\n\n\n\n\nCode\nresult\n\n\n\n\n\n\n\n\n\ntest_ADA\ntrain_ADA\n\n\n\n\nmae\n574.532983\n580.901639\n\n\nmape\n0.329233\n0.398783\n\n\nrmse\n838.365578\n816.508925\n\n\nr2\n0.777692\n0.788748\n\n\n\n\n\n\n\n\n\nCode\nfinal_result = pd.concat([final_result, result], axis = 1)"
  },
  {
    "objectID": "portfolios/houses_tangerang/index.html#choosing-the-best-combination",
    "href": "portfolios/houses_tangerang/index.html#choosing-the-best-combination",
    "title": "Tangerang house price prediction",
    "section": "Choosing The Best Combination",
    "text": "Choosing The Best Combination\nLet’s use the best combination\n\n\nCode\nfinal_result\n\n\n\n\n\n\n\n\n\ntest_OLS\ntrain_OLS\ntest_PolyRidgeOLS\ntrain_PolyRidgeOLS\ntest_LogOLS\ntrain_LogOLS\ntest_XGB\ntrain_XGB\ntest_RF\ntrain_RF\ntest_ADA\ntrain_ADA\n\n\n\n\nmae\n532.715149\n541.468771\n471.090238\n478.917020\n583.179251\n595.413840\n438.290716\n419.337764\n438.359894\n399.306010\n574.532983\n580.901639\n\n\nmape\n0.278840\n0.329897\n0.227019\n0.276395\n0.329967\n0.394927\n0.213093\n0.253128\n0.216506\n0.243069\n0.329233\n0.398783\n\n\nrmse\n800.262693\n805.231411\n729.230240\n732.438213\n835.080845\n841.121663\n678.516114\n626.639712\n687.059730\n599.451504\n838.365578\n816.508925\n\n\nr2\n0.797440\n0.794544\n0.831804\n0.830011\n0.779431\n0.775821\n0.854384\n0.875573\n0.850694\n0.886136\n0.777692\n0.788748\n\n\n\n\n\n\n\n\nBy seeing the results, 2 best models are XGBoost and RandomForrest. I’ll go with XGBoost because it way more less computational than RandomForest\n\n\n\nCode\n#choosing only the most important features for XGB\ndf = df[['LT','LB','Listrik','price','avg_price']]\n\nX_train, X_test, y_train, y_test = train_test_split(df.drop(columns='price'), df.price.to_numpy(), test_size = 0.2, random_state=123)\nX_train = X_train.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)\n\n\n\n\nCode\nbest_xgb_regressor = XGBRegressor(**best_params_xgb, random_state=42)\n\n\n\n\nCode\nbest_xgb_regressor.fit(X_train, y_train)\n\n\nXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric='rmse', feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=4, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=1000, n_jobs=None,\n             num_parallel_tree=None, random_state=42, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric='rmse', feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=4, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=1000, n_jobs=None,\n             num_parallel_tree=None, random_state=42, ...)\n\n\n\n\nCode\ny_pred_test = best_xgb_regressor.predict(X_test)\ny_pred_train = best_xgb_regressor.predict(X_train)\n\n\n\n\nCode\nresult = pd.DataFrame()\nmae_test = mean_absolute_error(y_test, y_pred_test)\nmape_test = mean_absolute_percentage_error(y_test, y_pred_test)\nrmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\nr2_test = r2_score(y_test, y_pred_test)\n\nmae_train = mean_absolute_error(y_train, y_pred_train)\nmape_train = mean_absolute_percentage_error(y_train, y_pred_train)\nrmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\nr2_train = r2_score(y_train, y_pred_train)\n\nresult['test_XGB'] = [mae_test, mape_test, rmse_test, r2_test]\nresult['train_XGB'] = [mae_train, mape_train, rmse_train, r2_train]\nresult.index = ['mae','mape','rmse','r2']\n\n\n\n\nCode\nresult\n\n\n\n\n\n\n\n\n\ntest_XGB\ntrain_XGB\n\n\n\n\nmae\n459.897612\n429.131317\n\n\nmape\n0.307846\n0.244483\n\n\nrmse\n699.862902\n643.969481\n\n\nr2\n0.846041\n0.868387\n\n\n\n\n\n\n\n\nThe results remain favorable and consistent"
  },
  {
    "objectID": "portfolios/houses_tangerang/index.html#model-evaluation-interpretation",
    "href": "portfolios/houses_tangerang/index.html#model-evaluation-interpretation",
    "title": "Tangerang house price prediction",
    "section": "Model Evaluation & Interpretation",
    "text": "Model Evaluation & Interpretation\nHere, I will assess and interpret the model’s performance using a new set of sample data that I collected separately from the main dataset.\n\n\nCode\ntest = pd.read_csv('test.csv')\ntest = pd.merge(test, avg , on=['Kecamatan','Kondisi']).convert_dtypes()\n\n\n\nAdd the avg_price column to test data.\n\n\n\nCode\ntest = test[(test.price &lt; 10000) & (test.price &gt; 100)]\ntest = test[test.LB &lt; 900]\ntest = test[test.LT &lt; 500]\ntest = test[['LT','avg_price','LB','Listrik','price']]\n\n\n\nClean the test data\n\n\n\nCode\ntest_x = test[['LT','LB','Listrik','avg_price']]\ntest_y = test.price\n\n\n\n\nCode\ny_pred_test = best_xgb_regressor.predict(test_x)\ny_test = test_y\n\n\n\n\nCode\nresult = pd.DataFrame()\nmae_test = mean_absolute_error(y_test, y_pred_test)\nmape_test = mean_absolute_percentage_error(y_test, y_pred_test)\nrmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\nr2_test = r2_score(y_test, y_pred_test)\n\nmae_train = mean_absolute_error(y_train, y_pred_train)\nmape_train = mean_absolute_percentage_error(y_train, y_pred_train)\nrmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\nr2_train = r2_score(y_train, y_pred_train)\n\nresult['test_XGB'] = [mae_test, mape_test, rmse_test, r2_test]\nresult['train_XGB'] = [mae_train, mape_train, rmse_train, r2_train]\nresult.index = ['mae','mape','rmse','r2']\n\n\n\n\nCode\nresult\n\n\n\n\n\n\n\n\n\ntest_XGB\ntrain_XGB\n\n\n\n\nmae\n531.780306\n429.131317\n\n\nmape\n0.214570\n0.244483\n\n\nrmse\n885.063409\n643.969481\n\n\nr2\n0.785323\n0.868387\n\n\n\n\n\n\n\n\nThe model perform worse on scrapped test data.\n\n\nResiduals Plot\n\n\nCode\nres = pd.DataFrame([list(y_train), list(y_pred_train)]).transpose()\nres.columns = ['test','pred']\nres['residual'] = res.test - res.pred\nres['residualp'] = abs(res.test - res.pred) * 100 / res.test\n\n\n\n\nCode\nfig = px.scatter(x=res['test'], y=res['residualp'])\nfig.update_layout( \n    height = 800,\n    title = 'Residual\\'s absolute percentage plot'\n)\nfig.add_hline(y=res['residualp'].mean(), line_dash=\"dot\", line_color=\"red\")\nfig\n\n\n\n                                                \n\n\n\nFrom the residual percentage above we can see that : 1. 7.5% of data is APE that is more than 50%. 2. 59% of data is APE that is lower than 20%. 3. 32.5% of data is ape that is 20% - 50%. 4. When the houses prices is 4 million, the model become worse as the price kept going up. 5. There are extreme APE error on 0 to 2k prices, this indicate that there are anomalies data inserted.\n\n\n\nCode\nfig = px.scatter(x=res['test'], y=res['residual'])\nfig.update_layout(  \n    height = 700,\n    title = 'Residuals plot'\n)\nfig.add_hline(y=0, line_dash=\"dot\", line_color=\"red\")\nfig\n\n\n\n                                                \n\n\n\nThe unequal variance in the data implies the presence of heteroscedasticity, and a higher price is associated with increased variability. This phenomenon is considered normal since more expensive houses involve a greater number of factors in determining their prices. Additionally, the model also consistently underpredicts as house prices increase. I think we need to gather more variables or predictors that influence house prices.\n\n\n\nPDP & ICE Plots\n\n\nCode\ndef pdp_ice_plot(model, df_test, column, clusters=True):\n    df_test = df_test.copy()\n    pdp_isolate = pdp.PDPIsolate(model = model, df = df_test, \n                      num_grid_points=20,\n                      model_features = df_test.columns, \n                      feature = column, feature_name=column, n_classes=0)\n    pdp_isolate.plot(\n                center=True, plot_lines=True,\n                cluster=clusters, n_cluster_centers=5,cluster_method='accurate',\n                plot_pts_dist=False, to_bins=True,\n                show_percentile=False, which_classes=None,\n                figsize=(10,6), dpi=300,\n                ncols=2, plot_params={\"pdp_hl\": True},\n                engine='matplotlib', template='plotly_white')\n    \n    plt.show()\n    return pdp_isolate\n\n\n\n\nCode\npdp_ice_plot(best_xgb_regressor, test_x, 'LT', clusters=False)\n\n\nobtain pred_func from the provided model.\n\n\n\n\n\n\n\n\n&lt;pdpbox.pdp.PDPIsolate at 0x7f92d07a8a30&gt;\n\n\n\nThe Larger the LT, higher the price. But as the LT get higher, the price is also more disperse.\n\n\n\nCode\ntest1 = test_x.copy()\npdp_interact = pdp.PDPInteract(model=best_xgb_regressor, df=test1,\n                              num_grid_points=10,\n                              model_features = test1.columns, \n                              features=['LB','avg_price'], \n                              feature_names=['LB','avg_price'], n_classes=0)\n\n\nobtain pred_func from the provided model.\nobtain pred_func from the provided model.\nobtain pred_func from the provided model.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npdp_ice_plot(best_xgb_regressor, test_x, 'avg_price', clusters=True)\n\n\nobtain pred_func from the provided model.\n\n\n\n\n\n\n\n\n&lt;pdpbox.pdp.PDPIsolate at 0x7f92c44449d0&gt;\n\n\n\nThe higher the avg_price, also the higher the price, but not all data is affected the same, some are highly affected, some are less affected.\n\n\n\nCode\nfig, axes = pdp_interact.plot(\n    plot_type=\"grid\",\n    to_bins=True,\n    plot_pdp=True,\n    show_percentile=True,\n    which_classes=None,\n    figsize=None,\n    dpi=300,\n    ncols=2,\n    plot_params=None,\n    engine=\"plotly\",\n    template=\"plotly_white\",\n)\nfig\n\n\n\n                                                \n\n\n\nBy combining the avg_price and the LT, you can see that low LT is not affected as much as it affect high LT. The price is more volatile on higher LT."
  },
  {
    "objectID": "portfolios/houses_tangerang/index.html#conclusion",
    "href": "portfolios/houses_tangerang/index.html#conclusion",
    "title": "Tangerang house price prediction",
    "section": "Conclusion",
    "text": "Conclusion\nI have concerns regarding the model’s accuracy, primarily because the residuals are still to be relatively high. This issue can be attributed to the limited sample size and the exclusion of critical factors influencing prices. Among these factors, ‘avg_price’ is of utmost importance. To enhance the accuracy of ‘avg_price,’ it’s imperative to expand the sample size and consider a broader spectrum of variables affecting prices. While I’ve used ‘Kecamatan’ and ‘Kondisi Perabotan’ in this context to estimate ‘avg_price,’ I believe these variables alone are insufficient. Moving forward, I plan to further develop this project by gathering additional data from various real estate websites to create a more robust and accurate model."
  },
  {
    "objectID": "ipynb/telco/final-project.html",
    "href": "ipynb/telco/final-project.html",
    "title": "Telco customer churn analysis",
    "section": "",
    "text": "• Data Introduction  • Data Preparation (Import libraries, data cleaning & data wrangling) • Basic exploratory data analysis      • Descriptive statistic      • Univariate analysis      • Multivariate analysis  • Deep exploratory data analysis      • Services and internet services analysis      • Monthly charges analysis      • Customer analysis      • Benefits analysis      • Churn analysis  • Modelling     • Features selection and encoding      • Splits data and define custom metrics      • Model building combination 1      • Model building combination 2      • Model building combination 3\n• Model interpretation     • PDP and ICE plots      • Checking prediction’s confidence using confidence interval \n\n\n\nThis dataset contain informations about customers that churn and not churned in a telco company. Below is the column informations: 1.customerID = customer unique ID. 2.gender = customer gender (M/F). 3.SeniorCitizen = old / young customer. 4.Partner = either a customer has partners or not. 5.Dependents = either a customer has dependents or not. 6.tenure = how long the customer subscribed (in month). 7.MultipleLines = either a customer using multiple lines or not (phone lines). 8.InternetService = either a customer using InternetService lines or not. 9.OnlineSecurity = either a customer has OnlineSecurity or not. 10.OnlineBackup = either a customer has OnlineBackup or not. 11.DeviceProtection = either a customer has DeviceProtection or not. 12.TechSupport = either a customer has TechSupport or not. 13.StreamingTV = either a customer has StreamingTV or not. 14.StreamingMovies = either a customer has StreamingMovie or not. 15.Contract = types of contract. 16.PaperlessBilling = either a customer has PaperlessBilling or not. 17.PaymentMethod = types of the payment method. 18.MonthlyCharges = how much charges per month. 19.TotalCharges = total charges of all time. 20.Churn = either a customer churn or not. 21.Hobby = customer hobby.\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import gaussian_kde\nimport matplotlib as mpl\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.metrics import confusion_matrix\nfrom pdpbox import pdp, info_plots\nsns.set_style('darkgrid')\nmpl.rcParams['figure.dpi'] = 300 #set figure dpi\nsns.set() #set figure styling\n\n\n#import dataset\ndf = pd.read_csv('telco.csv')\ndf.head()\n\n\n\n\n\n\n\n\ncustomerID\ngender\nSeniorCitizen\nPartner\nDependents\ntenure\nPhoneService\nMultipleLines\nInternetService\nOnlineSecurity\n...\nTechSupport\nStreamingTV\nStreamingMovies\nContract\nPaperlessBilling\nPaymentMethod\nMonthlyCharges\nTotalCharges\nChurn\nHobby\n\n\n\n\n0\n7590-VHVEG\nFemale\n0\nYes\nNo\n135\nNo\nNo phone service\nDSL\nNo\n...\nNo\nNo\nNo\nMonth-to-month\nYes\nElectronic check\n29.85\n29.85\nNo\nSwimming\n\n\n1\n5575-GNVDE\nMale\n0\nNo\nNo\n34\nYes\nNo\nDSL\nYes\n...\nNo\nNo\nNo\nOne year\nNo\nMailed check\n56.95\n1889.5\nNo\nRunning\n\n\n2\n3668-QPYBK\nMale\n0\nNo\nNo\n140\nYes\nNo\nDSL\nYes\n...\nNo\nNo\nNo\nMonth-to-month\nYes\nMailed check\n53.85\n7560\nYes\nHiking\n\n\n3\n7795-CFOCW\nMale\n0\nNo\nNo\n136\nNo\nNo phone service\nDSL\nYes\n...\nYes\nNo\nNo\nOne year\nNo\nBank transfer (automatic)\n42.45\n1840.75\nNo\nSwimming\n\n\n4\n9237-HQITU\nFemale\n0\nNo\nNo\n2\nYes\nNo\nFiber optic\nNo\n...\nNo\nNo\nNo\nMonth-to-month\nYes\nElectronic check\n70.70\n151.65\nYes\nRunning\n\n\n\n\n5 rows × 22 columns\n\n\n\n\n#take a look at the dataframe\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 7043 entries, 0 to 7042\nData columns (total 22 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   customerID        7043 non-null   object \n 1   gender            7043 non-null   object \n 2   SeniorCitizen     7043 non-null   int64  \n 3   Partner           7043 non-null   object \n 4   Dependents        7043 non-null   object \n 5   tenure            7043 non-null   int64  \n 6   PhoneService      7043 non-null   object \n 7   MultipleLines     7043 non-null   object \n 8   InternetService   7043 non-null   object \n 9   OnlineSecurity    7043 non-null   object \n 10  OnlineBackup      7043 non-null   object \n 11  DeviceProtection  6627 non-null   object \n 12  TechSupport       7043 non-null   object \n 13  StreamingTV       7043 non-null   object \n 14  StreamingMovies   7043 non-null   object \n 15  Contract          6798 non-null   object \n 16  PaperlessBilling  7043 non-null   object \n 17  PaymentMethod     7043 non-null   object \n 18  MonthlyCharges    7043 non-null   float64\n 19  TotalCharges      4859 non-null   object \n 20  Churn             7043 non-null   object \n 21  Hobby             4201 non-null   object \ndtypes: float64(1), int64(2), object(19)\nmemory usage: 1.2+ MB\n\n\nThere are 4 columns with missing values which are DeviceProtection, Contract, TotalCharges and Hobby.\n\n#checking percentage of missing values\nmissingkolom = ['DeviceProtection','Contract','TotalCharges','Hobby']\nfor x in missingkolom:\n    print(f'Missing value of column {x} (%) : {round(df[x].isna().sum()/len(df) * 100,2)}')\n\nMissing value of column DeviceProtection (%) : 5.91\nMissing value of column Contract (%) : 3.48\nMissing value of column TotalCharges (%) : 31.01\nMissing value of column Hobby (%) : 40.35\n\n\n\n#impute missing values with univariate imputation by value proportion\ndf['DeviceProtection'] = df['DeviceProtection'].fillna(\n                             pd.Series(np.random.choice(['No','Yes','No internet service'], \n                             p = list(df['DeviceProtection'].value_counts(normalize=True)), size=len(df))))\n\ndf['Contract'] = df['Contract'].fillna(\n                     pd.Series(np.random.choice(['Month-to-month','Two year','One year'], \n                     p = list(df['Contract'].value_counts(normalize=True)), size=len(df))))\n\nHere I impute DeviceProtection and Contract with univariate imputation by value proportion for the following reasons: 1.The missing values is not that much (&lt;10%). 2.The columns don’t have any relationship with other columns so that multivariate imputation is not possible. 3.Using proportion is more precise in this case rather than use ‘mode’.\n\n#delete column Hobby\ndf.drop(columns=['Hobby'],inplace=True)\n\nReasons to delete: 1.Missing values is too many. 2.By business context, Hobby doesn’t give enough useful informations. 3.Cannot be imputed by multivariate imputation.\n\n#impute TotalCharges from tenure and MonthlyCharges\ndf['TotalCharges'] = df['TotalCharges'].fillna(df['tenure'] * df['MonthlyCharges'])\n\nEven this column has so many missing values, I decided to impute it with multivariate imputation because: 1.By business context, TotalCharges is more or less tenure * MonthlyCharges. 2.So even the missing values are high, it can still be imputed with a strong justification.\n\n#there's a space in the total charges column.\nfor x in df.TotalCharges:\n    try:\n        float(x)\n    except:\n        print(f'Unable to convert to float with this value : {x}')\n\nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \n\n\n\ndf[df.TotalCharges == ' '].head()\n\n\n\n\n\n\n\n\ncustomerID\ngender\nSeniorCitizen\nPartner\nDependents\ntenure\nPhoneService\nMultipleLines\nInternetService\nOnlineSecurity\n...\nDeviceProtection\nTechSupport\nStreamingTV\nStreamingMovies\nContract\nPaperlessBilling\nPaymentMethod\nMonthlyCharges\nTotalCharges\nChurn\n\n\n\n\n488\n4472-LVYGI\nFemale\n0\nYes\nYes\n0\nNo\nNo phone service\nDSL\nYes\n...\nYes\nYes\nYes\nNo\nOne year\nYes\nBank transfer (automatic)\n52.55\n\nNo\n\n\n753\n3115-CZMZD\nMale\n0\nNo\nYes\n0\nYes\nNo\nNo\nNo internet service\n...\nNo internet service\nNo internet service\nNo internet service\nNo internet service\nTwo year\nNo\nMailed check\n20.25\n\nNo\n\n\n936\n5709-LVOEQ\nFemale\n0\nYes\nYes\n0\nYes\nNo\nDSL\nYes\n...\nNo\nNo\nYes\nYes\nTwo year\nNo\nMailed check\n80.85\n\nNo\n\n\n1082\n4367-NUYAO\nMale\n0\nYes\nYes\n0\nYes\nYes\nNo\nNo internet service\n...\nNo internet service\nNo internet service\nNo internet service\nNo internet service\nTwo year\nNo\nMailed check\n25.75\n\nNo\n\n\n1334\n1768-ZAIFU\nFemale\n1\nNo\nNo\n0\nNo\nNo phone service\nDSL\nNo\n...\nNo\nNo\nNo\nNo\nMonth-to-month\nYes\nElectronic check\n25.20\n\nYes\n\n\n\n\n5 rows × 21 columns\n\n\n\nWhen the tenure value is 0, the TotalCharges value is empty (‘space’). This is because customers who have just joined (less than a month) have not been charged yet, resulting in a TotalCharges value of 0. Since there are only 12 rows with this condition, I will delete them.\n\n#drop rows that has empty TotalCharges.\ndf = df.drop(df.index[df.TotalCharges == ' ']).reset_index(drop=True)\n\n\n#check for duplicate data, if True then there's no duplicate.\ndf.customerID.nunique() == len(df) \n\nTrue\n\n\n\n#feature engineered 2 new features for the sake of easier analysis.\ndf['Services'] = df[['PhoneService','InternetService']].apply(\n                     lambda x: 'Both' if list(x).count('No') == 0 else\n                     'Internet Only' if x[0] == 'No' else 'Phone Only', axis=1)\n\ndf['TotalBenefits'] = df.loc[:,'OnlineSecurity':'StreamingMovies']\\\n                          .apply(lambda x: list(x).count('Yes'), axis=1)\n\nNew features explanation:  1.Services = Combined values of PhoneService and InternetService. 2.TotalBenefits = Sum of benefits taken on OnlineSecurity until StreamingMovies.\n\n#Change values of 1 and 0 to 'Yes' and 'No'\ndf['SeniorCitizen'] = df.SeniorCitizen.apply(lambda x: 'Yes' if x == 1 else 'No')\n\nChange numerical value to strings for simpler and consistent analysis.\n\n#drop useless column\ndf.drop(columns=['customerID'], inplace=True)\n\n\n#change columns object data type to numerical\ndf.tenure = df.tenure.astype('int64')\ndf.MonthlyCharges = df.MonthlyCharges.astype('float64')\ndf.TotalCharges = df.TotalCharges.astype('float64')\n\n\n#checking values of real totalcharges and calculated totalcharges\ndf['TotalChargesDiff'] = df[['tenure','MonthlyCharges','TotalCharges']].apply(\n                             lambda x: round(abs(1 - (x[0] * x[1] / x[2])) * 100, 3), axis=1)\n\nHere, I have created a new column called TotalChargesDiff to check the differences (%) between the actual TotalCharges value (from the dataset) and the calculated TotalCharges value (obtained by multiplying tenure with MonthlyCharges). If the difference is above 40%, I will consider those rows as invalid because the values of tenure and MonthlyCharges cannot be trusted.\n\ndf['TotalChargesDiff'].sort_values(ascending=False).head(10)\n\n0       13400.000\n5        1357.404\n18        788.048\n19        330.214\n3         213.633\n128        73.511\n47         72.615\n4631       64.286\n5802       63.380\n20         58.263\nName: TotalChargesDiff, dtype: float64\n\n\nYou can observe that some data points have a TotalChargesDiff that reaches hundreds or even thousands percent.\n\n#removing rows that have &gt; 40% TotalChargesDiff.\ndf = df[df.TotalChargesDiff &lt; 40].reset_index(drop=True)\ndf.drop(columns=['TotalChargesDiff'], inplace=True)\n\n\ndef numericategoric(df):\n    num = len(df._get_numeric_data().columns)\n    cat = len(df.columns) - num\n    print(\"TotalNumericalData = \" + str(num))\n    print(\"TotalCategoricalData = \" + str(cat))\n    print(\"Numerical = \" + str(list(df._get_numeric_data().columns )))\n    print(\"Categorical = \" + str(list(df.drop(df._get_numeric_data().columns, axis=1).columns)))\n\n\nnumericategoric(df)\n\nTotalNumericalData = 4\nTotalCategoricalData = 18\nNumerical = ['tenure', 'MonthlyCharges', 'TotalCharges', 'TotalBenefits']\nCategorical = ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'Churn', 'Services']\n\n\nShow numerical and categorical columns\n\n#assign categorical and numerical columns on different dataframe for easier analysis.\ndfnum = df._get_numeric_data()\ndfcat = df.drop(columns = dfnum.columns)\n\n\n\n\n\n\n\n#numerical columns describe.\ndfnum.describe()\n\n\n\n\n\n\n\n\ntenure\nMonthlyCharges\nTotalCharges\nTotalBenefits\n\n\n\n\ncount\n7012.000000\n7012.000000\n7012.000000\n7012.000000\n\n\nmean\n32.506560\n64.732760\n2286.410207\n2.042356\n\n\nstd\n24.564234\n30.109753\n2265.759401\n1.835886\n\n\nmin\n1.000000\n12.000000\n13.500000\n0.000000\n\n\n25%\n9.000000\n35.450000\n402.437500\n0.000000\n\n\n50%\n29.000000\n70.300000\n1397.250000\n2.000000\n\n\n75%\n56.000000\n89.850000\n3784.125000\n3.000000\n\n\nmax\n140.000000\n118.750000\n8684.800000\n6.000000\n\n\n\n\n\n\n\n1.All columns seems to have a normal min-max values. Nothing weird here. 2.Average tenure is about 30 months which is pretty low. 3.Average MonthlyCharge is about 65-70 USD which is pretty good. 4.Out of 6 benefits available, the average taken by customer is around 2, which is pretty low.\n\ndfcat.describe()\n\n\n\n\n\n\n\n\ngender\nSeniorCitizen\nPartner\nDependents\nPhoneService\nMultipleLines\nInternetService\nOnlineSecurity\nOnlineBackup\nDeviceProtection\nTechSupport\nStreamingTV\nStreamingMovies\nContract\nPaperlessBilling\nPaymentMethod\nChurn\nServices\n\n\n\n\ncount\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n\n\nunique\n2\n2\n2\n2\n2\n3\n3\n3\n3\n3\n3\n3\n3\n3\n2\n4\n2\n3\n\n\ntop\nMale\nNo\nNo\nNo\nYes\nNo\nFiber optic\nNo\nNo\nNo\nNo\nNo\nNo\nMonth-to-month\nYes\nElectronic check\nNo\nBoth\n\n\nfreq\n3542\n5874\n3624\n4920\n6336\n3372\n3087\n3486\n3075\n3066\n3459\n2798\n2769\n3850\n4155\n2354\n5155\n4821\n\n\n\n\n\n\n\n1.Male and Female has the same proportion. 2.Most of customers is non SeniorCitizen with no Partner and No Dependents. 3.Favorite InternetService is Fiber optic. 4.Majority of customers is subscribed with ‘Month-to-month’ Contract. 5.5155 out of 7012 is non-Churn customers which make this dataset is imbalanced. 6.All these columns have low cardiality values.\n\n\n\n\nfig, axarr = plt.subplots(1,4, figsize=(10, 4))\nfor x in dfnum.columns:\n    axarr[dfnum.columns.get_loc(x)].boxplot(df[x],patch_artist=True)\n    axarr[dfnum.columns.get_loc(x)].set_xlabel(x)\nplt.suptitle('Outliers checking on numeric columns')\nfig.tight_layout(pad=1)\nplt.show()\n\n\n\n\nWill drop outlier in tenure.\n\n#drop outlier in tenure\ndf = df[df.tenure &lt; 125]\n\n\n#plot distribution for numerical columns\nfig, axarr = plt.subplots(1,4, figsize=(10, 4))\nfor x in dfnum.columns:\n    sns.histplot(data=dfnum[x], \n                 color='skyblue', \n                 kde=True, \n                 edgecolor='none', \n                 ax=axarr[dfnum.columns.get_loc(x)])\nplt.suptitle('Distribution plot', weight='bold')\nfig.tight_layout(pad=1)\n\n\n\n\nAbove plots are distribution plots on all numerical columns. 1.tenure and MonthlyCharges have a ‘U-shaped’ distribution. 2.TotalCharges has a positive-skew distribution.\n\n#count plot for categorical columns\nplt.figure(figsize=(15,12))\n\nfeatures = dfcat.columns\nfor i in np.arange(1, len(features)+1):\n    plt.subplot(5, len(features)//3 - 2, i)\n    sns.countplot(x=df[features[i-1]], color='green')\n    plt.xticks(rotation=10)\n    plt.xlabel(features[i-1])\nplt.suptitle('CountPlot', size=19, weight='bold')\nplt.tight_layout(pad = 1)\n\n\n\n\nHere I count plotted all categorical columns.\n\n\n\n\n#count plots against 'churn'\nplt.figure(figsize=(15,12))\n\nfeatures = dfcat.columns\nfor i in np.arange(1, len(features)+1):\n    plt.subplot(5, len(features)//3 - 2, i)\n    sns.countplot(data=df, x=df[features[i-1]], hue='Churn')\n    plt.legend(prop={'size': 8})\n    plt.xticks(rotation=10)\n    plt.xlabel(features[i-1])\nplt.suptitle('CountPlot vs Churned', size=19, weight='bold')\nplt.tight_layout(pad = 1)\n\n\n\n\nI added Churn count into the categorical plots. 1.You can see for column Gender, the values and Churn count is pretty equal which make this column will have a very low predictive power. 2.For InternetService, fiber optic has way higher in churn probability compare to DSL. 3.Same ways also applied on Month-to-month Contract and Electronic-check PaymentMethod.\n\n#distribution plots against 'churn'\nfig, axarr = plt.subplots(1,4, figsize=(10, 4))\nfor x in dfnum.columns:\n    sns.histplot(data=df, \n                 x = dfnum[x],\n                 color='skyblue', \n                 kde=True, \n                 edgecolor='none', \n                 ax=axarr[dfnum.columns.get_loc(x)], \n                 hue='Churn')\n    \nplt.suptitle(\"Distribution plot\", weight='bold')\nfig.tight_layout(pad=1)\n\n\n\n\nLet’s also compare Churn distribution on numerical columns. Customers tend to churn when the tenure is low and not churn when the MonthlyCharges is very low. I will do further analysis about these columns later.\n\n#change binary column into numerical\nbinary = ['gender','SeniorCitizen','Partner','Dependents','PhoneService','PaperlessBilling','Churn']\nvalue_mapping = {\n    'No': 0,\n    'Yes' : 1,\n    'Male' : 1,\n    'Female' : 0\n}\ndfcorr = df[binary]\nfor col in binary:\n    dfcorr[col] = dfcorr[col].map(value_mapping).astype('int64')\n\n\nplt.figure(figsize=(10,8))\nsns.heatmap(dfcorr.corr(), annot=True, fmt='.2f')\nplt.show()\n\n\n\n\nCorrelation! 1.TotalCharges and tenure have a high positive correlation (causation : the longer the customers subscribed, the more they paid). 2.TotalBenefits also has a strong correlation with MonthlyCharges and TotalCharges (causation : more benefits taken also make the MonthlyCharges higher). Note : Correlation doesn’t indicate causation. Understanding the specific context, industry knowledge, and conducting further analysis or experiments can help determine if there is a causal relationship between the variables or if other factors are influencing the observed correlations.\n\n\n\n\n\n#create a function to plot churn probability for numerical columns.\ndef prob_plot(df,colom,x):\n    means = df[colom].mean()\n    medians = df[colom].median()\n    data = df[df.Churn == 'Yes'][colom].astype('float64')\n    data1 = df[df.Churn == 'No'][colom].astype('float64')\n    \n    kde = gaussian_kde(data)\n    kde1 = gaussian_kde(data1)\n    dist_space = np.linspace( min(data), max(data), 200)\n    dist_space1 = np.linspace( min(data1), max(data1), 200)\n    axarr[x].plot( dist_space, kde(dist_space), label='Churned', color='orange' )\n    axarr[x].plot( dist_space1, kde1(dist_space1), label='Not churn', color='blue')\n    axarr[x].axvline(x = means, linestyle = '--', color='g', label='Mean')\n    axarr[x].axvline(x = medians, linestyle = '--', color='r', label='Median')\n    axarr[x].set_title('Probability', fontweight='bold', size=12)\n    axarr[x].set(ylabel = 'Probability', xlabel = colom)\n    axarr[x].legend()\n\n\n\n\n#count plot for Services.\nfig, axarr = plt.subplots(1,2, figsize=(12, 6))\nsns.countplot(data=df[df.Services == 'Both'], \n              x='InternetService', \n              ax=axarr[0])\n\nsns.countplot(data=df[df.Services == 'Internet Only'], \n              x='InternetService', \n              ax=axarr[1])\n\naxarr[0].set_title(\"Both phone service & internet service\", weight='bold')\naxarr[1].set_title(\"Internet service only\", weight='bold')\n\nplt.suptitle(\"Comparison of internet services on product services\")\nplt.tight_layout(pad=1)\nplt.show()\n\n\n\n\nIt is observed that customers tend to prefer Fiber optic over DSL for ‘phone & internet service’. However, when considering ‘internet service only’ without phone, there is no option for Fiber optic available. This suggests that in order to utilize Fiber optic, a phone connection (or phone service) is required.\n\n#plot churn probability for 'services' & 'internet services'\nfig, axarr = plt.subplots(1,2, figsize=(12, 6))\n\ndf.groupby('Services')['Churn'].agg(lambda x: list(x).count('Yes') * 100 / len(list(x)))\\\n                               .plot(kind='bar', width=0.3,rot=True, ax=axarr[0])\n\ndf[df.Services != 'Phone Only'].groupby(['InternetService'])['Churn'].agg(lambda x: list(x).count('Yes') * 100 / len(x))\\\n                               .plot(kind='bar', width=0.3,rot=True, ax=axarr[1])\n\naxarr[0].set_title('Services churn probability', weight='bold')\naxarr[1].set_title('Internet service churn probability', weight='bold')\n\nplt.tight_layout(pad=1)\nplt.show()\n\n\n\n\nCustomers that used ‘Both’ services and InternetService fiber optic tends to churn more.\n\n\n\n\n#checking InternetService price.\ndf_filtered = df[(df.Services == 'Both') & (df.TotalBenefits == 0) & (df.MultipleLines == 'No')].copy()\ndf_filtered.groupby('InternetService')['MonthlyCharges'].mean()\n\nInternetService\nDSL            44.965089\nFiber optic    70.074454\nName: MonthlyCharges, dtype: float64\n\n\n\nThe price of Fiber optic is higher, around 25 USD, compared to DSL. However, it’s important to keep in mind that these prices include a phone service with a single line.\n\n\ndf_filtered = df[(df.Services == 'Phone Only') & (df.TotalBenefits == 0)].copy()\ndf_filtered.groupby('MultipleLines')['MonthlyCharges'].mean()\n\nMultipleLines\nNo     19.958088\nYes    24.980060\nName: MonthlyCharges, dtype: float64\n\n\n\nplt.figure(figsize=(10,5))\nsns.histplot(data=df_filtered, \n             x='MonthlyCharges', \n             hue='MultipleLines', \n             multiple='stack')\n\nplt.title('Phone service distribution', weight='bold')\nplt.tight_layout()\nplt.show()\n\n\n\n\nWe can see that the price for a ‘phone service’ with a single line is around 20 USD, while the price for a ‘phone service’ with multiple lines is around 25 USD. This also means that the price for DSL is around 25 USD, while the price for Fiber optic is around 50 USD, which is twice as much as DSL.\n\nfig, axarr = plt.subplots(1,2, figsize=(12, 6))\nmc_dist = sns.histplot(data=df, \n                       x = 'MonthlyCharges',\n                       hue='Churn', \n                       ax=axarr[0], \n                       multiple='stack')\n\naxarr[0].set_title('Distribution', fontweight='bold', size=12)\n\nprob_plot(df,'MonthlyCharges',1)\naxarr[1].legend(loc='upper right')\nplt.show()\n\n\n\n\nAt a MonthlyCharges range of approximately +- 20 USD, the ratio of non-churn customers is very high. It is known that products within this price range are typically ‘phone service only’. However, between the price range of 60 - 100 USD, the churn probability increases significantly. I am planning to conduct further analysis specifically for customers within this price range.\n\ndf_filtered = df[(df.InternetService == 'Fiber optic') & (df.Services == 'Both') & (df.MultipleLines == 'No')]\ndf_agg = df_filtered.groupby('TotalBenefits')['MonthlyCharges'].agg('mean').reset_index()\ndf_agg['MonthlyCharges'] = df_agg.MonthlyCharges.round()\n\n\nfig, axarr = plt.subplots(1,2, figsize=(12, 6))\nsns.scatterplot(data=df_filtered, \n                x='MonthlyCharges', \n                y='TotalBenefits', \n                s=35, \n                ax=axarr[0])\n\nsns.barplot(df_agg, x = 'TotalBenefits', y = 'MonthlyCharges')\naxarr[0].set_title('MonthlyCharges vs Totalbenefits', weight='bold')\naxarr[1].set_title('Average MonthlyCharges vs Totalbenefits', weight='bold')\nfig.tight_layout(pad = 1)\nplt.show()\n\n\n\n\nHere, you can observe that as more TotalBenefits are taken, the MonthlyCharges also increase. On the left plot, you can see that there are 5 outlier data points, which will be removed later.\n\ndf_fil = df[(df.InternetService == 'Fiber optic') & (df.Services == 'Both')\\\n              & (df.MultipleLines == 'No') & (df.TotalBenefits == 1)]\n\nbenefits = ['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']\n\ndf_benefit = pd.DataFrame()\nfor x in benefits:\n    df_value = pd.DataFrame([x, \n                      df_fil[df_fil[x] == 'Yes']['MonthlyCharges'].min(), \n                      df_fil[df_fil[x] == 'Yes']['MonthlyCharges'].max(), \n                      df_fil[df_fil[x] == 'Yes']['MonthlyCharges'].mean(), \n                      df_fil[df_fil[x] == 'Yes']['MonthlyCharges'].median()\n                     ]).transpose()\n    \n    df_benefit = pd.concat([df_benefit, df_value])\ndf_benefit.columns = ['Benefit','MinCharges','MaxCharges','MeanCharges','MedianCharges']    \n\n\ndf_benefit\n\n\n\n\n\n\n\n\nBenefit\nMinCharges\nMaxCharges\nMeanCharges\nMedianCharges\n\n\n\n\n0\nOnlineSecurity\n73.2\n76.45\n74.795588\n74.95\n\n\n0\nOnlineBackup\n72.75\n76.65\n74.708511\n74.65\n\n\n0\nDeviceProtection\n69.55\n76.65\n74.604762\n74.9\n\n\n0\nTechSupport\n73.85\n76.55\n75.045455\n74.7\n\n\n0\nStreamingTV\n77.65\n81.9\n79.749242\n79.825\n\n\n0\nStreamingMovies\n12.0\n86.1\n79.045902\n80.15\n\n\n\n\n\n\n\nAbove are the prices of benefits with Fiber optic and a single line phone connection. You can see that StreamingTV and StreamingMovies are more expensive compared to other benefits, approximately +- 5 USD.\nNow that we know the prices of every product, here’s a recap:\nDSL = approximately 25 USD. Fiber optic = approximately 50 USD. Phone service (single line) = approximately 20 USD. Phone service (multiple lines) = approximately 25 USD. OnlineSecurity - TechSupport = approximately 5 USD. StreamingTV - StreamingMovies = approximately 10 USD.\nWith this data, we can perform a simple ‘anomaly detection’ by manually calculating the MonthlyCharges and comparing them with the actual MonthlyCharges, similar to how we calculated the TotalChargesDiff above.\n\n#checking for MonthlyCharges values with the calculated one (similar with checking TotalCharges difference).\ndef MonthlyChargesDiff(x):\n    estimation = 0\n    if x['PhoneService'] == 'Yes':\n        estimation += 20\n    if x['MultipleLines'] == 'Yes':\n        estimation += 5\n    if x['InternetService'] == 'DSL':\n        estimation += 25\n    if x['InternetService'] == 'Fiber optic':\n        estimation += 50\n        \n    if (x['StreamingTV'] == 'Yes') & (x['StreamingMovies'] == 'Yes'):\n        estimation += 20 + (x['TotalBenefits'] - 2) * 5\n    elif (x['StreamingTV'] == 'Yes') | (x['StreamingMovies'] == 'Yes'):\n        estimation += 10 + (x['TotalBenefits'] - 1) * 5\n    else:\n        estimation += x['TotalBenefits'] * 5\n        \n    return abs(1 - (estimation / x['MonthlyCharges'])) * 100\n    \n   \n\n\ndf['MonthlyChargesEstimationDifference'] = df.apply(MonthlyChargesDiff, axis=1)\n\n\ndf[df.MonthlyChargesEstimationDifference &gt; 40][['MonthlyCharges','MonthlyChargesEstimationDifference']]\n\n\n\n\n\n\n\n\nMonthlyCharges\nMonthlyChargesEstimationDifference\n\n\n\n\n12\n29.00\n296.551724\n\n\n389\n12.00\n733.333333\n\n\n666\n12.00\n566.666667\n\n\n859\n26.41\n278.644453\n\n\n1439\n18.26\n447.645126\n\n\n2185\n21.63\n362.320851\n\n\n4090\n31.26\n219.897633\n\n\n5848\n15.00\n466.666667\n\n\n6718\n21.00\n304.761905\n\n\n\n\n\n\n\nYou can see that there are 9 rows with extreme MonthlyCharges values. These are considered as ‘anomalies’, so let’s remove them.\n\n#remove MonthlyCharges extreme values.\ndf = df[df.MonthlyChargesEstimationDifference &lt; 40].reset_index(drop=True)\n\n\n\n\n\n#creating a function to engineered a new feature.\ndef statuss(x):\n    x = list(x)\n    if (x[0] == 'Yes') & (x[1] == 'Yes'):\n        return 'Both'\n    elif (x[0] == 'Yes') & (x[1] == 'No'):\n        return 'Partner Only'\n    elif (x[0] == 'No') & (x[1] == 'Yes'):\n        return 'Dependent Only'\n    else:\n        return 'Single'\n\n\ndf['Status'] = df[['Partner','Dependents']].apply(statuss, axis=1)\n\nI have created a new feature called ‘Status’. This feature is derived from the columns Partner and Dependents. 1.If customers have both Partner and Dependents, it will be labeled as ‘Both’. 2.If customers have Partner but no Dependents, it will be labeled as ‘Partner Only’. 3.If customers have Dependents but no Partner, it will be labeled as ‘Dependent Only’. 4.If customers have neither Partner nor Dependents, it will be labeled as ‘Single’.\n\nplt.figure(figsize=(10,5))\nsns.countplot(df.sort_values('Status', ascending=True), x='Status')\nplt.title('Status Count', size=16, weight='bold')\nplt.tight_layout()\nplt.show()\n\n\n\n\nMajority of customers are Single.\n\nfig, axarr = plt.subplots(1,3, figsize=(15, 6))\nk = ['tenure','MonthlyCharges','TotalBenefits']\nfor x in k:\n    sns.barplot(data=df.groupby(['Status'])[[x]].mean().reset_index(), \n                x='Status', \n                y=x, \n                ax=axarr[k.index(x)],\n                palette=['grey', 'g','m','b'])\n    \n    axarr[k.index(x)].set_title(f'{x} average', weight='bold', size=15)\n    \nfig.tight_layout()\nplt.show()\n\n\n\n\nCustomers labeled as ‘Partner Only’ are considered the best since they have the longest tenure and the highest MonthlyCharges. The second-best group is ‘Both’, although these customers may not have MonthlyCharges as high as those in the ‘Single’ group, their tenure is almost double that of the ‘Single’ group.\n\nplt.figure(figsize=(10,5))\nsns.barplot(data=df.groupby('Status')[['Churn']].agg(lambda x: list(x).count('Yes') / len(x)).reset_index(), \n            x='Status', \n            y='Churn')\n\nplt.title('Churn Probability', weight='bold', size=16)\nplt.tight_layout()\nplt.show()\n\n\n\n\nSingle customers have the highest churn probability.\n\ndf.SeniorCitizen.value_counts(normalize=True)\n\nSeniorCitizen\nNo     0.837903\nYes    0.162097\nName: proportion, dtype: float64\n\n\nThe majority of customers are young people.\n\ndf_status = df.groupby('SeniorCitizen')[['Status']]\ndf_status = df_status.agg(Single = ('Status', lambda x: list(x).count('Single') * 100 / len(x)), \n                          PartnerOnly = ('Status', lambda x: list(x).count('Partner Only') * 100  / len(x)), \n                          Both = ('Status', lambda x: list(x).count('Both') * 100  / len(x)), \n                          DependentOnly = ('Status', lambda x: list(x).count('Dependent Only') * 100  / len(x)))\ndf_status = df_status.reset_index().melt(id_vars='SeniorCitizen')\ndf_status = df_status.rename(columns={'variable':'Status'})\n\n\nplt.figure(figsize=(10,5))\n\nsns.barplot(data=df_status, \n            x ='SeniorCitizen', \n            y='value', \n            hue='Status')\n\nplt.title(\"Status comparison between senior citizen\", size=15, weight='bold')\nplt.tight_layout()\nplt.show()\n\n\n\n\nFor non-Senior citizens, ‘Single’ customers have the highest frequency, followed by ‘Both’. For Senior citizens, ‘Single’ is also the highest category, but the difference with ‘PartnerOnly’ is not as significant. From the plots above, we can also conclude that young people tend to have dependents more than older people.\n\ndf_status = df.groupby('Status')[['OnlineSecurity','OnlineBackup',\n                                  'DeviceProtection','TechSupport','StreamingTV','StreamingMovies']]\\\n                                  .agg(lambda x: list(x).count('Yes'))\n    \ndf_status['total'] = df_status.apply('sum',axis=1)\n\nfor x in df_status.drop(columns='total').columns:\n    df_status[x] = (df_status[x] * 100 / df_status.total).round()\ndf_status.drop(columns='total', inplace=True)\n\n\ndf_status.plot(kind='bar', rot=0)\nplt.title('Benefit count comparison between status (%)', size=12, weight='bold')\nplt.legend(bbox_to_anchor=(1, 1))\nplt.show()\n\n\n\n\nSingle and PartnerOnly customers tend to prefer entertainment benefits such as StreamingTV and StreamingMovies compared to other customers. Additionally, these customers show a lower preference for using TechSupport and OnlineSecurity.\n\n\n\n\nfig, axarr = plt.subplots(1,2, figsize=(12, 6))\nsns.countplot(df[df.Services != 'Phone Only'], \n              x = 'TotalBenefits', ax=axarr[0])\n\nsns.barplot(df[['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']]\\\n            .apply(lambda x: list(x).count('Yes')).reset_index(), x = 'index', y = 0, ax=axarr[1])\n\naxarr[0].set_title('TotalBenefits Count', size=12, weight='bold')\naxarr[0].set(ylabel=None)\n\naxarr[1].set(ylabel=None)\naxarr[1].set_title('Benefits Count', size=12, weight='bold')\naxarr[1].tick_params(axis='x', rotation=25)\nfig.tight_layout()\nplt.show()\n\n\n\n\nThe average number of TotalBenefits taken by customers is around 3, with StreamingTV and StreamingMovies being the most popular choices.\n\ndf_total = pd.DataFrame()\nfor x in ['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']:\n    total = list(df[df[x] == 'Yes']['Churn']).count('Yes') / len(df[df[x] == 'Yes']['Churn'])\n    df_total = pd.concat([df_total, pd.DataFrame([x],[total])])\n\n\nfig, axarr = plt.subplots(1,2, figsize=(12, 6))\nsns.barplot(df.groupby('TotalBenefits')[['Churn']]\\\n            .agg(lambda x: list(x).count('Yes') * 100 / len(x)).round().reset_index(), \n            x='TotalBenefits', \n            y='Churn', ax=axarr[0])\n\nsns.barplot(df_total.reset_index(), \n            x=0, \n            y='index', \n            ax=axarr[1])\n\naxarr[0].set_title('TotalBenefits Churn probability', size=12, weight='bold')\naxarr[0].set(ylabel=None)\n\naxarr[1].set(ylabel=None, xlabel='Benefits')\naxarr[1].set_title('Benefits Churn probability', size=12, weight='bold')\naxarr[1].tick_params(axis='x', rotation=25)\nfig.tight_layout()\nplt.show()\n\n\n\n\nWhile StreamingTV and StreamingMovies are the most favored choices, the churn probability associated with them is also the highest.\n\n\n\n\nfig, axarr = plt.subplots(1, figsize=(10, 6))\nsns.scatterplot(data=df, \n                x = 'MonthlyCharges', \n                y = 'tenure', \n                hue ='Churn', \n                s=20)\n\nplt.fill_between((68 , 97),20, alpha=0.2, color='green')\nplt.fill_between((68 , 97),52.5, 72.5, alpha=0.2, color='blue')\n\nplt.title('MonthlyCharges vs tenure vs Churn', size = 15, weight = 'bold')\nplt.show()\n\n\n\n\nHere, I have created two areas, denoted by green and blue, both focusing on MonthlyCharges in the range of 70 - 95 USD. This price range corresponds to the highest churn probability. The green area represents customers with low tenure and is predominantly occupied by churned customers, while the blue area represents customers with high tenure and is predominantly occupied by non-churned customers.\n\nplt.subplots(1, figsize=(15, 8))\nsns.scatterplot(data=df, \n                x = 'MonthlyCharges', \n                y = 'tenure', \n                s=35, \n                hue='TotalBenefits', \n                style='InternetService', \n                palette='coolwarm')\n\nplt.fill_between((68 , 97),20, alpha=0.15, color='green')\nplt.fill_between((68 , 97),52.5, 72.5, alpha=0.15, color='blue')\n\nplt.title('MonthlyCharges vs tenure vs TotalBenefits vs InternetService', size = 15, weight = 'bold')\nplt.show()\n\n\n\n\nStill on the same plot, I have added TotalBenefits and InternetService. It can be observed that in the green area, Fiber optic is the dominant InternetService with low TotalBenefits. On the other hand, the blue area is dominated by DSL with high TotalBenefits. This indicates that customers, at the same price point, tend to choose DSL with high TotalBenefits rather than Fiber optic with low TotalBenefits. Note that Fiber optic prices are doubled than DSL.\nWith the observed pattern above, we can create an important new feature, which we will refer to as ‘FO_LB’ (Fiber optic_Low benefit). I will assign a value of ‘1’ to indicate that the internet service is Fiber optic and the Totalbenefits taken are less than or equal to 3. For other cases, I will assign ‘0’.\n\ndf['FO_LB'] = df[['InternetService','TotalBenefits']].apply(\n    lambda x: 1 if (x['InternetService'] == 'Fiber optic') & (x['TotalBenefits'] &lt;= 3) else 0, axis=1)\n\n\n\n\n\nOne important thing to address before we proceed is considering the types of errors to make this project as realistic as possible. Typically, there are two types of errors: false positive (FP) and false negative (FN). However, in this project, I will introduce three types of errors.\n1.FP: False positive  2.FN1: False negative for customers with MonthlyCharges below 95 USD  3.FN2: False negative for customers with MonthlyCharges above 95 USD (VIP customers)  Let’s agree on the misclassification ratio, which is FP:FN1:FN2 = 1:3:5 \nIt’s important to note that this dataset is imbalanced, meaning there is a significant difference in the number of samples between the classes.\nBased on these problems, we can set up our model’s parameters as follows: 1.Hyperparameter tuning. 2.Decision threshold tuning. 3.Oversampling data using SMOTE. 4.Applying weights to the models. I will be using Random Forest, XGBoost, and Logistic Regression.\nMetrics: Custom scoring based on sample misclassification. Precision. Recall. F1_score. Once the models are evaluated using these metrics, I will interpret the best model.\n\n\n\ndf1 = df.copy()\n\n\ndf = df1.copy()\n\n\n#drop columns\ndf.drop(columns = ['Services','MonthlyChargesEstimationDifference','Status','PaperlessBilling','PaymentMethod'], inplace=True)\n\nI have dropped the columns from ‘Services’ to ‘Status’ as these columns were engineered features created for simpler exploratory data analysis (EDA). Additionally, I have also dropped the ‘PaperlessBilling’ and ‘PaymentMethod’ columns because, in the business context, these columns are considered irrelevant for determining customer churn since they represent optional ‘features’ for customers.\n\n#converting 'No internet service' to 'No' in benefit columns.\nKolomBenefit = ['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']\n\nfor x in KolomBenefit:\n    df[x] = df[x].apply(lambda x: 'No' if x == 'No internet service' else x)\n\n\n#converting 'No phone service' to 'No'\ndf['MultipleLines'] = df[x].apply(lambda x: 'No' if x == 'No phone service' else x)\n\n\n#dict to mapping string to numerical.\nvalue_mapping = {\n    'No': 0,\n    'Yes' : 1,\n    'Male' : 1,\n    'Female' : 0\n}\n\n\n#binary encoding\nbinary = list(df.drop(columns=['tenure','InternetService','MonthlyCharges','TotalCharges','TotalBenefits','Contract','FO_LB']).columns)\n\nfor col in binary:\n    df[col] = df[col].map(value_mapping).astype('int64')\n\n\n#label encoding\ndf['Contract'] = df['Contract'].apply(lambda x: 1 if x == 'Month-to-month' else 2 if x == 'One year' else 3) \n\n\n#one hot encoding\ndf = pd.get_dummies(df, columns=['InternetService'])\n\n\n#feature selection\ndf = df[['Contract','tenure','InternetService_Fiber optic','MonthlyCharges','FO_LB','InternetService_No','Churn']]\ndf = df.rename(columns={\n                'InternetService_Fiber optic':'Fiber_optic',\n                'InternetService_No':'No_internet'})\n\nHere I only choose a feature that have a strong predictive power (by using feature of importances)\n\n\n\n\n#split train-test\nX_train, X_test, y_train, y_test = train_test_split(df.drop(columns='Churn'), df.Churn.to_numpy(), test_size = 0.2, random_state=123)\nX_train = X_train.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)\n\n\n#custom function to do threshold tuning and custom metrics (used in GridSearchCV)\ndef my_scorer_2(clf, X, y_true, thres = np.arange(0.1,1,0.1)):\n    result_dict = {}\n    for threshold in np.atleast_1d(thres):\n        y_pred = (clf.predict_proba(X)[:,1] &gt; threshold).astype(int)\n        X_segment = (X['MonthlyCharges'] &gt; 95).to_numpy().astype(int)\n        y_stack = np.column_stack((X_segment, y_pred, y_true))\n        y_stack_reg, y_stack_vip = y_stack[y_stack[:,0] == 0], y_stack[y_stack[:,0] == 1]\n        cm_reg = confusion_matrix(y_stack_reg[:,2], y_stack_reg[:,1])\n        cm_vip = confusion_matrix(y_stack_vip[:,2], y_stack_vip[:,1])\n        fn_reg, fn_vip = cm_reg[1][0], cm_vip[1][0]\n        fp = cm_reg[0][1] + cm_vip[0][1]\n        loss_score = (fp * 1) + (fn_reg * 3) + (fn_vip * 5)\n        result_dict[threshold] = np.array([loss_score, metrics.precision_score(y_true, y_pred, zero_division = 0), \n                                           metrics.recall_score(y_true, y_pred), metrics.f1_score(y_true, y_pred)])\n        \n    result_np = np.array([np.insert(value, 0, key) for key, value in result_dict.items()])\n    best_np = result_np[result_np[:,1] == np.min(result_np[:,1])][0]\n    return best_np\n\ndef my_scorer_threshold(clf, X, y_true):\n    return my_scorer_2(clf, X, y_true)[0]\n\ndef my_scorer_ls(clf, X, y_true):\n    return my_scorer_2(clf, X, y_true)[1]\n\ndef my_scorer_precision(clf, X, y_true):\n    return my_scorer_2(clf, X, y_true)[2]\n\ndef my_scorer_recall(clf, X, y_true):\n    return my_scorer_2(clf, X, y_true)[3]\n\ndef my_scorer_f1(clf, X, y_true):\n    return my_scorer_2(clf, X, y_true)[4]\n        \n        \n\n\n#Grid scoring parameter\ngrid_scoring = {\n    'threshold': my_scorer_threshold,\n    'loss_score': my_scorer_ls,\n    'precision': my_scorer_precision,\n    'recall': my_scorer_recall,\n    'f1': my_scorer_f1\n}\n\n\n#define weight by missclassification cost which is FP:FN1:FN2 = 1:3:5\nX_segment = (X_train['MonthlyCharges'] &gt; 95).to_numpy().astype(int)\narr_weight = np.column_stack((X_segment, y_train))\nweight = np.apply_along_axis(lambda x: 1 if x[1] == 0 else 5 if x[0] == 1 else 3 , axis=1, arr=arr_weight)\n\n\n\n\n\n#RANDOM FOREST MODELLING\n\n#define parameter for tuning\nparam_grid_rf = {\n    'n_estimators': [250 , 400],\n    'max_depth': [10, 25, 50],\n    'min_samples_split': [25, 50, 70, 120],\n    'min_samples_leaf': [50, 75, 120],\n    'bootstrap' : [True, False]\n}\n\n#run grid search cv\nrf = GridSearchCV(estimator = RandomForestClassifier(),\n                  param_grid = param_grid_rf,\n                  cv=5,\n                  scoring = grid_scoring, refit=False, n_jobs = -1)\n\n#fit grid search cv with train data\nrf.fit(X_train, y_train)\n\n#selecting the best parameter and metrics\ngrid_result = pd.DataFrame(rf.cv_results_)\ngrid_result = grid_result[grid_result['mean_test_loss_score'] == grid_result['mean_test_loss_score'].min()][['params','mean_test_threshold','mean_test_loss_score','mean_test_precision','mean_test_recall','mean_test_f1']]\n\n#train model with the best hyperparameter\nrf = RandomForestClassifier(**grid_result.iloc[0,0])\n\n#train model with train data\nrf = rf.fit(X_train, y_train)\n\n#evaluate the model\nmb1_rf = my_scorer_2(rf, X_test, y_test, grid_result.iloc[0,1])\nmb1_rf = np.append(mb1_rf, grid_result.iloc[0,0])\n\n\n#XGBOOST MODELLING\n\n#define parameter for tuning\nparam_grid_xg = {\n    'learning_rate': [0.1, 0.01, 0.001],\n    'n_estimators': [100, 500],\n    'max_depth': [5, 10, 25],\n    'subsample': [0.8, 0.9, 1.0],\n    'colsample_bytree': [0.8, 0.9, 1.0]\n}\n\n#run grid search cv\nxg = GridSearchCV(estimator = XGBClassifier(),\n                  param_grid = param_grid_xg,\n                  cv=5,\n                  scoring = grid_scoring, refit=False, n_jobs = -1)\n\n#fit grid search cv with train data\nxg.fit(X_train, y_train)\n\n#selecting the best parameter and metrics\ngrid_result = pd.DataFrame(xg.cv_results_)\ngrid_result = grid_result[grid_result['mean_test_loss_score'] == grid_result['mean_test_loss_score'].min()][['params','mean_test_threshold','mean_test_loss_score','mean_test_precision','mean_test_recall','mean_test_f1']]\n\n#train model with the best hyperparameter\nxg = XGBClassifier(**grid_result.iloc[0,0])\n\n#train model with train data\nxg = xg.fit(X_train, y_train)\n\n#evaluate the model\nmb1_xg = my_scorer_2(xg, X_test, y_test, grid_result.iloc[0,1])\nmb1_xg = np.append(mb1_xg, grid_result.iloc[0,0])\n\n\n#LOGISTIC REGRESSION MODELLING\n\n#define parameter for tuning\nparam_grid_lg = {\n    'penalty': ['l1', 'l2'], \n    'C': [0.1, 1.0, 10.0],  \n    'solver': ['liblinear'],  \n    'max_iter': [50,100,200] \n}\n\n#run grid search cv\nlg = GridSearchCV(estimator = LogisticRegression(),\n                  param_grid = param_grid_lg,\n                  cv=5,\n                  scoring = grid_scoring, refit=False, n_jobs = -1)\n\n#fit grid search cv with train data\nlg.fit(X_train, y_train)\n\n#selecting the best parameter and metrics\ngrid_result = pd.DataFrame(lg.cv_results_)\ngrid_result = grid_result[grid_result['mean_test_loss_score'] == grid_result['mean_test_loss_score'].min()][['params','mean_test_threshold','mean_test_loss_score','mean_test_precision','mean_test_recall','mean_test_f1']]\n\n#train model with the best hyperparameter\nlg = LogisticRegression(**grid_result.iloc[0,0])\n\n#train model with train data\nlg = lg.fit(X_train, y_train)\n\n#evaluate the model\nmb1_lg = my_scorer_2(lg, X_test, y_test, grid_result.iloc[0,1])\nmb1_lg = np.append(mb1_lg, grid_result.iloc[0,0])\n\n\nresult_mb1 = pd.DataFrame([mb1_rf,mb1_xg,mb1_lg])\nresult_mb1.columns = ['threshold','score','precision','recall','f1','params']\nresult_mb1['model'] = ['MB1_RF','MB1_XG','MB1_Log_Reg']\nresult_mb1\n\n\n\n\n\n\n\n\nthreshold\nscore\nprecision\nrecall\nf1\nparams\nmodel\n\n\n\n\n0\n0.24\n539.0\n0.473101\n0.828255\n0.602216\n{'bootstrap': True, 'max_depth': 25, 'min_samp...\nMB1_RF\n\n\n1\n0.20\n558.0\n0.449190\n0.844875\n0.586538\n{'colsample_bytree': 0.8, 'learning_rate': 0.0...\nMB1_XG\n\n\n2\n0.22\n541.0\n0.467492\n0.836565\n0.599801\n{'C': 0.1, 'max_iter': 50, 'penalty': 'l2', 's...\nMB1_Log_Reg\n\n\n\n\n\n\n\n\n\n\n\n#RANDOM FOREST MODELLING\n\n#define parameter for tuning\nparam_grid_rf_smote = {\n    'class__n_estimators': [250 , 400],\n    'class__max_depth': [10, 25, 50],\n    'class__min_samples_split': [25, 50, 70, 120],\n    'class__min_samples_leaf': [50, 75, 120],\n    'class__bootstrap' : [True, False]\n}\n\n#create imbalanced pipeline to SMOTE \npipelinerf = Pipeline([\n        ('sampling', SMOTE()),\n        ('class', RandomForestClassifier())])\n\n#run grid search cv\nrf = GridSearchCV(estimator = pipelinerf,\n                  param_grid = param_grid_rf_smote,\n                  cv=5,\n                  scoring = grid_scoring, refit=False, n_jobs = -1)\n\n#fit grid search cv with train data\nrf.fit(X_train, y_train)\n\n#selecting the best parameter and metrics\ngrid_result = pd.DataFrame(rf.cv_results_)\ngrid_result = grid_result[grid_result['mean_test_loss_score'] == grid_result['mean_test_loss_score'].min()][['params','mean_test_threshold','mean_test_loss_score','mean_test_precision','mean_test_recall','mean_test_f1']]\nparams = grid_result.iloc[0,0]\n\nparams_update = {}\nfor key, value in params.items():\n    new_key = key.replace('class__', '')\n    params_update[new_key] = value\nparams = params_update\n    \n#train model with the best hyperparameter\nrf = RandomForestClassifier(**params)\n\n#train model with SMOTE train data\nX_train1, y_train1 = SMOTE().fit_resample(X_train, y_train)\nrf = rf.fit(X_train1, y_train1)\n\n#evaluate the model\nmb2_rf = my_scorer_2(rf, X_test, y_test, grid_result.iloc[0,1])\nmb2_rf = np.append(mb2_rf, params)\n\n\n#XGBOOST MODELLING\n\n#define parameter for tuning\nparam_grid_xg_smote = {\n    'class__learning_rate': [0.1, 0.01, 0.001],\n    'class__n_estimators': [100, 500],\n    'class__max_depth': [5, 10, 25],\n    'class__subsample': [0.8, 0.9, 1.0],\n    'class__colsample_bytree': [0.8, 0.9, 1.0]\n}\n\n#create imbalanced pipeline to SMOTE \npipelinexg = Pipeline([\n        ('sampling', SMOTE()),\n        ('class', XGBClassifier())])\n\n#run grid search cv\nxg = GridSearchCV(estimator = pipelinexg,\n                  param_grid = param_grid_xg_smote,\n                  cv=5,\n                  scoring = grid_scoring, refit=False, n_jobs = -1)\n\n#fit grid search cv with train data\nxg.fit(X_train, y_train)\n\n#selecting the best parameter and metrics\ngrid_result = pd.DataFrame(xg.cv_results_)\ngrid_result = grid_result[grid_result['mean_test_loss_score'] == grid_result['mean_test_loss_score'].min()][['params','mean_test_threshold','mean_test_loss_score','mean_test_precision','mean_test_recall','mean_test_f1']]\nparams = grid_result.iloc[0,0]\n\nparams_update = {}\nfor key, value in params.items():\n    new_key = key.replace('class__', '')\n    params_update[new_key] = value\nparams = params_update\n    \n#train model with the best hyperparameter\nxg = XGBClassifier(**params)\n\n#train model with SMOTE train data\nX_train1, y_train1 = SMOTE().fit_resample(X_train, y_train)\nxg = xg.fit(X_train1, y_train1)\n\n#evaluate the model\nmb2_xg = my_scorer_2(xg, X_test, y_test, grid_result.iloc[0,1])\nmb2_xg = np.append(mb2_xg, params)\n\n\n#LOGISTIC REGRESSION MODELLING\n\n#define parameter for tuning\nparam_grid_lg_smote = {\n    'class__penalty': ['l1', 'l2'], \n    'class__C': [0.1, 1.0, 10.0],  \n    'class__solver': ['liblinear'],  \n    'class__max_iter': [50,100,200] \n}\n\n\n#create imbalanced pipeline to SMOTE \npipelinelg = Pipeline([\n        ('sampling', SMOTE()),\n        ('class', LogisticRegression())])\n\n#run grid search cv\nlg = GridSearchCV(estimator = pipelinelg,\n                  param_grid = param_grid_lg_smote,\n                  cv=5,\n                  scoring = grid_scoring, refit=False, n_jobs = -1)\n\n#fit grid search cv with train data\nlg.fit(X_train, y_train)\n\n#selecting the best parameter and metrics\ngrid_result = pd.DataFrame(lg.cv_results_)\ngrid_result = grid_result[grid_result['mean_test_loss_score'] == grid_result['mean_test_loss_score'].min()][['params','mean_test_threshold','mean_test_loss_score','mean_test_precision','mean_test_recall','mean_test_f1']]\nparams = grid_result.iloc[0,0]\n\nparams_update = {}\nfor key, value in params.items():\n    new_key = key.replace('class__', '')\n    params_update[new_key] = value\nparams = params_update\n    \n#train model with the best hyperparameter\nlg = LogisticRegression(**params)\n\n#train model with SMOTE train data\nX_train1, y_train1 = SMOTE().fit_resample(X_train, y_train)\nlg = lg.fit(X_train1, y_train1)\n\n#evaluate the model\nmb2_lg = my_scorer_2(lg, X_test, y_test, grid_result.iloc[0,1])\nmb2_lg = np.append(mb2_lg, params)\n\n\nresult_mb2 = pd.DataFrame([mb2_rf,mb2_xg,mb2_lg])\nresult_mb2.columns = ['threshold','score','precision','recall','f1','params']\nresult_mb2['model'] = ['MB2_RF','MB2_XG','MB2_Log_Reg']\nresult_mb2\n\n\n\n\n\n\n\n\nthreshold\nscore\nprecision\nrecall\nf1\nparams\nmodel\n\n\n\n\n0\n0.42\n557.0\n0.454955\n0.839335\n0.590068\n{'bootstrap': False, 'max_depth': 25, 'min_sam...\nMB2_RF\n\n\n1\n0.44\n546.0\n0.472266\n0.825485\n0.600806\n{'colsample_bytree': 1.0, 'learning_rate': 0.0...\nMB2_XG\n\n\n2\n0.48\n546.0\n0.472178\n0.822715\n0.600000\n{'C': 1.0, 'max_iter': 50, 'penalty': 'l2', 's...\nMB2_Log_Reg\n\n\n\n\n\n\n\nYou see that the threshold on all models are increasing than model building 1 because SMOTE make the models learn more on the positive class.\n\n\n\n\n#RANDOM FOREST MODELLING\n\n#define parameter for tuning\nparam_grid_rf = {\n    'n_estimators': [250 , 400],\n    'max_depth': [10, 25, 50],\n    'min_samples_split': [25, 50, 70, 120],\n    'min_samples_leaf': [50, 75, 120],\n    'bootstrap' : [True, False]\n}\n\n#run grid search cv\nrf = GridSearchCV(estimator = RandomForestClassifier(),\n                  param_grid = param_grid_rf,\n                  cv=5,\n                  scoring = grid_scoring, refit=False, n_jobs = -1)\n\n#fit grid search cv with train data\nrf.fit(X_train, y_train, sample_weight = weight)\n\n#selecting the best parameter and metrics\ngrid_result = pd.DataFrame(rf.cv_results_)\ngrid_result = grid_result[grid_result['mean_test_loss_score'] == grid_result['mean_test_loss_score'].min()][['params','mean_test_threshold','mean_test_loss_score','mean_test_precision','mean_test_recall','mean_test_f1']]\n\n#train model with the best hyperparameter\nrf = RandomForestClassifier(**grid_result.iloc[0,0])\n\n#train model with train data\nrf = rf.fit(X_train, y_train, sample_weight = weight)\n\n#evaluate the model\nmb3_rf = my_scorer_2(rf, X_test, y_test, grid_result.iloc[0,1])\nmb3_rf = np.append(mb3_rf, grid_result.iloc[0,0])\n\n\n#XGBOOST MODELLING\n\n#define parameter for tuning\nparam_grid_xg = {\n    'learning_rate': [0.1, 0.01, 0.001],\n    'n_estimators': [100, 500],\n    'max_depth': [5, 10, 25],\n    'subsample': [0.8, 0.9, 1.0],\n    'colsample_bytree': [0.8, 0.9, 1.0]\n}\n\n#run grid search cv\nxg = GridSearchCV(estimator = XGBClassifier(),\n                  param_grid = param_grid_xg,\n                  cv=5,\n                  scoring = grid_scoring, refit=False, n_jobs = -1)\n\n#fit grid search cv with train data\nxg.fit(X_train, y_train, sample_weight = weight)\n\n#selecting the best parameter and metrics\ngrid_result = pd.DataFrame(xg.cv_results_)\ngrid_result = grid_result[grid_result['mean_test_loss_score'] == grid_result['mean_test_loss_score'].min()][['params','mean_test_threshold','mean_test_loss_score','mean_test_precision','mean_test_recall','mean_test_f1']]\n\n#train model with the best hyperparameter\nxg = XGBClassifier(**grid_result.iloc[0,0])\n\n#train model with train data\nxg = xg.fit(X_train, y_train, sample_weight = weight)\n\n#evaluate the model\nmb3_xg = my_scorer_2(xg, X_test, y_test, grid_result.iloc[0,1])\nmb3_xg = np.append(mb3_xg, grid_result.iloc[0,0])\n\n\n#LOGISTIC REGRESSION MODELLING\n\n#define parameter for tuning\nparam_grid_lg = {\n    'penalty': ['l1', 'l2'], \n    'C': [0.1, 1.0, 10.0],  \n    'solver': ['liblinear'],  \n    'max_iter': [50,100,200] \n}\n\n\n#run grid search cv\nlg = GridSearchCV(estimator = LogisticRegression(),\n                  param_grid = param_grid_lg,\n                  cv=5,\n                  scoring = grid_scoring, refit=False, n_jobs = -1)\n\n#fit grid search cv with train data\nlg.fit(X_train, y_train, sample_weight = weight)\n\n#selecting the best parameter and metrics\ngrid_result = pd.DataFrame(lg.cv_results_)\ngrid_result = grid_result[grid_result['mean_test_loss_score'] == grid_result['mean_test_loss_score'].min()][['params','mean_test_threshold','mean_test_loss_score','mean_test_precision','mean_test_recall','mean_test_f1']]\n\n#train model with the best hyperparameter\nlg = LogisticRegression(**grid_result.iloc[0,0])\n\n#train model with train data\nlg = lg.fit(X_train, y_train, sample_weight = weight)\n\n#evaluate the model\nmb3_lg = my_scorer_2(lg, X_test, y_test, grid_result.iloc[0,1])\nmb3_lg = np.append(mb3_lg, grid_result.iloc[0,0])\n\n\nresult_mb3 = pd.DataFrame([mb3_rf,mb3_xg,mb3_lg])\nresult_mb3.columns = ['threshold','score','precision','recall','f1','params']\nresult_mb3['model'] = ['MB3_RF','MB3_XG','MB3_Log_Reg']\nresult_mb3\n\n\n\n\n\n\n\n\nthreshold\nscore\nprecision\nrecall\nf1\nparams\nmodel\n\n\n\n\n0\n0.48\n549.0\n0.456193\n0.836565\n0.590420\n{'bootstrap': True, 'max_depth': 25, 'min_samp...\nMB3_RF\n\n\n1\n0.48\n551.0\n0.465409\n0.819945\n0.593781\n{'colsample_bytree': 0.8, 'learning_rate': 0.0...\nMB3_XG\n\n\n2\n0.44\n571.0\n0.433803\n0.853186\n0.575163\n{'C': 0.1, 'max_iter': 50, 'penalty': 'l2', 's...\nMB3_Log_Reg\n\n\n\n\n\n\n\n\nfinal_result = pd.concat([result_mb1, result_mb2, result_mb3]).sort_values('score')\nfinal_result\n\n\n\n\n\n\n\n\nthreshold\nscore\nprecision\nrecall\nf1\nparams\nmodel\n\n\n\n\n0\n0.24\n539.0\n0.473101\n0.828255\n0.602216\n{'bootstrap': True, 'max_depth': 25, 'min_samp...\nMB1_RF\n\n\n2\n0.22\n541.0\n0.467492\n0.836565\n0.599801\n{'C': 0.1, 'max_iter': 50, 'penalty': 'l2', 's...\nMB1_Log_Reg\n\n\n1\n0.44\n546.0\n0.472266\n0.825485\n0.600806\n{'colsample_bytree': 1.0, 'learning_rate': 0.0...\nMB2_XG\n\n\n2\n0.48\n546.0\n0.472178\n0.822715\n0.600000\n{'C': 1.0, 'max_iter': 50, 'penalty': 'l2', 's...\nMB2_Log_Reg\n\n\n0\n0.48\n549.0\n0.456193\n0.836565\n0.590420\n{'bootstrap': True, 'max_depth': 25, 'min_samp...\nMB3_RF\n\n\n1\n0.48\n551.0\n0.465409\n0.819945\n0.593781\n{'colsample_bytree': 0.8, 'learning_rate': 0.0...\nMB3_XG\n\n\n0\n0.42\n557.0\n0.454955\n0.839335\n0.590068\n{'bootstrap': False, 'max_depth': 25, 'min_sam...\nMB2_RF\n\n\n1\n0.20\n558.0\n0.449190\n0.844875\n0.586538\n{'colsample_bytree': 0.8, 'learning_rate': 0.0...\nMB1_XG\n\n\n2\n0.44\n571.0\n0.433803\n0.853186\n0.575163\n{'C': 0.1, 'max_iter': 50, 'penalty': 'l2', 's...\nMB3_Log_Reg\n\n\n\n\n\n\n\nAnalysis of how the models performed: 1.RandomForest performs best with hyperparameter tuning and a lower decision threshold. However, this model performs worst when using SMOTE. 2.LogisticRegression performs worst when using ‘sample weighting’. 3.XGBoost performs best when using SMOTE. 4.It’s important to note that all models produce similar results when using their best parameters and conditions. 5.In my opinion, the greatest impact is achieved by using hyperparameter tuning and decision threshold tuning, rather than using SMOTE and weighting techniques.\n\n\n\n\nIn this section, I want to show you how to interpret a tree-based model, such as Random Forest, so we can have a better understanding of how the model actually works.\n\n#selecting the best parameter for random forest\nrf_param = final_result[final_result['model'] == 'MB1_RF']['params'][0]\nrf_param\n\n{'bootstrap': True,\n 'max_depth': 25,\n 'min_samples_leaf': 50,\n 'min_samples_split': 50,\n 'n_estimators': 250}\n\n\n\nrf = RandomForestClassifier(**rf_param)\nrf.fit(X_train, y_train)\n\nRandomForestClassifier(max_depth=25, min_samples_leaf=50, min_samples_split=50,\n                       n_estimators=250)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(max_depth=25, min_samples_leaf=50, min_samples_split=50,\n                       n_estimators=250)\n\n\n\n\nWhat is ICE? It is a plot that shows how a model makes predictions based on changing the value of one or more features, while keeping the values of other features constant. This provides us with more insights and understanding of how the model treats features to make predictions. ICE works per row (or per customer in this case), and PDP is simply the average of ICE.\n\ndef pdp_ice_plot(model, df_test, column, clusters=True):\n    df_test = df_test.copy()\n    pdp_isolate = pdp.PDPIsolate(model = model, df = df_test, \n                      num_grid_points=15,\n                      model_features = df_test.columns, \n                      feature = column, feature_name=column)\n    pdp_isolate.plot(\n                center=True, plot_lines=True,\n                cluster=clusters, n_cluster_centers=5,cluster_method='accurate',\n                plot_pts_dist=False, to_bins=True,\n                show_percentile=False, which_classes=None,\n                figsize=(10,6), dpi=300,\n                ncols=2, plot_params={\"pdp_hl\": True},\n                engine='matplotlib', template='plotly_white')\n    plt.show()\n    \n\n\npdp_ice_plot(rf, X_test, 'MonthlyCharges', False)\n\nobtain pred_func from the provided model.\n\n\n\n\n\n\n\n\nThe light blue lines represent ICE (Individual Conditional Expectation), and the yellowish blue line represents PDP (Partial Dependence Plot). The X-axis represents MonthlyCharges, while the Y-axis represents the change in prediction probability. At MonthlyCharges of 60.7 USD, you can observe that some customers experience a significant increase in churn probability as the MonthlyCharges increase. However, it is important to note that not all customers have the same response. Some customers are minimally affected, and some may not be affected at all.\n\npdp_ice_plot(rf, X_test, 'MonthlyCharges')\n\nobtain pred_func from the provided model.\n\n\n\n\n\n\n\n\nThis is the same plot as above, but I have grouped the ICE into 5 clusters for easier viewing and analysis. You can see that there are some customers who experience a significant increase in churn probability as the MonthlyCharges increase.\n\npdp_ice_plot(rf, X_test, 'tenure')\n\nobtain pred_func from the provided model.\n\n\n\n\n\n\n\n\nThe longer the tenure, the lower the churn probability. However, the effect is not the same for all customers. Some customers are greatly affected, while others are barely affected.\n\npdp_ice_plot(rf, X_test, 'Contract')\n\nobtain pred_func from the provided model.\n\n\n\n\n\n\n\n\nThe same also goes with Contract. Longer contract means lower churn probability.\n\nX_test_copy = X_test.copy()\npdp_interact = pdp.PDPInteract(model=rf, df=X_test_copy,\n                              num_grid_points=6,\n                              model_features = X_test.columns, \n                              features=['MonthlyCharges','tenure'], \n                              feature_names=['MonthlyCharges','tenure'])\npdp_interact.plot(\n            plot_type=\"grid\", plot_pdp=True,\n            to_bins=True, show_percentile=False,\n            which_classes=None, figsize=(10,6),\n            dpi=300, ncols=2, plot_params=None, \n            engine='matplotlib', template='plotly_white',\n)\nplt.show()\n\nobtain pred_func from the provided model.\nobtain pred_func from the provided model.\nobtain pred_func from the provided model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s analyze the combination of MonthlyCharges and tenure. We can observe a spike in churn probability for MonthlyCharges ranging from 56.3 USD to 79.2 USD, particularly for customers with a tenure of less than 7 months.\n\nX_test_copy = X_test.copy()\npdp_interact = pdp.PDPInteract(model=rf, df=X_test_copy,\n                              num_grid_points=10,\n                              model_features = X_test.columns, \n                              features=['tenure','Contract'], \n                              feature_names=['tenure','Contract'])\npdp_interact.plot(\n            plot_type=\"grid\", plot_pdp=True,\n            to_bins=True, show_percentile=False,\n            which_classes=None, figsize=(10,6),\n            dpi=300, ncols=2, plot_params=None, \n            engine='matplotlib', template='plotly_white',\n)\nplt.show()\n\nobtain pred_func from the provided model.\nobtain pred_func from the provided model.\nobtain pred_func from the provided model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe model produces similar churn probabilities for customers with a combination of a 2-year contract and low tenure compared to those with a month-to-month contract and medium tenure (24-36 months).\nYou can see how PDP and ICE plots can be very beneficial in understanding how the model utilizes features to make predictions. In the next section, I will demonstrate how to assess the model’s prediction confidence level.\n\n\n\nTree-based models like RandomForest make predictions by using the mean of all the trees’ prediction probabilities. However, instead of solely relying on the mean, we can also calculate the standard deviation. A higher standard deviation indicates lower confidence in the predictions. Additionally, we can utilize confidence intervals, such as 95% or even more extreme at 99%.\n\n#extract all trees' prediction probability per row\npredict = np.stack([x.predict_proba(X_test)[:,1] for x in rf.estimators_])\n\n\n#assign mean and std. deviation of trees' prediction probability.\ndf_pred = X_test.copy()\ndf_pred['avg'] = np.round(np.mean(predict, axis = 0) * 100, 2)\ndf_pred['std_dev'] = np.round(np.std(predict, axis = 0) * 100, 2)\n\n\nplt.figure(figsize=(10,6))\nsns.histplot(df_pred['std_dev'],color='skyblue', kde=True, edgecolor='none')\nplt.title('Standard deviation distribution', weight='bold')\nplt.show()\n\n\n\n\nMost of predictions have std.deviation under 10%. Let’s calculate confidence interval with 99%.\n\ndf_pred['CI-99%'] = (2.576 * df_pred['std_dev'] / np.sqrt(len(predict))) * 100 / (df_pred['avg'])\n\n\ndf_pred[df_pred.avg &gt; 40].sort_values('CI-99%', ascending=False).head(5)\n\n\n\n\n\n\n\n\nContract\ntenure\nFiber_optic\nMonthlyCharges\nFO_LB\nNo_internet\navg\nstd_dev\nCI-99%\n\n\n\n\n852\n2\n7\n1\n94.05\n1\n0\n40.06\n25.08\n10.199818\n\n\n499\n1\n58\n1\n98.70\n1\n0\n41.05\n18.24\n7.239149\n\n\n1186\n1\n59\n1\n101.10\n1\n0\n40.37\n16.73\n6.751699\n\n\n1137\n1\n15\n1\n96.30\n0\n0\n48.14\n19.69\n6.663701\n\n\n1184\n1\n10\n1\n92.50\n0\n0\n48.17\n19.12\n6.466765\n\n\n\n\n\n\n\nLet’s consider the example of row 1. The model predicts a 40% probability of churn for the customer, with a confidence interval of +- 10%. By default, the model’s output indicates that the customer will not churn. However, due to the high confidence interval, it is safer to assume that the customer will churn.\nChecking the standard deviation and confidence interval of the trees is extremely useful, particularly when the cost of ‘False Negative’ is significant and can have severe consequences."
  },
  {
    "objectID": "ipynb/telco/final-project.html#table-of-contents",
    "href": "ipynb/telco/final-project.html#table-of-contents",
    "title": "Telco customer churn analysis",
    "section": "",
    "text": "• Data Introduction  • Data Preparation (Import libraries, data cleaning & data wrangling) • Basic exploratory data analysis      • Descriptive statistic      • Univariate analysis      • Multivariate analysis  • Deep exploratory data analysis      • Services and internet services analysis      • Monthly charges analysis      • Customer analysis      • Benefits analysis      • Churn analysis  • Modelling     • Features selection and encoding      • Splits data and define custom metrics      • Model building combination 1      • Model building combination 2      • Model building combination 3\n• Model interpretation     • PDP and ICE plots      • Checking prediction’s confidence using confidence interval"
  },
  {
    "objectID": "ipynb/telco/final-project.html#data-introduction",
    "href": "ipynb/telco/final-project.html#data-introduction",
    "title": "Telco customer churn analysis",
    "section": "",
    "text": "This dataset contain informations about customers that churn and not churned in a telco company. Below is the column informations: 1.customerID = customer unique ID. 2.gender = customer gender (M/F). 3.SeniorCitizen = old / young customer. 4.Partner = either a customer has partners or not. 5.Dependents = either a customer has dependents or not. 6.tenure = how long the customer subscribed (in month). 7.MultipleLines = either a customer using multiple lines or not (phone lines). 8.InternetService = either a customer using InternetService lines or not. 9.OnlineSecurity = either a customer has OnlineSecurity or not. 10.OnlineBackup = either a customer has OnlineBackup or not. 11.DeviceProtection = either a customer has DeviceProtection or not. 12.TechSupport = either a customer has TechSupport or not. 13.StreamingTV = either a customer has StreamingTV or not. 14.StreamingMovies = either a customer has StreamingMovie or not. 15.Contract = types of contract. 16.PaperlessBilling = either a customer has PaperlessBilling or not. 17.PaymentMethod = types of the payment method. 18.MonthlyCharges = how much charges per month. 19.TotalCharges = total charges of all time. 20.Churn = either a customer churn or not. 21.Hobby = customer hobby."
  },
  {
    "objectID": "ipynb/telco/final-project.html#data-preparation",
    "href": "ipynb/telco/final-project.html#data-preparation",
    "title": "Telco customer churn analysis",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import gaussian_kde\nimport matplotlib as mpl\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.metrics import confusion_matrix\nfrom pdpbox import pdp, info_plots\nsns.set_style('darkgrid')\nmpl.rcParams['figure.dpi'] = 300 #set figure dpi\nsns.set() #set figure styling\n\n\n#import dataset\ndf = pd.read_csv('telco.csv')\ndf.head()\n\n\n\n\n\n\n\n\ncustomerID\ngender\nSeniorCitizen\nPartner\nDependents\ntenure\nPhoneService\nMultipleLines\nInternetService\nOnlineSecurity\n...\nTechSupport\nStreamingTV\nStreamingMovies\nContract\nPaperlessBilling\nPaymentMethod\nMonthlyCharges\nTotalCharges\nChurn\nHobby\n\n\n\n\n0\n7590-VHVEG\nFemale\n0\nYes\nNo\n135\nNo\nNo phone service\nDSL\nNo\n...\nNo\nNo\nNo\nMonth-to-month\nYes\nElectronic check\n29.85\n29.85\nNo\nSwimming\n\n\n1\n5575-GNVDE\nMale\n0\nNo\nNo\n34\nYes\nNo\nDSL\nYes\n...\nNo\nNo\nNo\nOne year\nNo\nMailed check\n56.95\n1889.5\nNo\nRunning\n\n\n2\n3668-QPYBK\nMale\n0\nNo\nNo\n140\nYes\nNo\nDSL\nYes\n...\nNo\nNo\nNo\nMonth-to-month\nYes\nMailed check\n53.85\n7560\nYes\nHiking\n\n\n3\n7795-CFOCW\nMale\n0\nNo\nNo\n136\nNo\nNo phone service\nDSL\nYes\n...\nYes\nNo\nNo\nOne year\nNo\nBank transfer (automatic)\n42.45\n1840.75\nNo\nSwimming\n\n\n4\n9237-HQITU\nFemale\n0\nNo\nNo\n2\nYes\nNo\nFiber optic\nNo\n...\nNo\nNo\nNo\nMonth-to-month\nYes\nElectronic check\n70.70\n151.65\nYes\nRunning\n\n\n\n\n5 rows × 22 columns\n\n\n\n\n#take a look at the dataframe\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 7043 entries, 0 to 7042\nData columns (total 22 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   customerID        7043 non-null   object \n 1   gender            7043 non-null   object \n 2   SeniorCitizen     7043 non-null   int64  \n 3   Partner           7043 non-null   object \n 4   Dependents        7043 non-null   object \n 5   tenure            7043 non-null   int64  \n 6   PhoneService      7043 non-null   object \n 7   MultipleLines     7043 non-null   object \n 8   InternetService   7043 non-null   object \n 9   OnlineSecurity    7043 non-null   object \n 10  OnlineBackup      7043 non-null   object \n 11  DeviceProtection  6627 non-null   object \n 12  TechSupport       7043 non-null   object \n 13  StreamingTV       7043 non-null   object \n 14  StreamingMovies   7043 non-null   object \n 15  Contract          6798 non-null   object \n 16  PaperlessBilling  7043 non-null   object \n 17  PaymentMethod     7043 non-null   object \n 18  MonthlyCharges    7043 non-null   float64\n 19  TotalCharges      4859 non-null   object \n 20  Churn             7043 non-null   object \n 21  Hobby             4201 non-null   object \ndtypes: float64(1), int64(2), object(19)\nmemory usage: 1.2+ MB\n\n\nThere are 4 columns with missing values which are DeviceProtection, Contract, TotalCharges and Hobby.\n\n#checking percentage of missing values\nmissingkolom = ['DeviceProtection','Contract','TotalCharges','Hobby']\nfor x in missingkolom:\n    print(f'Missing value of column {x} (%) : {round(df[x].isna().sum()/len(df) * 100,2)}')\n\nMissing value of column DeviceProtection (%) : 5.91\nMissing value of column Contract (%) : 3.48\nMissing value of column TotalCharges (%) : 31.01\nMissing value of column Hobby (%) : 40.35\n\n\n\n#impute missing values with univariate imputation by value proportion\ndf['DeviceProtection'] = df['DeviceProtection'].fillna(\n                             pd.Series(np.random.choice(['No','Yes','No internet service'], \n                             p = list(df['DeviceProtection'].value_counts(normalize=True)), size=len(df))))\n\ndf['Contract'] = df['Contract'].fillna(\n                     pd.Series(np.random.choice(['Month-to-month','Two year','One year'], \n                     p = list(df['Contract'].value_counts(normalize=True)), size=len(df))))\n\nHere I impute DeviceProtection and Contract with univariate imputation by value proportion for the following reasons: 1.The missing values is not that much (&lt;10%). 2.The columns don’t have any relationship with other columns so that multivariate imputation is not possible. 3.Using proportion is more precise in this case rather than use ‘mode’.\n\n#delete column Hobby\ndf.drop(columns=['Hobby'],inplace=True)\n\nReasons to delete: 1.Missing values is too many. 2.By business context, Hobby doesn’t give enough useful informations. 3.Cannot be imputed by multivariate imputation.\n\n#impute TotalCharges from tenure and MonthlyCharges\ndf['TotalCharges'] = df['TotalCharges'].fillna(df['tenure'] * df['MonthlyCharges'])\n\nEven this column has so many missing values, I decided to impute it with multivariate imputation because: 1.By business context, TotalCharges is more or less tenure * MonthlyCharges. 2.So even the missing values are high, it can still be imputed with a strong justification.\n\n#there's a space in the total charges column.\nfor x in df.TotalCharges:\n    try:\n        float(x)\n    except:\n        print(f'Unable to convert to float with this value : {x}')\n\nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \n\n\n\ndf[df.TotalCharges == ' '].head()\n\n\n\n\n\n\n\n\ncustomerID\ngender\nSeniorCitizen\nPartner\nDependents\ntenure\nPhoneService\nMultipleLines\nInternetService\nOnlineSecurity\n...\nDeviceProtection\nTechSupport\nStreamingTV\nStreamingMovies\nContract\nPaperlessBilling\nPaymentMethod\nMonthlyCharges\nTotalCharges\nChurn\n\n\n\n\n488\n4472-LVYGI\nFemale\n0\nYes\nYes\n0\nNo\nNo phone service\nDSL\nYes\n...\nYes\nYes\nYes\nNo\nOne year\nYes\nBank transfer (automatic)\n52.55\n\nNo\n\n\n753\n3115-CZMZD\nMale\n0\nNo\nYes\n0\nYes\nNo\nNo\nNo internet service\n...\nNo internet service\nNo internet service\nNo internet service\nNo internet service\nTwo year\nNo\nMailed check\n20.25\n\nNo\n\n\n936\n5709-LVOEQ\nFemale\n0\nYes\nYes\n0\nYes\nNo\nDSL\nYes\n...\nNo\nNo\nYes\nYes\nTwo year\nNo\nMailed check\n80.85\n\nNo\n\n\n1082\n4367-NUYAO\nMale\n0\nYes\nYes\n0\nYes\nYes\nNo\nNo internet service\n...\nNo internet service\nNo internet service\nNo internet service\nNo internet service\nTwo year\nNo\nMailed check\n25.75\n\nNo\n\n\n1334\n1768-ZAIFU\nFemale\n1\nNo\nNo\n0\nNo\nNo phone service\nDSL\nNo\n...\nNo\nNo\nNo\nNo\nMonth-to-month\nYes\nElectronic check\n25.20\n\nYes\n\n\n\n\n5 rows × 21 columns\n\n\n\nWhen the tenure value is 0, the TotalCharges value is empty (‘space’). This is because customers who have just joined (less than a month) have not been charged yet, resulting in a TotalCharges value of 0. Since there are only 12 rows with this condition, I will delete them.\n\n#drop rows that has empty TotalCharges.\ndf = df.drop(df.index[df.TotalCharges == ' ']).reset_index(drop=True)\n\n\n#check for duplicate data, if True then there's no duplicate.\ndf.customerID.nunique() == len(df) \n\nTrue\n\n\n\n#feature engineered 2 new features for the sake of easier analysis.\ndf['Services'] = df[['PhoneService','InternetService']].apply(\n                     lambda x: 'Both' if list(x).count('No') == 0 else\n                     'Internet Only' if x[0] == 'No' else 'Phone Only', axis=1)\n\ndf['TotalBenefits'] = df.loc[:,'OnlineSecurity':'StreamingMovies']\\\n                          .apply(lambda x: list(x).count('Yes'), axis=1)\n\nNew features explanation:  1.Services = Combined values of PhoneService and InternetService. 2.TotalBenefits = Sum of benefits taken on OnlineSecurity until StreamingMovies.\n\n#Change values of 1 and 0 to 'Yes' and 'No'\ndf['SeniorCitizen'] = df.SeniorCitizen.apply(lambda x: 'Yes' if x == 1 else 'No')\n\nChange numerical value to strings for simpler and consistent analysis.\n\n#drop useless column\ndf.drop(columns=['customerID'], inplace=True)\n\n\n#change columns object data type to numerical\ndf.tenure = df.tenure.astype('int64')\ndf.MonthlyCharges = df.MonthlyCharges.astype('float64')\ndf.TotalCharges = df.TotalCharges.astype('float64')\n\n\n#checking values of real totalcharges and calculated totalcharges\ndf['TotalChargesDiff'] = df[['tenure','MonthlyCharges','TotalCharges']].apply(\n                             lambda x: round(abs(1 - (x[0] * x[1] / x[2])) * 100, 3), axis=1)\n\nHere, I have created a new column called TotalChargesDiff to check the differences (%) between the actual TotalCharges value (from the dataset) and the calculated TotalCharges value (obtained by multiplying tenure with MonthlyCharges). If the difference is above 40%, I will consider those rows as invalid because the values of tenure and MonthlyCharges cannot be trusted.\n\ndf['TotalChargesDiff'].sort_values(ascending=False).head(10)\n\n0       13400.000\n5        1357.404\n18        788.048\n19        330.214\n3         213.633\n128        73.511\n47         72.615\n4631       64.286\n5802       63.380\n20         58.263\nName: TotalChargesDiff, dtype: float64\n\n\nYou can observe that some data points have a TotalChargesDiff that reaches hundreds or even thousands percent.\n\n#removing rows that have &gt; 40% TotalChargesDiff.\ndf = df[df.TotalChargesDiff &lt; 40].reset_index(drop=True)\ndf.drop(columns=['TotalChargesDiff'], inplace=True)\n\n\ndef numericategoric(df):\n    num = len(df._get_numeric_data().columns)\n    cat = len(df.columns) - num\n    print(\"TotalNumericalData = \" + str(num))\n    print(\"TotalCategoricalData = \" + str(cat))\n    print(\"Numerical = \" + str(list(df._get_numeric_data().columns )))\n    print(\"Categorical = \" + str(list(df.drop(df._get_numeric_data().columns, axis=1).columns)))\n\n\nnumericategoric(df)\n\nTotalNumericalData = 4\nTotalCategoricalData = 18\nNumerical = ['tenure', 'MonthlyCharges', 'TotalCharges', 'TotalBenefits']\nCategorical = ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'Churn', 'Services']\n\n\nShow numerical and categorical columns\n\n#assign categorical and numerical columns on different dataframe for easier analysis.\ndfnum = df._get_numeric_data()\ndfcat = df.drop(columns = dfnum.columns)"
  },
  {
    "objectID": "ipynb/telco/final-project.html#basic-exploratory-data-analysis.",
    "href": "ipynb/telco/final-project.html#basic-exploratory-data-analysis.",
    "title": "Telco customer churn analysis",
    "section": "",
    "text": "#numerical columns describe.\ndfnum.describe()\n\n\n\n\n\n\n\n\ntenure\nMonthlyCharges\nTotalCharges\nTotalBenefits\n\n\n\n\ncount\n7012.000000\n7012.000000\n7012.000000\n7012.000000\n\n\nmean\n32.506560\n64.732760\n2286.410207\n2.042356\n\n\nstd\n24.564234\n30.109753\n2265.759401\n1.835886\n\n\nmin\n1.000000\n12.000000\n13.500000\n0.000000\n\n\n25%\n9.000000\n35.450000\n402.437500\n0.000000\n\n\n50%\n29.000000\n70.300000\n1397.250000\n2.000000\n\n\n75%\n56.000000\n89.850000\n3784.125000\n3.000000\n\n\nmax\n140.000000\n118.750000\n8684.800000\n6.000000\n\n\n\n\n\n\n\n1.All columns seems to have a normal min-max values. Nothing weird here. 2.Average tenure is about 30 months which is pretty low. 3.Average MonthlyCharge is about 65-70 USD which is pretty good. 4.Out of 6 benefits available, the average taken by customer is around 2, which is pretty low.\n\ndfcat.describe()\n\n\n\n\n\n\n\n\ngender\nSeniorCitizen\nPartner\nDependents\nPhoneService\nMultipleLines\nInternetService\nOnlineSecurity\nOnlineBackup\nDeviceProtection\nTechSupport\nStreamingTV\nStreamingMovies\nContract\nPaperlessBilling\nPaymentMethod\nChurn\nServices\n\n\n\n\ncount\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n\n\nunique\n2\n2\n2\n2\n2\n3\n3\n3\n3\n3\n3\n3\n3\n3\n2\n4\n2\n3\n\n\ntop\nMale\nNo\nNo\nNo\nYes\nNo\nFiber optic\nNo\nNo\nNo\nNo\nNo\nNo\nMonth-to-month\nYes\nElectronic check\nNo\nBoth\n\n\nfreq\n3542\n5874\n3624\n4920\n6336\n3372\n3087\n3486\n3075\n3066\n3459\n2798\n2769\n3850\n4155\n2354\n5155\n4821\n\n\n\n\n\n\n\n1.Male and Female has the same proportion. 2.Most of customers is non SeniorCitizen with no Partner and No Dependents. 3.Favorite InternetService is Fiber optic. 4.Majority of customers is subscribed with ‘Month-to-month’ Contract. 5.5155 out of 7012 is non-Churn customers which make this dataset is imbalanced. 6.All these columns have low cardiality values.\n\n\n\n\nfig, axarr = plt.subplots(1,4, figsize=(10, 4))\nfor x in dfnum.columns:\n    axarr[dfnum.columns.get_loc(x)].boxplot(df[x],patch_artist=True)\n    axarr[dfnum.columns.get_loc(x)].set_xlabel(x)\nplt.suptitle('Outliers checking on numeric columns')\nfig.tight_layout(pad=1)\nplt.show()\n\n\n\n\nWill drop outlier in tenure.\n\n#drop outlier in tenure\ndf = df[df.tenure &lt; 125]\n\n\n#plot distribution for numerical columns\nfig, axarr = plt.subplots(1,4, figsize=(10, 4))\nfor x in dfnum.columns:\n    sns.histplot(data=dfnum[x], \n                 color='skyblue', \n                 kde=True, \n                 edgecolor='none', \n                 ax=axarr[dfnum.columns.get_loc(x)])\nplt.suptitle('Distribution plot', weight='bold')\nfig.tight_layout(pad=1)\n\n\n\n\nAbove plots are distribution plots on all numerical columns. 1.tenure and MonthlyCharges have a ‘U-shaped’ distribution. 2.TotalCharges has a positive-skew distribution.\n\n#count plot for categorical columns\nplt.figure(figsize=(15,12))\n\nfeatures = dfcat.columns\nfor i in np.arange(1, len(features)+1):\n    plt.subplot(5, len(features)//3 - 2, i)\n    sns.countplot(x=df[features[i-1]], color='green')\n    plt.xticks(rotation=10)\n    plt.xlabel(features[i-1])\nplt.suptitle('CountPlot', size=19, weight='bold')\nplt.tight_layout(pad = 1)\n\n\n\n\nHere I count plotted all categorical columns.\n\n\n\n\n#count plots against 'churn'\nplt.figure(figsize=(15,12))\n\nfeatures = dfcat.columns\nfor i in np.arange(1, len(features)+1):\n    plt.subplot(5, len(features)//3 - 2, i)\n    sns.countplot(data=df, x=df[features[i-1]], hue='Churn')\n    plt.legend(prop={'size': 8})\n    plt.xticks(rotation=10)\n    plt.xlabel(features[i-1])\nplt.suptitle('CountPlot vs Churned', size=19, weight='bold')\nplt.tight_layout(pad = 1)\n\n\n\n\nI added Churn count into the categorical plots. 1.You can see for column Gender, the values and Churn count is pretty equal which make this column will have a very low predictive power. 2.For InternetService, fiber optic has way higher in churn probability compare to DSL. 3.Same ways also applied on Month-to-month Contract and Electronic-check PaymentMethod.\n\n#distribution plots against 'churn'\nfig, axarr = plt.subplots(1,4, figsize=(10, 4))\nfor x in dfnum.columns:\n    sns.histplot(data=df, \n                 x = dfnum[x],\n                 color='skyblue', \n                 kde=True, \n                 edgecolor='none', \n                 ax=axarr[dfnum.columns.get_loc(x)], \n                 hue='Churn')\n    \nplt.suptitle(\"Distribution plot\", weight='bold')\nfig.tight_layout(pad=1)\n\n\n\n\nLet’s also compare Churn distribution on numerical columns. Customers tend to churn when the tenure is low and not churn when the MonthlyCharges is very low. I will do further analysis about these columns later.\n\n#change binary column into numerical\nbinary = ['gender','SeniorCitizen','Partner','Dependents','PhoneService','PaperlessBilling','Churn']\nvalue_mapping = {\n    'No': 0,\n    'Yes' : 1,\n    'Male' : 1,\n    'Female' : 0\n}\ndfcorr = df[binary]\nfor col in binary:\n    dfcorr[col] = dfcorr[col].map(value_mapping).astype('int64')\n\n\nplt.figure(figsize=(10,8))\nsns.heatmap(dfcorr.corr(), annot=True, fmt='.2f')\nplt.show()\n\n\n\n\nCorrelation! 1.TotalCharges and tenure have a high positive correlation (causation : the longer the customers subscribed, the more they paid). 2.TotalBenefits also has a strong correlation with MonthlyCharges and TotalCharges (causation : more benefits taken also make the MonthlyCharges higher). Note : Correlation doesn’t indicate causation. Understanding the specific context, industry knowledge, and conducting further analysis or experiments can help determine if there is a causal relationship between the variables or if other factors are influencing the observed correlations."
  },
  {
    "objectID": "ipynb/telco/final-project.html#deep-dive-exploratory-data-analysis",
    "href": "ipynb/telco/final-project.html#deep-dive-exploratory-data-analysis",
    "title": "Telco customer churn analysis",
    "section": "",
    "text": "#create a function to plot churn probability for numerical columns.\ndef prob_plot(df,colom,x):\n    means = df[colom].mean()\n    medians = df[colom].median()\n    data = df[df.Churn == 'Yes'][colom].astype('float64')\n    data1 = df[df.Churn == 'No'][colom].astype('float64')\n    \n    kde = gaussian_kde(data)\n    kde1 = gaussian_kde(data1)\n    dist_space = np.linspace( min(data), max(data), 200)\n    dist_space1 = np.linspace( min(data1), max(data1), 200)\n    axarr[x].plot( dist_space, kde(dist_space), label='Churned', color='orange' )\n    axarr[x].plot( dist_space1, kde1(dist_space1), label='Not churn', color='blue')\n    axarr[x].axvline(x = means, linestyle = '--', color='g', label='Mean')\n    axarr[x].axvline(x = medians, linestyle = '--', color='r', label='Median')\n    axarr[x].set_title('Probability', fontweight='bold', size=12)\n    axarr[x].set(ylabel = 'Probability', xlabel = colom)\n    axarr[x].legend()\n\n\n\n\n#count plot for Services.\nfig, axarr = plt.subplots(1,2, figsize=(12, 6))\nsns.countplot(data=df[df.Services == 'Both'], \n              x='InternetService', \n              ax=axarr[0])\n\nsns.countplot(data=df[df.Services == 'Internet Only'], \n              x='InternetService', \n              ax=axarr[1])\n\naxarr[0].set_title(\"Both phone service & internet service\", weight='bold')\naxarr[1].set_title(\"Internet service only\", weight='bold')\n\nplt.suptitle(\"Comparison of internet services on product services\")\nplt.tight_layout(pad=1)\nplt.show()\n\n\n\n\nIt is observed that customers tend to prefer Fiber optic over DSL for ‘phone & internet service’. However, when considering ‘internet service only’ without phone, there is no option for Fiber optic available. This suggests that in order to utilize Fiber optic, a phone connection (or phone service) is required.\n\n#plot churn probability for 'services' & 'internet services'\nfig, axarr = plt.subplots(1,2, figsize=(12, 6))\n\ndf.groupby('Services')['Churn'].agg(lambda x: list(x).count('Yes') * 100 / len(list(x)))\\\n                               .plot(kind='bar', width=0.3,rot=True, ax=axarr[0])\n\ndf[df.Services != 'Phone Only'].groupby(['InternetService'])['Churn'].agg(lambda x: list(x).count('Yes') * 100 / len(x))\\\n                               .plot(kind='bar', width=0.3,rot=True, ax=axarr[1])\n\naxarr[0].set_title('Services churn probability', weight='bold')\naxarr[1].set_title('Internet service churn probability', weight='bold')\n\nplt.tight_layout(pad=1)\nplt.show()\n\n\n\n\nCustomers that used ‘Both’ services and InternetService fiber optic tends to churn more.\n\n\n\n\n#checking InternetService price.\ndf_filtered = df[(df.Services == 'Both') & (df.TotalBenefits == 0) & (df.MultipleLines == 'No')].copy()\ndf_filtered.groupby('InternetService')['MonthlyCharges'].mean()\n\nInternetService\nDSL            44.965089\nFiber optic    70.074454\nName: MonthlyCharges, dtype: float64\n\n\n\nThe price of Fiber optic is higher, around 25 USD, compared to DSL. However, it’s important to keep in mind that these prices include a phone service with a single line.\n\n\ndf_filtered = df[(df.Services == 'Phone Only') & (df.TotalBenefits == 0)].copy()\ndf_filtered.groupby('MultipleLines')['MonthlyCharges'].mean()\n\nMultipleLines\nNo     19.958088\nYes    24.980060\nName: MonthlyCharges, dtype: float64\n\n\n\nplt.figure(figsize=(10,5))\nsns.histplot(data=df_filtered, \n             x='MonthlyCharges', \n             hue='MultipleLines', \n             multiple='stack')\n\nplt.title('Phone service distribution', weight='bold')\nplt.tight_layout()\nplt.show()\n\n\n\n\nWe can see that the price for a ‘phone service’ with a single line is around 20 USD, while the price for a ‘phone service’ with multiple lines is around 25 USD. This also means that the price for DSL is around 25 USD, while the price for Fiber optic is around 50 USD, which is twice as much as DSL.\n\nfig, axarr = plt.subplots(1,2, figsize=(12, 6))\nmc_dist = sns.histplot(data=df, \n                       x = 'MonthlyCharges',\n                       hue='Churn', \n                       ax=axarr[0], \n                       multiple='stack')\n\naxarr[0].set_title('Distribution', fontweight='bold', size=12)\n\nprob_plot(df,'MonthlyCharges',1)\naxarr[1].legend(loc='upper right')\nplt.show()\n\n\n\n\nAt a MonthlyCharges range of approximately +- 20 USD, the ratio of non-churn customers is very high. It is known that products within this price range are typically ‘phone service only’. However, between the price range of 60 - 100 USD, the churn probability increases significantly. I am planning to conduct further analysis specifically for customers within this price range.\n\ndf_filtered = df[(df.InternetService == 'Fiber optic') & (df.Services == 'Both') & (df.MultipleLines == 'No')]\ndf_agg = df_filtered.groupby('TotalBenefits')['MonthlyCharges'].agg('mean').reset_index()\ndf_agg['MonthlyCharges'] = df_agg.MonthlyCharges.round()\n\n\nfig, axarr = plt.subplots(1,2, figsize=(12, 6))\nsns.scatterplot(data=df_filtered, \n                x='MonthlyCharges', \n                y='TotalBenefits', \n                s=35, \n                ax=axarr[0])\n\nsns.barplot(df_agg, x = 'TotalBenefits', y = 'MonthlyCharges')\naxarr[0].set_title('MonthlyCharges vs Totalbenefits', weight='bold')\naxarr[1].set_title('Average MonthlyCharges vs Totalbenefits', weight='bold')\nfig.tight_layout(pad = 1)\nplt.show()\n\n\n\n\nHere, you can observe that as more TotalBenefits are taken, the MonthlyCharges also increase. On the left plot, you can see that there are 5 outlier data points, which will be removed later.\n\ndf_fil = df[(df.InternetService == 'Fiber optic') & (df.Services == 'Both')\\\n              & (df.MultipleLines == 'No') & (df.TotalBenefits == 1)]\n\nbenefits = ['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']\n\ndf_benefit = pd.DataFrame()\nfor x in benefits:\n    df_value = pd.DataFrame([x, \n                      df_fil[df_fil[x] == 'Yes']['MonthlyCharges'].min(), \n                      df_fil[df_fil[x] == 'Yes']['MonthlyCharges'].max(), \n                      df_fil[df_fil[x] == 'Yes']['MonthlyCharges'].mean(), \n                      df_fil[df_fil[x] == 'Yes']['MonthlyCharges'].median()\n                     ]).transpose()\n    \n    df_benefit = pd.concat([df_benefit, df_value])\ndf_benefit.columns = ['Benefit','MinCharges','MaxCharges','MeanCharges','MedianCharges']    \n\n\ndf_benefit\n\n\n\n\n\n\n\n\nBenefit\nMinCharges\nMaxCharges\nMeanCharges\nMedianCharges\n\n\n\n\n0\nOnlineSecurity\n73.2\n76.45\n74.795588\n74.95\n\n\n0\nOnlineBackup\n72.75\n76.65\n74.708511\n74.65\n\n\n0\nDeviceProtection\n69.55\n76.65\n74.604762\n74.9\n\n\n0\nTechSupport\n73.85\n76.55\n75.045455\n74.7\n\n\n0\nStreamingTV\n77.65\n81.9\n79.749242\n79.825\n\n\n0\nStreamingMovies\n12.0\n86.1\n79.045902\n80.15\n\n\n\n\n\n\n\nAbove are the prices of benefits with Fiber optic and a single line phone connection. You can see that StreamingTV and StreamingMovies are more expensive compared to other benefits, approximately +- 5 USD.\nNow that we know the prices of every product, here’s a recap:\nDSL = approximately 25 USD. Fiber optic = approximately 50 USD. Phone service (single line) = approximately 20 USD. Phone service (multiple lines) = approximately 25 USD. OnlineSecurity - TechSupport = approximately 5 USD. StreamingTV - StreamingMovies = approximately 10 USD.\nWith this data, we can perform a simple ‘anomaly detection’ by manually calculating the MonthlyCharges and comparing them with the actual MonthlyCharges, similar to how we calculated the TotalChargesDiff above.\n\n#checking for MonthlyCharges values with the calculated one (similar with checking TotalCharges difference).\ndef MonthlyChargesDiff(x):\n    estimation = 0\n    if x['PhoneService'] == 'Yes':\n        estimation += 20\n    if x['MultipleLines'] == 'Yes':\n        estimation += 5\n    if x['InternetService'] == 'DSL':\n        estimation += 25\n    if x['InternetService'] == 'Fiber optic':\n        estimation += 50\n        \n    if (x['StreamingTV'] == 'Yes') & (x['StreamingMovies'] == 'Yes'):\n        estimation += 20 + (x['TotalBenefits'] - 2) * 5\n    elif (x['StreamingTV'] == 'Yes') | (x['StreamingMovies'] == 'Yes'):\n        estimation += 10 + (x['TotalBenefits'] - 1) * 5\n    else:\n        estimation += x['TotalBenefits'] * 5\n        \n    return abs(1 - (estimation / x['MonthlyCharges'])) * 100\n    \n   \n\n\ndf['MonthlyChargesEstimationDifference'] = df.apply(MonthlyChargesDiff, axis=1)\n\n\ndf[df.MonthlyChargesEstimationDifference &gt; 40][['MonthlyCharges','MonthlyChargesEstimationDifference']]\n\n\n\n\n\n\n\n\nMonthlyCharges\nMonthlyChargesEstimationDifference\n\n\n\n\n12\n29.00\n296.551724\n\n\n389\n12.00\n733.333333\n\n\n666\n12.00\n566.666667\n\n\n859\n26.41\n278.644453\n\n\n1439\n18.26\n447.645126\n\n\n2185\n21.63\n362.320851\n\n\n4090\n31.26\n219.897633\n\n\n5848\n15.00\n466.666667\n\n\n6718\n21.00\n304.761905\n\n\n\n\n\n\n\nYou can see that there are 9 rows with extreme MonthlyCharges values. These are considered as ‘anomalies’, so let’s remove them.\n\n#remove MonthlyCharges extreme values.\ndf = df[df.MonthlyChargesEstimationDifference &lt; 40].reset_index(drop=True)\n\n\n\n\n\n#creating a function to engineered a new feature.\ndef statuss(x):\n    x = list(x)\n    if (x[0] == 'Yes') & (x[1] == 'Yes'):\n        return 'Both'\n    elif (x[0] == 'Yes') & (x[1] == 'No'):\n        return 'Partner Only'\n    elif (x[0] == 'No') & (x[1] == 'Yes'):\n        return 'Dependent Only'\n    else:\n        return 'Single'\n\n\ndf['Status'] = df[['Partner','Dependents']].apply(statuss, axis=1)\n\nI have created a new feature called ‘Status’. This feature is derived from the columns Partner and Dependents. 1.If customers have both Partner and Dependents, it will be labeled as ‘Both’. 2.If customers have Partner but no Dependents, it will be labeled as ‘Partner Only’. 3.If customers have Dependents but no Partner, it will be labeled as ‘Dependent Only’. 4.If customers have neither Partner nor Dependents, it will be labeled as ‘Single’.\n\nplt.figure(figsize=(10,5))\nsns.countplot(df.sort_values('Status', ascending=True), x='Status')\nplt.title('Status Count', size=16, weight='bold')\nplt.tight_layout()\nplt.show()\n\n\n\n\nMajority of customers are Single.\n\nfig, axarr = plt.subplots(1,3, figsize=(15, 6))\nk = ['tenure','MonthlyCharges','TotalBenefits']\nfor x in k:\n    sns.barplot(data=df.groupby(['Status'])[[x]].mean().reset_index(), \n                x='Status', \n                y=x, \n                ax=axarr[k.index(x)],\n                palette=['grey', 'g','m','b'])\n    \n    axarr[k.index(x)].set_title(f'{x} average', weight='bold', size=15)\n    \nfig.tight_layout()\nplt.show()\n\n\n\n\nCustomers labeled as ‘Partner Only’ are considered the best since they have the longest tenure and the highest MonthlyCharges. The second-best group is ‘Both’, although these customers may not have MonthlyCharges as high as those in the ‘Single’ group, their tenure is almost double that of the ‘Single’ group.\n\nplt.figure(figsize=(10,5))\nsns.barplot(data=df.groupby('Status')[['Churn']].agg(lambda x: list(x).count('Yes') / len(x)).reset_index(), \n            x='Status', \n            y='Churn')\n\nplt.title('Churn Probability', weight='bold', size=16)\nplt.tight_layout()\nplt.show()\n\n\n\n\nSingle customers have the highest churn probability.\n\ndf.SeniorCitizen.value_counts(normalize=True)\n\nSeniorCitizen\nNo     0.837903\nYes    0.162097\nName: proportion, dtype: float64\n\n\nThe majority of customers are young people.\n\ndf_status = df.groupby('SeniorCitizen')[['Status']]\ndf_status = df_status.agg(Single = ('Status', lambda x: list(x).count('Single') * 100 / len(x)), \n                          PartnerOnly = ('Status', lambda x: list(x).count('Partner Only') * 100  / len(x)), \n                          Both = ('Status', lambda x: list(x).count('Both') * 100  / len(x)), \n                          DependentOnly = ('Status', lambda x: list(x).count('Dependent Only') * 100  / len(x)))\ndf_status = df_status.reset_index().melt(id_vars='SeniorCitizen')\ndf_status = df_status.rename(columns={'variable':'Status'})\n\n\nplt.figure(figsize=(10,5))\n\nsns.barplot(data=df_status, \n            x ='SeniorCitizen', \n            y='value', \n            hue='Status')\n\nplt.title(\"Status comparison between senior citizen\", size=15, weight='bold')\nplt.tight_layout()\nplt.show()\n\n\n\n\nFor non-Senior citizens, ‘Single’ customers have the highest frequency, followed by ‘Both’. For Senior citizens, ‘Single’ is also the highest category, but the difference with ‘PartnerOnly’ is not as significant. From the plots above, we can also conclude that young people tend to have dependents more than older people.\n\ndf_status = df.groupby('Status')[['OnlineSecurity','OnlineBackup',\n                                  'DeviceProtection','TechSupport','StreamingTV','StreamingMovies']]\\\n                                  .agg(lambda x: list(x).count('Yes'))\n    \ndf_status['total'] = df_status.apply('sum',axis=1)\n\nfor x in df_status.drop(columns='total').columns:\n    df_status[x] = (df_status[x] * 100 / df_status.total).round()\ndf_status.drop(columns='total', inplace=True)\n\n\ndf_status.plot(kind='bar', rot=0)\nplt.title('Benefit count comparison between status (%)', size=12, weight='bold')\nplt.legend(bbox_to_anchor=(1, 1))\nplt.show()\n\n\n\n\nSingle and PartnerOnly customers tend to prefer entertainment benefits such as StreamingTV and StreamingMovies compared to other customers. Additionally, these customers show a lower preference for using TechSupport and OnlineSecurity.\n\n\n\n\nfig, axarr = plt.subplots(1,2, figsize=(12, 6))\nsns.countplot(df[df.Services != 'Phone Only'], \n              x = 'TotalBenefits', ax=axarr[0])\n\nsns.barplot(df[['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']]\\\n            .apply(lambda x: list(x).count('Yes')).reset_index(), x = 'index', y = 0, ax=axarr[1])\n\naxarr[0].set_title('TotalBenefits Count', size=12, weight='bold')\naxarr[0].set(ylabel=None)\n\naxarr[1].set(ylabel=None)\naxarr[1].set_title('Benefits Count', size=12, weight='bold')\naxarr[1].tick_params(axis='x', rotation=25)\nfig.tight_layout()\nplt.show()\n\n\n\n\nThe average number of TotalBenefits taken by customers is around 3, with StreamingTV and StreamingMovies being the most popular choices.\n\ndf_total = pd.DataFrame()\nfor x in ['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']:\n    total = list(df[df[x] == 'Yes']['Churn']).count('Yes') / len(df[df[x] == 'Yes']['Churn'])\n    df_total = pd.concat([df_total, pd.DataFrame([x],[total])])\n\n\nfig, axarr = plt.subplots(1,2, figsize=(12, 6))\nsns.barplot(df.groupby('TotalBenefits')[['Churn']]\\\n            .agg(lambda x: list(x).count('Yes') * 100 / len(x)).round().reset_index(), \n            x='TotalBenefits', \n            y='Churn', ax=axarr[0])\n\nsns.barplot(df_total.reset_index(), \n            x=0, \n            y='index', \n            ax=axarr[1])\n\naxarr[0].set_title('TotalBenefits Churn probability', size=12, weight='bold')\naxarr[0].set(ylabel=None)\n\naxarr[1].set(ylabel=None, xlabel='Benefits')\naxarr[1].set_title('Benefits Churn probability', size=12, weight='bold')\naxarr[1].tick_params(axis='x', rotation=25)\nfig.tight_layout()\nplt.show()\n\n\n\n\nWhile StreamingTV and StreamingMovies are the most favored choices, the churn probability associated with them is also the highest.\n\n\n\n\nfig, axarr = plt.subplots(1, figsize=(10, 6))\nsns.scatterplot(data=df, \n                x = 'MonthlyCharges', \n                y = 'tenure', \n                hue ='Churn', \n                s=20)\n\nplt.fill_between((68 , 97),20, alpha=0.2, color='green')\nplt.fill_between((68 , 97),52.5, 72.5, alpha=0.2, color='blue')\n\nplt.title('MonthlyCharges vs tenure vs Churn', size = 15, weight = 'bold')\nplt.show()\n\n\n\n\nHere, I have created two areas, denoted by green and blue, both focusing on MonthlyCharges in the range of 70 - 95 USD. This price range corresponds to the highest churn probability. The green area represents customers with low tenure and is predominantly occupied by churned customers, while the blue area represents customers with high tenure and is predominantly occupied by non-churned customers.\n\nplt.subplots(1, figsize=(15, 8))\nsns.scatterplot(data=df, \n                x = 'MonthlyCharges', \n                y = 'tenure', \n                s=35, \n                hue='TotalBenefits', \n                style='InternetService', \n                palette='coolwarm')\n\nplt.fill_between((68 , 97),20, alpha=0.15, color='green')\nplt.fill_between((68 , 97),52.5, 72.5, alpha=0.15, color='blue')\n\nplt.title('MonthlyCharges vs tenure vs TotalBenefits vs InternetService', size = 15, weight = 'bold')\nplt.show()\n\n\n\n\nStill on the same plot, I have added TotalBenefits and InternetService. It can be observed that in the green area, Fiber optic is the dominant InternetService with low TotalBenefits. On the other hand, the blue area is dominated by DSL with high TotalBenefits. This indicates that customers, at the same price point, tend to choose DSL with high TotalBenefits rather than Fiber optic with low TotalBenefits. Note that Fiber optic prices are doubled than DSL.\nWith the observed pattern above, we can create an important new feature, which we will refer to as ‘FO_LB’ (Fiber optic_Low benefit). I will assign a value of ‘1’ to indicate that the internet service is Fiber optic and the Totalbenefits taken are less than or equal to 3. For other cases, I will assign ‘0’.\n\ndf['FO_LB'] = df[['InternetService','TotalBenefits']].apply(\n    lambda x: 1 if (x['InternetService'] == 'Fiber optic') & (x['TotalBenefits'] &lt;= 3) else 0, axis=1)"
  },
  {
    "objectID": "ipynb/telco/final-project.html#modelling",
    "href": "ipynb/telco/final-project.html#modelling",
    "title": "Telco customer churn analysis",
    "section": "",
    "text": "One important thing to address before we proceed is considering the types of errors to make this project as realistic as possible. Typically, there are two types of errors: false positive (FP) and false negative (FN). However, in this project, I will introduce three types of errors.\n1.FP: False positive  2.FN1: False negative for customers with MonthlyCharges below 95 USD  3.FN2: False negative for customers with MonthlyCharges above 95 USD (VIP customers)  Let’s agree on the misclassification ratio, which is FP:FN1:FN2 = 1:3:5 \nIt’s important to note that this dataset is imbalanced, meaning there is a significant difference in the number of samples between the classes.\nBased on these problems, we can set up our model’s parameters as follows: 1.Hyperparameter tuning. 2.Decision threshold tuning. 3.Oversampling data using SMOTE. 4.Applying weights to the models. I will be using Random Forest, XGBoost, and Logistic Regression.\nMetrics: Custom scoring based on sample misclassification. Precision. Recall. F1_score. Once the models are evaluated using these metrics, I will interpret the best model.\n\n\n\ndf1 = df.copy()\n\n\ndf = df1.copy()\n\n\n#drop columns\ndf.drop(columns = ['Services','MonthlyChargesEstimationDifference','Status','PaperlessBilling','PaymentMethod'], inplace=True)\n\nI have dropped the columns from ‘Services’ to ‘Status’ as these columns were engineered features created for simpler exploratory data analysis (EDA). Additionally, I have also dropped the ‘PaperlessBilling’ and ‘PaymentMethod’ columns because, in the business context, these columns are considered irrelevant for determining customer churn since they represent optional ‘features’ for customers.\n\n#converting 'No internet service' to 'No' in benefit columns.\nKolomBenefit = ['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']\n\nfor x in KolomBenefit:\n    df[x] = df[x].apply(lambda x: 'No' if x == 'No internet service' else x)\n\n\n#converting 'No phone service' to 'No'\ndf['MultipleLines'] = df[x].apply(lambda x: 'No' if x == 'No phone service' else x)\n\n\n#dict to mapping string to numerical.\nvalue_mapping = {\n    'No': 0,\n    'Yes' : 1,\n    'Male' : 1,\n    'Female' : 0\n}\n\n\n#binary encoding\nbinary = list(df.drop(columns=['tenure','InternetService','MonthlyCharges','TotalCharges','TotalBenefits','Contract','FO_LB']).columns)\n\nfor col in binary:\n    df[col] = df[col].map(value_mapping).astype('int64')\n\n\n#label encoding\ndf['Contract'] = df['Contract'].apply(lambda x: 1 if x == 'Month-to-month' else 2 if x == 'One year' else 3) \n\n\n#one hot encoding\ndf = pd.get_dummies(df, columns=['InternetService'])\n\n\n#feature selection\ndf = df[['Contract','tenure','InternetService_Fiber optic','MonthlyCharges','FO_LB','InternetService_No','Churn']]\ndf = df.rename(columns={\n                'InternetService_Fiber optic':'Fiber_optic',\n                'InternetService_No':'No_internet'})\n\nHere I only choose a feature that have a strong predictive power (by using feature of importances)\n\n\n\n\n#split train-test\nX_train, X_test, y_train, y_test = train_test_split(df.drop(columns='Churn'), df.Churn.to_numpy(), test_size = 0.2, random_state=123)\nX_train = X_train.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)\n\n\n#custom function to do threshold tuning and custom metrics (used in GridSearchCV)\ndef my_scorer_2(clf, X, y_true, thres = np.arange(0.1,1,0.1)):\n    result_dict = {}\n    for threshold in np.atleast_1d(thres):\n        y_pred = (clf.predict_proba(X)[:,1] &gt; threshold).astype(int)\n        X_segment = (X['MonthlyCharges'] &gt; 95).to_numpy().astype(int)\n        y_stack = np.column_stack((X_segment, y_pred, y_true))\n        y_stack_reg, y_stack_vip = y_stack[y_stack[:,0] == 0], y_stack[y_stack[:,0] == 1]\n        cm_reg = confusion_matrix(y_stack_reg[:,2], y_stack_reg[:,1])\n        cm_vip = confusion_matrix(y_stack_vip[:,2], y_stack_vip[:,1])\n        fn_reg, fn_vip = cm_reg[1][0], cm_vip[1][0]\n        fp = cm_reg[0][1] + cm_vip[0][1]\n        loss_score = (fp * 1) + (fn_reg * 3) + (fn_vip * 5)\n        result_dict[threshold] = np.array([loss_score, metrics.precision_score(y_true, y_pred, zero_division = 0), \n                                           metrics.recall_score(y_true, y_pred), metrics.f1_score(y_true, y_pred)])\n        \n    result_np = np.array([np.insert(value, 0, key) for key, value in result_dict.items()])\n    best_np = result_np[result_np[:,1] == np.min(result_np[:,1])][0]\n    return best_np\n\ndef my_scorer_threshold(clf, X, y_true):\n    return my_scorer_2(clf, X, y_true)[0]\n\ndef my_scorer_ls(clf, X, y_true):\n    return my_scorer_2(clf, X, y_true)[1]\n\ndef my_scorer_precision(clf, X, y_true):\n    return my_scorer_2(clf, X, y_true)[2]\n\ndef my_scorer_recall(clf, X, y_true):\n    return my_scorer_2(clf, X, y_true)[3]\n\ndef my_scorer_f1(clf, X, y_true):\n    return my_scorer_2(clf, X, y_true)[4]\n        \n        \n\n\n#Grid scoring parameter\ngrid_scoring = {\n    'threshold': my_scorer_threshold,\n    'loss_score': my_scorer_ls,\n    'precision': my_scorer_precision,\n    'recall': my_scorer_recall,\n    'f1': my_scorer_f1\n}\n\n\n#define weight by missclassification cost which is FP:FN1:FN2 = 1:3:5\nX_segment = (X_train['MonthlyCharges'] &gt; 95).to_numpy().astype(int)\narr_weight = np.column_stack((X_segment, y_train))\nweight = np.apply_along_axis(lambda x: 1 if x[1] == 0 else 5 if x[0] == 1 else 3 , axis=1, arr=arr_weight)\n\n\n\n\n\n#RANDOM FOREST MODELLING\n\n#define parameter for tuning\nparam_grid_rf = {\n    'n_estimators': [250 , 400],\n    'max_depth': [10, 25, 50],\n    'min_samples_split': [25, 50, 70, 120],\n    'min_samples_leaf': [50, 75, 120],\n    'bootstrap' : [True, False]\n}\n\n#run grid search cv\nrf = GridSearchCV(estimator = RandomForestClassifier(),\n                  param_grid = param_grid_rf,\n                  cv=5,\n                  scoring = grid_scoring, refit=False, n_jobs = -1)\n\n#fit grid search cv with train data\nrf.fit(X_train, y_train)\n\n#selecting the best parameter and metrics\ngrid_result = pd.DataFrame(rf.cv_results_)\ngrid_result = grid_result[grid_result['mean_test_loss_score'] == grid_result['mean_test_loss_score'].min()][['params','mean_test_threshold','mean_test_loss_score','mean_test_precision','mean_test_recall','mean_test_f1']]\n\n#train model with the best hyperparameter\nrf = RandomForestClassifier(**grid_result.iloc[0,0])\n\n#train model with train data\nrf = rf.fit(X_train, y_train)\n\n#evaluate the model\nmb1_rf = my_scorer_2(rf, X_test, y_test, grid_result.iloc[0,1])\nmb1_rf = np.append(mb1_rf, grid_result.iloc[0,0])\n\n\n#XGBOOST MODELLING\n\n#define parameter for tuning\nparam_grid_xg = {\n    'learning_rate': [0.1, 0.01, 0.001],\n    'n_estimators': [100, 500],\n    'max_depth': [5, 10, 25],\n    'subsample': [0.8, 0.9, 1.0],\n    'colsample_bytree': [0.8, 0.9, 1.0]\n}\n\n#run grid search cv\nxg = GridSearchCV(estimator = XGBClassifier(),\n                  param_grid = param_grid_xg,\n                  cv=5,\n                  scoring = grid_scoring, refit=False, n_jobs = -1)\n\n#fit grid search cv with train data\nxg.fit(X_train, y_train)\n\n#selecting the best parameter and metrics\ngrid_result = pd.DataFrame(xg.cv_results_)\ngrid_result = grid_result[grid_result['mean_test_loss_score'] == grid_result['mean_test_loss_score'].min()][['params','mean_test_threshold','mean_test_loss_score','mean_test_precision','mean_test_recall','mean_test_f1']]\n\n#train model with the best hyperparameter\nxg = XGBClassifier(**grid_result.iloc[0,0])\n\n#train model with train data\nxg = xg.fit(X_train, y_train)\n\n#evaluate the model\nmb1_xg = my_scorer_2(xg, X_test, y_test, grid_result.iloc[0,1])\nmb1_xg = np.append(mb1_xg, grid_result.iloc[0,0])\n\n\n#LOGISTIC REGRESSION MODELLING\n\n#define parameter for tuning\nparam_grid_lg = {\n    'penalty': ['l1', 'l2'], \n    'C': [0.1, 1.0, 10.0],  \n    'solver': ['liblinear'],  \n    'max_iter': [50,100,200] \n}\n\n#run grid search cv\nlg = GridSearchCV(estimator = LogisticRegression(),\n                  param_grid = param_grid_lg,\n                  cv=5,\n                  scoring = grid_scoring, refit=False, n_jobs = -1)\n\n#fit grid search cv with train data\nlg.fit(X_train, y_train)\n\n#selecting the best parameter and metrics\ngrid_result = pd.DataFrame(lg.cv_results_)\ngrid_result = grid_result[grid_result['mean_test_loss_score'] == grid_result['mean_test_loss_score'].min()][['params','mean_test_threshold','mean_test_loss_score','mean_test_precision','mean_test_recall','mean_test_f1']]\n\n#train model with the best hyperparameter\nlg = LogisticRegression(**grid_result.iloc[0,0])\n\n#train model with train data\nlg = lg.fit(X_train, y_train)\n\n#evaluate the model\nmb1_lg = my_scorer_2(lg, X_test, y_test, grid_result.iloc[0,1])\nmb1_lg = np.append(mb1_lg, grid_result.iloc[0,0])\n\n\nresult_mb1 = pd.DataFrame([mb1_rf,mb1_xg,mb1_lg])\nresult_mb1.columns = ['threshold','score','precision','recall','f1','params']\nresult_mb1['model'] = ['MB1_RF','MB1_XG','MB1_Log_Reg']\nresult_mb1\n\n\n\n\n\n\n\n\nthreshold\nscore\nprecision\nrecall\nf1\nparams\nmodel\n\n\n\n\n0\n0.24\n539.0\n0.473101\n0.828255\n0.602216\n{'bootstrap': True, 'max_depth': 25, 'min_samp...\nMB1_RF\n\n\n1\n0.20\n558.0\n0.449190\n0.844875\n0.586538\n{'colsample_bytree': 0.8, 'learning_rate': 0.0...\nMB1_XG\n\n\n2\n0.22\n541.0\n0.467492\n0.836565\n0.599801\n{'C': 0.1, 'max_iter': 50, 'penalty': 'l2', 's...\nMB1_Log_Reg\n\n\n\n\n\n\n\n\n\n\n\n#RANDOM FOREST MODELLING\n\n#define parameter for tuning\nparam_grid_rf_smote = {\n    'class__n_estimators': [250 , 400],\n    'class__max_depth': [10, 25, 50],\n    'class__min_samples_split': [25, 50, 70, 120],\n    'class__min_samples_leaf': [50, 75, 120],\n    'class__bootstrap' : [True, False]\n}\n\n#create imbalanced pipeline to SMOTE \npipelinerf = Pipeline([\n        ('sampling', SMOTE()),\n        ('class', RandomForestClassifier())])\n\n#run grid search cv\nrf = GridSearchCV(estimator = pipelinerf,\n                  param_grid = param_grid_rf_smote,\n                  cv=5,\n                  scoring = grid_scoring, refit=False, n_jobs = -1)\n\n#fit grid search cv with train data\nrf.fit(X_train, y_train)\n\n#selecting the best parameter and metrics\ngrid_result = pd.DataFrame(rf.cv_results_)\ngrid_result = grid_result[grid_result['mean_test_loss_score'] == grid_result['mean_test_loss_score'].min()][['params','mean_test_threshold','mean_test_loss_score','mean_test_precision','mean_test_recall','mean_test_f1']]\nparams = grid_result.iloc[0,0]\n\nparams_update = {}\nfor key, value in params.items():\n    new_key = key.replace('class__', '')\n    params_update[new_key] = value\nparams = params_update\n    \n#train model with the best hyperparameter\nrf = RandomForestClassifier(**params)\n\n#train model with SMOTE train data\nX_train1, y_train1 = SMOTE().fit_resample(X_train, y_train)\nrf = rf.fit(X_train1, y_train1)\n\n#evaluate the model\nmb2_rf = my_scorer_2(rf, X_test, y_test, grid_result.iloc[0,1])\nmb2_rf = np.append(mb2_rf, params)\n\n\n#XGBOOST MODELLING\n\n#define parameter for tuning\nparam_grid_xg_smote = {\n    'class__learning_rate': [0.1, 0.01, 0.001],\n    'class__n_estimators': [100, 500],\n    'class__max_depth': [5, 10, 25],\n    'class__subsample': [0.8, 0.9, 1.0],\n    'class__colsample_bytree': [0.8, 0.9, 1.0]\n}\n\n#create imbalanced pipeline to SMOTE \npipelinexg = Pipeline([\n        ('sampling', SMOTE()),\n        ('class', XGBClassifier())])\n\n#run grid search cv\nxg = GridSearchCV(estimator = pipelinexg,\n                  param_grid = param_grid_xg_smote,\n                  cv=5,\n                  scoring = grid_scoring, refit=False, n_jobs = -1)\n\n#fit grid search cv with train data\nxg.fit(X_train, y_train)\n\n#selecting the best parameter and metrics\ngrid_result = pd.DataFrame(xg.cv_results_)\ngrid_result = grid_result[grid_result['mean_test_loss_score'] == grid_result['mean_test_loss_score'].min()][['params','mean_test_threshold','mean_test_loss_score','mean_test_precision','mean_test_recall','mean_test_f1']]\nparams = grid_result.iloc[0,0]\n\nparams_update = {}\nfor key, value in params.items():\n    new_key = key.replace('class__', '')\n    params_update[new_key] = value\nparams = params_update\n    \n#train model with the best hyperparameter\nxg = XGBClassifier(**params)\n\n#train model with SMOTE train data\nX_train1, y_train1 = SMOTE().fit_resample(X_train, y_train)\nxg = xg.fit(X_train1, y_train1)\n\n#evaluate the model\nmb2_xg = my_scorer_2(xg, X_test, y_test, grid_result.iloc[0,1])\nmb2_xg = np.append(mb2_xg, params)\n\n\n#LOGISTIC REGRESSION MODELLING\n\n#define parameter for tuning\nparam_grid_lg_smote = {\n    'class__penalty': ['l1', 'l2'], \n    'class__C': [0.1, 1.0, 10.0],  \n    'class__solver': ['liblinear'],  \n    'class__max_iter': [50,100,200] \n}\n\n\n#create imbalanced pipeline to SMOTE \npipelinelg = Pipeline([\n        ('sampling', SMOTE()),\n        ('class', LogisticRegression())])\n\n#run grid search cv\nlg = GridSearchCV(estimator = pipelinelg,\n                  param_grid = param_grid_lg_smote,\n                  cv=5,\n                  scoring = grid_scoring, refit=False, n_jobs = -1)\n\n#fit grid search cv with train data\nlg.fit(X_train, y_train)\n\n#selecting the best parameter and metrics\ngrid_result = pd.DataFrame(lg.cv_results_)\ngrid_result = grid_result[grid_result['mean_test_loss_score'] == grid_result['mean_test_loss_score'].min()][['params','mean_test_threshold','mean_test_loss_score','mean_test_precision','mean_test_recall','mean_test_f1']]\nparams = grid_result.iloc[0,0]\n\nparams_update = {}\nfor key, value in params.items():\n    new_key = key.replace('class__', '')\n    params_update[new_key] = value\nparams = params_update\n    \n#train model with the best hyperparameter\nlg = LogisticRegression(**params)\n\n#train model with SMOTE train data\nX_train1, y_train1 = SMOTE().fit_resample(X_train, y_train)\nlg = lg.fit(X_train1, y_train1)\n\n#evaluate the model\nmb2_lg = my_scorer_2(lg, X_test, y_test, grid_result.iloc[0,1])\nmb2_lg = np.append(mb2_lg, params)\n\n\nresult_mb2 = pd.DataFrame([mb2_rf,mb2_xg,mb2_lg])\nresult_mb2.columns = ['threshold','score','precision','recall','f1','params']\nresult_mb2['model'] = ['MB2_RF','MB2_XG','MB2_Log_Reg']\nresult_mb2\n\n\n\n\n\n\n\n\nthreshold\nscore\nprecision\nrecall\nf1\nparams\nmodel\n\n\n\n\n0\n0.42\n557.0\n0.454955\n0.839335\n0.590068\n{'bootstrap': False, 'max_depth': 25, 'min_sam...\nMB2_RF\n\n\n1\n0.44\n546.0\n0.472266\n0.825485\n0.600806\n{'colsample_bytree': 1.0, 'learning_rate': 0.0...\nMB2_XG\n\n\n2\n0.48\n546.0\n0.472178\n0.822715\n0.600000\n{'C': 1.0, 'max_iter': 50, 'penalty': 'l2', 's...\nMB2_Log_Reg\n\n\n\n\n\n\n\nYou see that the threshold on all models are increasing than model building 1 because SMOTE make the models learn more on the positive class.\n\n\n\n\n#RANDOM FOREST MODELLING\n\n#define parameter for tuning\nparam_grid_rf = {\n    'n_estimators': [250 , 400],\n    'max_depth': [10, 25, 50],\n    'min_samples_split': [25, 50, 70, 120],\n    'min_samples_leaf': [50, 75, 120],\n    'bootstrap' : [True, False]\n}\n\n#run grid search cv\nrf = GridSearchCV(estimator = RandomForestClassifier(),\n                  param_grid = param_grid_rf,\n                  cv=5,\n                  scoring = grid_scoring, refit=False, n_jobs = -1)\n\n#fit grid search cv with train data\nrf.fit(X_train, y_train, sample_weight = weight)\n\n#selecting the best parameter and metrics\ngrid_result = pd.DataFrame(rf.cv_results_)\ngrid_result = grid_result[grid_result['mean_test_loss_score'] == grid_result['mean_test_loss_score'].min()][['params','mean_test_threshold','mean_test_loss_score','mean_test_precision','mean_test_recall','mean_test_f1']]\n\n#train model with the best hyperparameter\nrf = RandomForestClassifier(**grid_result.iloc[0,0])\n\n#train model with train data\nrf = rf.fit(X_train, y_train, sample_weight = weight)\n\n#evaluate the model\nmb3_rf = my_scorer_2(rf, X_test, y_test, grid_result.iloc[0,1])\nmb3_rf = np.append(mb3_rf, grid_result.iloc[0,0])\n\n\n#XGBOOST MODELLING\n\n#define parameter for tuning\nparam_grid_xg = {\n    'learning_rate': [0.1, 0.01, 0.001],\n    'n_estimators': [100, 500],\n    'max_depth': [5, 10, 25],\n    'subsample': [0.8, 0.9, 1.0],\n    'colsample_bytree': [0.8, 0.9, 1.0]\n}\n\n#run grid search cv\nxg = GridSearchCV(estimator = XGBClassifier(),\n                  param_grid = param_grid_xg,\n                  cv=5,\n                  scoring = grid_scoring, refit=False, n_jobs = -1)\n\n#fit grid search cv with train data\nxg.fit(X_train, y_train, sample_weight = weight)\n\n#selecting the best parameter and metrics\ngrid_result = pd.DataFrame(xg.cv_results_)\ngrid_result = grid_result[grid_result['mean_test_loss_score'] == grid_result['mean_test_loss_score'].min()][['params','mean_test_threshold','mean_test_loss_score','mean_test_precision','mean_test_recall','mean_test_f1']]\n\n#train model with the best hyperparameter\nxg = XGBClassifier(**grid_result.iloc[0,0])\n\n#train model with train data\nxg = xg.fit(X_train, y_train, sample_weight = weight)\n\n#evaluate the model\nmb3_xg = my_scorer_2(xg, X_test, y_test, grid_result.iloc[0,1])\nmb3_xg = np.append(mb3_xg, grid_result.iloc[0,0])\n\n\n#LOGISTIC REGRESSION MODELLING\n\n#define parameter for tuning\nparam_grid_lg = {\n    'penalty': ['l1', 'l2'], \n    'C': [0.1, 1.0, 10.0],  \n    'solver': ['liblinear'],  \n    'max_iter': [50,100,200] \n}\n\n\n#run grid search cv\nlg = GridSearchCV(estimator = LogisticRegression(),\n                  param_grid = param_grid_lg,\n                  cv=5,\n                  scoring = grid_scoring, refit=False, n_jobs = -1)\n\n#fit grid search cv with train data\nlg.fit(X_train, y_train, sample_weight = weight)\n\n#selecting the best parameter and metrics\ngrid_result = pd.DataFrame(lg.cv_results_)\ngrid_result = grid_result[grid_result['mean_test_loss_score'] == grid_result['mean_test_loss_score'].min()][['params','mean_test_threshold','mean_test_loss_score','mean_test_precision','mean_test_recall','mean_test_f1']]\n\n#train model with the best hyperparameter\nlg = LogisticRegression(**grid_result.iloc[0,0])\n\n#train model with train data\nlg = lg.fit(X_train, y_train, sample_weight = weight)\n\n#evaluate the model\nmb3_lg = my_scorer_2(lg, X_test, y_test, grid_result.iloc[0,1])\nmb3_lg = np.append(mb3_lg, grid_result.iloc[0,0])\n\n\nresult_mb3 = pd.DataFrame([mb3_rf,mb3_xg,mb3_lg])\nresult_mb3.columns = ['threshold','score','precision','recall','f1','params']\nresult_mb3['model'] = ['MB3_RF','MB3_XG','MB3_Log_Reg']\nresult_mb3\n\n\n\n\n\n\n\n\nthreshold\nscore\nprecision\nrecall\nf1\nparams\nmodel\n\n\n\n\n0\n0.48\n549.0\n0.456193\n0.836565\n0.590420\n{'bootstrap': True, 'max_depth': 25, 'min_samp...\nMB3_RF\n\n\n1\n0.48\n551.0\n0.465409\n0.819945\n0.593781\n{'colsample_bytree': 0.8, 'learning_rate': 0.0...\nMB3_XG\n\n\n2\n0.44\n571.0\n0.433803\n0.853186\n0.575163\n{'C': 0.1, 'max_iter': 50, 'penalty': 'l2', 's...\nMB3_Log_Reg\n\n\n\n\n\n\n\n\nfinal_result = pd.concat([result_mb1, result_mb2, result_mb3]).sort_values('score')\nfinal_result\n\n\n\n\n\n\n\n\nthreshold\nscore\nprecision\nrecall\nf1\nparams\nmodel\n\n\n\n\n0\n0.24\n539.0\n0.473101\n0.828255\n0.602216\n{'bootstrap': True, 'max_depth': 25, 'min_samp...\nMB1_RF\n\n\n2\n0.22\n541.0\n0.467492\n0.836565\n0.599801\n{'C': 0.1, 'max_iter': 50, 'penalty': 'l2', 's...\nMB1_Log_Reg\n\n\n1\n0.44\n546.0\n0.472266\n0.825485\n0.600806\n{'colsample_bytree': 1.0, 'learning_rate': 0.0...\nMB2_XG\n\n\n2\n0.48\n546.0\n0.472178\n0.822715\n0.600000\n{'C': 1.0, 'max_iter': 50, 'penalty': 'l2', 's...\nMB2_Log_Reg\n\n\n0\n0.48\n549.0\n0.456193\n0.836565\n0.590420\n{'bootstrap': True, 'max_depth': 25, 'min_samp...\nMB3_RF\n\n\n1\n0.48\n551.0\n0.465409\n0.819945\n0.593781\n{'colsample_bytree': 0.8, 'learning_rate': 0.0...\nMB3_XG\n\n\n0\n0.42\n557.0\n0.454955\n0.839335\n0.590068\n{'bootstrap': False, 'max_depth': 25, 'min_sam...\nMB2_RF\n\n\n1\n0.20\n558.0\n0.449190\n0.844875\n0.586538\n{'colsample_bytree': 0.8, 'learning_rate': 0.0...\nMB1_XG\n\n\n2\n0.44\n571.0\n0.433803\n0.853186\n0.575163\n{'C': 0.1, 'max_iter': 50, 'penalty': 'l2', 's...\nMB3_Log_Reg\n\n\n\n\n\n\n\nAnalysis of how the models performed: 1.RandomForest performs best with hyperparameter tuning and a lower decision threshold. However, this model performs worst when using SMOTE. 2.LogisticRegression performs worst when using ‘sample weighting’. 3.XGBoost performs best when using SMOTE. 4.It’s important to note that all models produce similar results when using their best parameters and conditions. 5.In my opinion, the greatest impact is achieved by using hyperparameter tuning and decision threshold tuning, rather than using SMOTE and weighting techniques."
  },
  {
    "objectID": "ipynb/telco/final-project.html#model-interpretation",
    "href": "ipynb/telco/final-project.html#model-interpretation",
    "title": "Telco customer churn analysis",
    "section": "",
    "text": "In this section, I want to show you how to interpret a tree-based model, such as Random Forest, so we can have a better understanding of how the model actually works.\n\n#selecting the best parameter for random forest\nrf_param = final_result[final_result['model'] == 'MB1_RF']['params'][0]\nrf_param\n\n{'bootstrap': True,\n 'max_depth': 25,\n 'min_samples_leaf': 50,\n 'min_samples_split': 50,\n 'n_estimators': 250}\n\n\n\nrf = RandomForestClassifier(**rf_param)\nrf.fit(X_train, y_train)\n\nRandomForestClassifier(max_depth=25, min_samples_leaf=50, min_samples_split=50,\n                       n_estimators=250)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(max_depth=25, min_samples_leaf=50, min_samples_split=50,\n                       n_estimators=250)\n\n\n\n\nWhat is ICE? It is a plot that shows how a model makes predictions based on changing the value of one or more features, while keeping the values of other features constant. This provides us with more insights and understanding of how the model treats features to make predictions. ICE works per row (or per customer in this case), and PDP is simply the average of ICE.\n\ndef pdp_ice_plot(model, df_test, column, clusters=True):\n    df_test = df_test.copy()\n    pdp_isolate = pdp.PDPIsolate(model = model, df = df_test, \n                      num_grid_points=15,\n                      model_features = df_test.columns, \n                      feature = column, feature_name=column)\n    pdp_isolate.plot(\n                center=True, plot_lines=True,\n                cluster=clusters, n_cluster_centers=5,cluster_method='accurate',\n                plot_pts_dist=False, to_bins=True,\n                show_percentile=False, which_classes=None,\n                figsize=(10,6), dpi=300,\n                ncols=2, plot_params={\"pdp_hl\": True},\n                engine='matplotlib', template='plotly_white')\n    plt.show()\n    \n\n\npdp_ice_plot(rf, X_test, 'MonthlyCharges', False)\n\nobtain pred_func from the provided model.\n\n\n\n\n\n\n\n\nThe light blue lines represent ICE (Individual Conditional Expectation), and the yellowish blue line represents PDP (Partial Dependence Plot). The X-axis represents MonthlyCharges, while the Y-axis represents the change in prediction probability. At MonthlyCharges of 60.7 USD, you can observe that some customers experience a significant increase in churn probability as the MonthlyCharges increase. However, it is important to note that not all customers have the same response. Some customers are minimally affected, and some may not be affected at all.\n\npdp_ice_plot(rf, X_test, 'MonthlyCharges')\n\nobtain pred_func from the provided model.\n\n\n\n\n\n\n\n\nThis is the same plot as above, but I have grouped the ICE into 5 clusters for easier viewing and analysis. You can see that there are some customers who experience a significant increase in churn probability as the MonthlyCharges increase.\n\npdp_ice_plot(rf, X_test, 'tenure')\n\nobtain pred_func from the provided model.\n\n\n\n\n\n\n\n\nThe longer the tenure, the lower the churn probability. However, the effect is not the same for all customers. Some customers are greatly affected, while others are barely affected.\n\npdp_ice_plot(rf, X_test, 'Contract')\n\nobtain pred_func from the provided model.\n\n\n\n\n\n\n\n\nThe same also goes with Contract. Longer contract means lower churn probability.\n\nX_test_copy = X_test.copy()\npdp_interact = pdp.PDPInteract(model=rf, df=X_test_copy,\n                              num_grid_points=6,\n                              model_features = X_test.columns, \n                              features=['MonthlyCharges','tenure'], \n                              feature_names=['MonthlyCharges','tenure'])\npdp_interact.plot(\n            plot_type=\"grid\", plot_pdp=True,\n            to_bins=True, show_percentile=False,\n            which_classes=None, figsize=(10,6),\n            dpi=300, ncols=2, plot_params=None, \n            engine='matplotlib', template='plotly_white',\n)\nplt.show()\n\nobtain pred_func from the provided model.\nobtain pred_func from the provided model.\nobtain pred_func from the provided model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s analyze the combination of MonthlyCharges and tenure. We can observe a spike in churn probability for MonthlyCharges ranging from 56.3 USD to 79.2 USD, particularly for customers with a tenure of less than 7 months.\n\nX_test_copy = X_test.copy()\npdp_interact = pdp.PDPInteract(model=rf, df=X_test_copy,\n                              num_grid_points=10,\n                              model_features = X_test.columns, \n                              features=['tenure','Contract'], \n                              feature_names=['tenure','Contract'])\npdp_interact.plot(\n            plot_type=\"grid\", plot_pdp=True,\n            to_bins=True, show_percentile=False,\n            which_classes=None, figsize=(10,6),\n            dpi=300, ncols=2, plot_params=None, \n            engine='matplotlib', template='plotly_white',\n)\nplt.show()\n\nobtain pred_func from the provided model.\nobtain pred_func from the provided model.\nobtain pred_func from the provided model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe model produces similar churn probabilities for customers with a combination of a 2-year contract and low tenure compared to those with a month-to-month contract and medium tenure (24-36 months).\nYou can see how PDP and ICE plots can be very beneficial in understanding how the model utilizes features to make predictions. In the next section, I will demonstrate how to assess the model’s prediction confidence level.\n\n\n\nTree-based models like RandomForest make predictions by using the mean of all the trees’ prediction probabilities. However, instead of solely relying on the mean, we can also calculate the standard deviation. A higher standard deviation indicates lower confidence in the predictions. Additionally, we can utilize confidence intervals, such as 95% or even more extreme at 99%.\n\n#extract all trees' prediction probability per row\npredict = np.stack([x.predict_proba(X_test)[:,1] for x in rf.estimators_])\n\n\n#assign mean and std. deviation of trees' prediction probability.\ndf_pred = X_test.copy()\ndf_pred['avg'] = np.round(np.mean(predict, axis = 0) * 100, 2)\ndf_pred['std_dev'] = np.round(np.std(predict, axis = 0) * 100, 2)\n\n\nplt.figure(figsize=(10,6))\nsns.histplot(df_pred['std_dev'],color='skyblue', kde=True, edgecolor='none')\nplt.title('Standard deviation distribution', weight='bold')\nplt.show()\n\n\n\n\nMost of predictions have std.deviation under 10%. Let’s calculate confidence interval with 99%.\n\ndf_pred['CI-99%'] = (2.576 * df_pred['std_dev'] / np.sqrt(len(predict))) * 100 / (df_pred['avg'])\n\n\ndf_pred[df_pred.avg &gt; 40].sort_values('CI-99%', ascending=False).head(5)\n\n\n\n\n\n\n\n\nContract\ntenure\nFiber_optic\nMonthlyCharges\nFO_LB\nNo_internet\navg\nstd_dev\nCI-99%\n\n\n\n\n852\n2\n7\n1\n94.05\n1\n0\n40.06\n25.08\n10.199818\n\n\n499\n1\n58\n1\n98.70\n1\n0\n41.05\n18.24\n7.239149\n\n\n1186\n1\n59\n1\n101.10\n1\n0\n40.37\n16.73\n6.751699\n\n\n1137\n1\n15\n1\n96.30\n0\n0\n48.14\n19.69\n6.663701\n\n\n1184\n1\n10\n1\n92.50\n0\n0\n48.17\n19.12\n6.466765\n\n\n\n\n\n\n\nLet’s consider the example of row 1. The model predicts a 40% probability of churn for the customer, with a confidence interval of +- 10%. By default, the model’s output indicates that the customer will not churn. However, due to the high confidence interval, it is safer to assume that the customer will churn.\nChecking the standard deviation and confidence interval of the trees is extremely useful, particularly when the cost of ‘False Negative’ is significant and can have severe consequences."
  },
  {
    "objectID": "ipynb/houses/house_tangerang.html",
    "href": "ipynb/houses/house_tangerang.html",
    "title": "Tangerang house price prediction",
    "section": "",
    "text": "• Introduction  • Data Preparation • Basic exploratory data analysis      • Descriptive statistic      • Univariate analysis      • Multivariate analysis  • Deep-dive exploratory data analysis  • Modelling     • Model building combination 1      • Model building combination 2      • Model building combination 3      • Model building combination 4      • Model building combination 5      • Model building combination 6  • Choosing the best model combination  • Model evaluation and interpretation      • Residuals plot      • PDP & ICE plots  • Conclusion \n\n\n\nThis project revolves around predicting house prices in Tangerang City, which encompasses more than 40 regions. The data for this project was gathered by scraping information from one of Indonesia’s largest online real estate listing platforms. After conducting this exploratory phase, I will deploy an application publicly to assist people in estimating house prices.\n\n\n\nImporting libraries, cleaning data and choosing features.\n\n#import libraries\nimport numpy as np\nimport pandas as pd\nimport ast\nimport re\nfrom sklearn.impute import KNNImputer\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\nimport warnings\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\nfrom sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\nfrom xgboost import XGBRegressor\nimport xgboost as xgb\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\nimport plotly.express as px\nimport matplotlib.patches as mpatches\nfrom pdpbox import pdp\n\n#set some library settings\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_columns', None)\nsns.set()\nsns.set_style('darkgrid')\nmpl.rcParams['figure.dpi'] = 150 \nplt.rc('legend',**{'fontsize':10})\n\n\n#read csv\ndf = pd.read_csv('tangerang.csv')\n\n\n#first look of data\ndf.head()\n\n\n\n\n\n\n\n\nharga\nalamat\nfasilitas\nspesifikasi\n\n\n\n\n0\nRp 2,6 Miliar\nBSD City, Tangerang\nTempat Jemuran,Keamanan 24 jam,Playground,Wast...\n[['Kamar Tidur', '3'], ['Kamar Mandi', '2'], [...\n\n\n1\nRp 20 Miliar\nBSD City, Tangerang\nNaN\n[['Kamar Tidur', '5'], ['Kamar Mandi', '5'], [...\n\n\n2\nRp 1,68 Miliar\nBSD City, Tangerang\nTaman,Tempat Jemuran,Keamanan 24 jam\n[['Kamar Tidur', '2'], ['Kamar Mandi', '2'], [...\n\n\n3\nRp 1,61 Miliar\nBSD City, Tangerang\nTaman,Tempat Jemuran,Keamanan 24 jam\n[['Kamar Tidur', '4'], ['Kamar Mandi', '3'], [...\n\n\n4\nRp 2,14 Miliar\nBSD City, Tangerang\nTaman,Tempat Jemuran,Keamanan 24 jam\n[['Kamar Tidur', '3'], ['Kamar Mandi', '3'], [...\n\n\n\n\n\n\n\n\n#change 'spesifikasi' to type list and explode it to become columns\ndf['spesifikasi'] = df.spesifikasi.apply(ast.literal_eval)\ndf = df.explode('spesifikasi')\n\n\n#split into 'keterangan' and 'qty'\ndf['keterangan'] = df.spesifikasi.str[0]\ndf['qty'] = df.spesifikasi.str[1]\n\n\ndf_columns = df[['harga','alamat','fasilitas']].reset_index().drop_duplicates('index').set_index('index')\ndf_pivot = df.pivot(columns='keterangan', values='qty').rename_axis(None, axis=1)\ndf = pd.concat([df_columns, df_pivot], axis = 1)\n\n\nCode above is about to transform ‘keterangan’ into several columns.\n\n\n#new transformed dataframe\ndf.head()\n\n\n\n\n\n\n\n\nharga\nalamat\nfasilitas\nCarport\nDapur\nDaya Listrik\nGarasi\nHadap\nHook\nID Iklan\nJumlah Lantai\nKamar Mandi\nKamar Mandi Pembantu\nKamar Pembantu\nKamar Tidur\nKondisi Perabotan\nKondisi Properti\nKonsep dan Gaya Rumah\nLebar Jalan\nLuas Bangunan\nLuas Tanah\nMaterial Bangunan\nMaterial Lantai\nNomor Lantai\nPemandangan\nPeriode Sewa\nRuang Makan\nRuang Tamu\nSertifikat\nSumber Air\nTahun Di Renovasi\nTahun Dibangun\nTerjangkau Internet\nTipe Properti\n\n\n\n\n0\nRp 2,6 Miliar\nBSD City, Tangerang\nTempat Jemuran,Keamanan 24 jam,Playground,Wast...\n1\nNaN\nNaN\nNaN\nNaN\nTidak\nhos14814642\n1\n2\n2\n1\n3\nNaN\nBagus\nMinimalis Modern\n3 Mobil\n91 m²\n91 m²\nBatako\nGranit\nNaN\nTaman Kota\nNaN\nYa\nYa\nLainnya (PPJB,Girik,Adat,dll)\nPAM atau PDAM\nNaN\nNaN\nYa\nRumah\n\n\n1\nRp 20 Miliar\nBSD City, Tangerang\nNaN\nNaN\n2\n7700 Watt\nNaN\nNaN\nTidak\nhos13101318\n2\n5\n1\n1\n5\nUnfurnished\nBagus\nNaN\nNaN\n465 m²\n396 m²\nNaN\nNaN\nNaN\nNaN\nNaN\nYa\nYa\nLainnya (PPJB,Girik,Adat,dll)\nPAM atau PDAM\nNaN\nNaN\nTidak\nRumah\n\n\n2\nRp 1,68 Miliar\nBSD City, Tangerang\nTaman,Tempat Jemuran,Keamanan 24 jam\n1\n1\n1300 Watt\nNaN\nUtara\nTidak\nhos14850708\n1\n2\nNaN\nNaN\n2\nUnfurnished\nBagus\nNaN\nNaN\n70 m²\n105 m²\nNaN\nNaN\nNaN\nNaN\nNaN\nYa\nYa\nHGB - Hak Guna Bangunan\nPAM atau PDAM\nNaN\nNaN\nTidak\nRumah\n\n\n3\nRp 1,61 Miliar\nBSD City, Tangerang\nTaman,Tempat Jemuran,Keamanan 24 jam\n1\n1\n2200 Watt\nNaN\nTimur Laut\nTidak\nhos14850789\n2\n3\nNaN\nNaN\n4\nUnfurnished\nBagus\nNaN\nNaN\n174 m²\n144 m²\nNaN\nNaN\nNaN\nNaN\nNaN\nYa\nYa\nSHM - Sertifikat Hak Milik\nPAM atau PDAM\nNaN\nNaN\nTidak\nRumah\n\n\n4\nRp 2,14 Miliar\nBSD City, Tangerang\nTaman,Tempat Jemuran,Keamanan 24 jam\n1\n1\n5500 Watt\nNaN\nUtara\nTidak\nhos14003997\n3\n3\nNaN\n1\n3\nUnfurnished\nBagus\nNaN\nNaN\n170 m²\n105 m²\nNaN\nNaN\nNaN\nNaN\nNaN\nYa\nYa\nSHM - Sertifikat Hak Milik\nPAM atau PDAM\nNaN\nNaN\nTidak\nRumah\n\n\n\n\n\n\n\n\n#remove duplicate data / 'iklan' (ads)\ndf = df.drop_duplicates('ID Iklan').reset_index(drop=True)\n\n\ndf.insert(3, 'komplek', df[['fasilitas']].fillna('none').apply(lambda z: 'ya' if any([x in z.fasilitas.lower() for x in ['lapangan','gym','jogging','playground','one gate system']]) else 'tidak', axis = 1))\ndf.insert(1, 'Kecamatan', df.alamat.str.split(',').str[0])\ndf.insert(2, 'Kota', df.alamat.str.split(',').str[1])\ndf.drop(columns=['alamat','fasilitas'], inplace=True)\n\n\nCreate 3 new columns: 1.’komplek’ = check if the house is in a ‘komplek’ or not. 2.’Kecamatan’ = ‘kecamatan’ of the address. 3.’Kota’ = ‘kota’ of the address.\n\n\n#ensure the selected city is Tangerang\ndf = df[df.Kota == ' Tangerang']\ndf.drop(columns = ['Kota'], inplace=True)\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 22667 entries, 0 to 22673\nData columns (total 34 columns):\n #   Column                 Non-Null Count  Dtype \n---  ------                 --------------  ----- \n 0   harga                  22667 non-null  object\n 1   Kecamatan              22667 non-null  object\n 2   komplek                22667 non-null  object\n 3   Carport                14104 non-null  object\n 4   Dapur                  12157 non-null  object\n 5   Daya Listrik           18287 non-null  object\n 6   Garasi                 6450 non-null   object\n 7   Hadap                  8528 non-null   object\n 8   Hook                   18764 non-null  object\n 9   ID Iklan               22667 non-null  object\n 10  Jumlah Lantai          20569 non-null  object\n 11  Kamar Mandi            21871 non-null  object\n 12  Kamar Mandi Pembantu   8945 non-null   object\n 13  Kamar Pembantu         9597 non-null   object\n 14  Kamar Tidur            21864 non-null  object\n 15  Kondisi Perabotan      16139 non-null  object\n 16  Kondisi Properti       19797 non-null  object\n 17  Konsep dan Gaya Rumah  9493 non-null   object\n 18  Lebar Jalan            12045 non-null  object\n 19  Luas Bangunan          22624 non-null  object\n 20  Luas Tanah             22658 non-null  object\n 21  Material Bangunan      7709 non-null   object\n 22  Material Lantai        8380 non-null   object\n 23  Nomor Lantai           0 non-null      object\n 24  Pemandangan            9987 non-null   object\n 25  Periode Sewa           1 non-null      object\n 26  Ruang Makan            11288 non-null  object\n 27  Ruang Tamu             18765 non-null  object\n 28  Sertifikat             22489 non-null  object\n 29  Sumber Air             13236 non-null  object\n 30  Tahun Di Renovasi      2856 non-null   object\n 31  Tahun Dibangun         8878 non-null   object\n 32  Terjangkau Internet    18761 non-null  object\n 33  Tipe Properti          22667 non-null  object\ndtypes: object(34)\nmemory usage: 6.1+ MB\n\n\n\n\nThere are 29 missing columns. After carefully reviewing each house one by one in the context, I categorized the missing columns into: \n\n\n1.Missing Completely At Random This means that the values are not input by the users due to accidents, forgetfulness, or oversight. These columns are:  Daya Listrik, Kamar Mandi, Kamar Tidur, Luas Bangunan, Luas Tanah, Sertifikat, Sumber Air, Hook\n\n\n2.Missing At Random This means that the values are not input by the users because these values can be obtained from other sources, such as photos or other information. These columns are:  Carport, Dapur, Garasi, Kamar Mandi Pembantu, Kamar Pembantu, Kondisi Perabotan, Kondisi Properti, Jumlah Lantai, Ruang Makan, Ruang Tamu\n\n\n3.Missing Not At Random This means that the values are not input by the users because they either do not know the value or they perceive it as unimportant to input the value. These columns are:  Konsep dan Gaya Rumah, Hadap, Lebar Jalan, Material Bangunan, Material Lantai, Nomor Lantai, Pemandangan, Periode Sewa, Tahun Di Renovasi, Tahun Dibangun, Terjangkau Internet ***\n\n\n#create a function to categorize multiple 'perabot' conditions into more general category\ndef perabot(kondisi):\n    kondisi = str(kondisi)\n    if kondisi.lower() in ['unfurnished','butuh renovasi']:\n        return 'Unfurnished'\n    elif kondisi.lower() in ['furnished','bagus','bagus sekali','baru','sudah renovasi','semi furnished']:\n        return 'Furnished'\n    else:\n        return np.nan\n\ndf['Kondisi Perabotan'] = df[['Kondisi Perabotan']].apply(lambda x: perabot(x['Kondisi Perabotan']), axis = 1)\n\n\n#filtered only houses and not other property\ndf = df[df['Tipe Properti'] == 'Rumah']\n\n\n#extract prices from column 'harga' into integer\ndef price_extract(price):\n    if \"Triliun\" in price:\n        numbers = re.findall(r'\\d+\\.\\d+|\\d+', price)\n        numbers = float(\".\".join(numbers))\n        return int(numbers * 1000000)\n    elif \"Miliar\" in price:\n        numbers = re.findall(r'\\d+\\.\\d+|\\d+', price)\n        numbers = float(\".\".join(numbers))\n        return int(numbers * 1000)\n    elif \"Juta\" in price:\n        numbers = re.findall(r'\\d+\\.\\d+|\\d+', price)\n        numbers = float(\".\".join(numbers))\n        return int(numbers * 1)      \n\ndf.insert(1, 'price',  df[['harga']].apply(lambda x: price_extract(x.harga), axis = 1))\ndf.drop(columns = ['harga'], inplace = True)\n\n\nHere, I converted the ‘price’ column into integers and divided it by one million to make it easier to view and analyze.\n\n\n#move 'ID Iklan' column to left\ndf.insert(3, 'ID',  df['ID Iklan'])\n\n\n#filter Sertifikat to SHM only\ndf = df[df.Sertifikat == 'SHM - Sertifikat Hak Milik']\n\n\n#drop useless columns\ndf.drop(columns=['Kondisi Properti','Sertifikat','Hook','ID Iklan','Lebar Jalan','Dapur','Tipe Properti','Terjangkau Internet','Sumber Air','Ruang Tamu','Ruang Makan','Carport','Garasi','Tahun Di Renovasi','Tahun Dibangun','Hadap','Konsep dan Gaya Rumah','Material Bangunan','Material Lantai','Nomor Lantai','Pemandangan','Periode Sewa'], inplace = True, errors='ignore')\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 17746 entries, 3 to 22673\nData columns (total 13 columns):\n #   Column                Non-Null Count  Dtype \n---  ------                --------------  ----- \n 0   price                 17746 non-null  int64 \n 1   Kecamatan             17746 non-null  object\n 2   komplek               17746 non-null  object\n 3   ID                    17746 non-null  object\n 4   Daya Listrik          14640 non-null  object\n 5   Jumlah Lantai         16312 non-null  object\n 6   Kamar Mandi           17110 non-null  object\n 7   Kamar Mandi Pembantu  6714 non-null   object\n 8   Kamar Pembantu        7296 non-null   object\n 9   Kamar Tidur           17103 non-null  object\n 10  Kondisi Perabotan     12942 non-null  object\n 11  Luas Bangunan         17719 non-null  object\n 12  Luas Tanah            17743 non-null  object\ndtypes: int64(1), object(12)\nmemory usage: 1.9+ MB\n\n\n\ndf = df[~df['Luas Tanah'].isna()].reset_index(drop=True)\ndf = df[~((df['Kamar Mandi'].isna()) & (df['Kamar Mandi Pembantu'].isna()) & (df['Kamar Pembantu'].isna()) & (df['Kamar Tidur'].isna()))].reset_index(drop=True)\n\n\nI removed rows that lacked critical information, such as ‘Luas Tanah’ and the total number of ‘Kamar’.\n\n\nfor kol in ['Kamar Mandi','Kamar Tidur']:\n    df[kol] = df[kol].fillna(1)\n\ndf['Luas Bangunan'] = df['Luas Bangunan'].fillna(df['Luas Tanah'])\ndf['Kamar Mandi Pembantu'] = df['Kamar Mandi Pembantu'].fillna(0)\ndf['Kamar Pembantu'] = df['Kamar Mandi Pembantu'].fillna(0)\n\n\nThe columns mentioned above have a small number of missing values. I filled in the missing values for ‘Kamar Mandi’ and ‘Kamar Tidur’ with ‘1’ because there is at least one of each of them in a house. For ‘Kamar & Kamar Mandi Pembantu,’ I filled in ‘0’.\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 17149 entries, 0 to 17148\nData columns (total 13 columns):\n #   Column                Non-Null Count  Dtype \n---  ------                --------------  ----- \n 0   price                 17149 non-null  int64 \n 1   Kecamatan             17149 non-null  object\n 2   komplek               17149 non-null  object\n 3   ID                    17149 non-null  object\n 4   Daya Listrik          14208 non-null  object\n 5   Jumlah Lantai         15731 non-null  object\n 6   Kamar Mandi           17149 non-null  object\n 7   Kamar Mandi Pembantu  17149 non-null  object\n 8   Kamar Pembantu        17149 non-null  object\n 9   Kamar Tidur           17149 non-null  object\n 10  Kondisi Perabotan     12890 non-null  object\n 11  Luas Bangunan         17149 non-null  object\n 12  Luas Tanah            17149 non-null  object\ndtypes: int64(1), object(12)\nmemory usage: 1.7+ MB\n\n\n\n#If a LB is bigger than LT, I assume that the houses has more than 1 floor.\ndef conditions(x):\n    if pd.isnull(x['Jumlah Lantai']):\n        if x['Luas Bangunan'] &gt; x['Luas Tanah']:\n            return '2'\n        else:\n            return '1'\n    else:\n        return x['Jumlah Lantai']\n\ndf['Jumlah Lantai'] = df[['Jumlah Lantai','Luas Bangunan','Luas Tanah']].apply(lambda x: conditions(x) , axis = 1 )\n\n\n#I replaced certain ambiguous values in the 'Daya Listrik' column with 'NaN' so it can be imputed with KNN Imputer\ncondition = ((df['Daya Listrik'] == 'Lainnya Watt') | (df['Daya Listrik'] == 'lainnya Watt'))\ndf.loc[condition, 'Daya Listrik'] = np.nan\n\n\n#change data types\ndf['Daya Listrik'] = df['Daya Listrik'].str.replace('Watt','').astype('Float64')\ndf['Luas Bangunan'] = df['Luas Bangunan'].str.replace('m²','').astype(int)\ndf['Luas Tanah'] = df['Luas Tanah'].str.replace('m²','').astype(int)\n\n\n#impute with KNN\ndf['avg_bangunan'] = df.groupby('Daya Listrik')['Luas Bangunan'].transform('median').fillna(df['Luas Bangunan'])\nlistrik_impute = pd.DataFrame(KNNImputer(n_neighbors=1).fit_transform(df[['Daya Listrik','avg_bangunan']]))\ndf['Daya Listrik'] = listrik_impute[0]\ndf.drop(columns=['avg_bangunan'], inplace=True)\n\n\n#I selected only the 'Kecamatan' with more than 100 samples to ensure a reliable result\ndf = df[ df.groupby('Kecamatan')['price'].transform('count') &gt; 100]\n\n\ndf['price/m'] = df.price / (df['Luas Tanah'] + df['Luas Bangunan'])\n\n\n#create dictionary to map 'Kondisi Perabotan'\nvalue_mapping = {\n    'Unfurnished': 1,\n    'Furnished' : 2,\n}\n\nreverse_mapping = {\n    1.0 : 'Unfurnished',\n    2.0 : 'Furnished',\n}\n\n\ndf['Kondisi Perabotan'] = df['Kondisi Perabotan'].map(value_mapping)\n\n\nfor kec in df.Kecamatan.unique() :\n    df1 = df[df.Kecamatan == kec][['Kondisi Perabotan','price/m']].copy().reset_index(drop=True)\n    df1['price/m'] = df1.groupby(['Kondisi Perabotan'])['price/m'].transform('median').fillna(df1['price/m'])\n\n    imputer = KNNImputer(n_neighbors = 1)\n    imputer.fit(df1[['Kondisi Perabotan','price/m']])\n    missing_values = df.loc[df[df.Kecamatan == kec].index,['Kondisi Perabotan','price/m']]\n    df.loc[df[df.Kecamatan == kec].index,['Kondisi Perabotan','price/m']] = imputer.transform(missing_values)\n        \ndf.reset_index(drop=True, inplace=True)\ndf['Kondisi Perabotan'] = df['Kondisi Perabotan'].map(reverse_mapping)\n\n\nTo impute missing values for ‘Kondisi Perabotan’, first gather the ‘price/m’ data for each ‘Kecamatan’. Once we have this data, we can observe that ‘Kondisi Perabotan’ prices vary significantly, with furnished properties being considerably more expensive than unfurnished ones. Then, we will use the KNN Imputer to fill in the missing values with the nearest neighbors’ values.\n\n\ndf.drop(columns = ['price/m'], inplace = True)\n\n\n#change data types\ndf = df.convert_dtypes(convert_string = False)\nfor column in ['Kamar Mandi','Kamar Mandi Pembantu','Kamar Pembantu','Kamar Tidur','price','Daya Listrik','Luas Bangunan','Luas Tanah']:\n    df[column] = df[column].astype(int)\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 17050 entries, 0 to 17049\nData columns (total 13 columns):\n #   Column                Non-Null Count  Dtype \n---  ------                --------------  ----- \n 0   price                 17050 non-null  int64 \n 1   Kecamatan             17050 non-null  object\n 2   komplek               17050 non-null  object\n 3   ID                    17050 non-null  object\n 4   Daya Listrik          17050 non-null  int64 \n 5   Jumlah Lantai         17050 non-null  object\n 6   Kamar Mandi           17050 non-null  int64 \n 7   Kamar Mandi Pembantu  17050 non-null  int64 \n 8   Kamar Pembantu        17050 non-null  int64 \n 9   Kamar Tidur           17050 non-null  int64 \n 10  Kondisi Perabotan     17050 non-null  object\n 11  Luas Bangunan         17050 non-null  int64 \n 12  Luas Tanah            17050 non-null  int64 \ndtypes: int64(8), object(5)\nmemory usage: 1.7+ MB\n\n\n\ndf.drop(columns = 'ID', inplace = True)\ndf.duplicated().sum()\n\n2877\n\n\n\nAfter getting rid of the ‘ID’ column, you’ll notice there are still lots of duplicates left. It seems like the same ads are being posted multiple times. Let’s go ahead and clean those up.\n\n\ndf = df.drop_duplicates()\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 14173 entries, 0 to 17049\nData columns (total 12 columns):\n #   Column                Non-Null Count  Dtype \n---  ------                --------------  ----- \n 0   price                 14173 non-null  int64 \n 1   Kecamatan             14173 non-null  object\n 2   komplek               14173 non-null  object\n 3   Daya Listrik          14173 non-null  int64 \n 4   Jumlah Lantai         14173 non-null  object\n 5   Kamar Mandi           14173 non-null  int64 \n 6   Kamar Mandi Pembantu  14173 non-null  int64 \n 7   Kamar Pembantu        14173 non-null  int64 \n 8   Kamar Tidur           14173 non-null  int64 \n 9   Kondisi Perabotan     14173 non-null  object\n 10  Luas Bangunan         14173 non-null  int64 \n 11  Luas Tanah            14173 non-null  int64 \ndtypes: int64(8), object(4)\nmemory usage: 1.4+ MB\n\n\n\ndf = df[df['Jumlah Lantai'].astype(int) &lt;= 4]\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 14161 entries, 0 to 17049\nData columns (total 12 columns):\n #   Column                Non-Null Count  Dtype \n---  ------                --------------  ----- \n 0   price                 14161 non-null  int64 \n 1   Kecamatan             14161 non-null  object\n 2   komplek               14161 non-null  object\n 3   Daya Listrik          14161 non-null  int64 \n 4   Jumlah Lantai         14161 non-null  object\n 5   Kamar Mandi           14161 non-null  int64 \n 6   Kamar Mandi Pembantu  14161 non-null  int64 \n 7   Kamar Pembantu        14161 non-null  int64 \n 8   Kamar Tidur           14161 non-null  int64 \n 9   Kondisi Perabotan     14161 non-null  object\n 10  Luas Bangunan         14161 non-null  int64 \n 11  Luas Tanah            14161 non-null  int64 \ndtypes: int64(8), object(4)\nmemory usage: 1.4+ MB\n\n\n\ndf5 = df.copy()\n\n\n#rename columns\ndf.rename(columns={'Daya Listrik': 'Listrik', \n                   'Jumlah Lantai': 'Lantai',\n                   'Kamar Mandi': 'KM',\n                   'Kamar Mandi Pembantu': 'KMP',\n                   'Kamar Pembantu': 'KP',\n                   'Kamar Tidur': 'KT',\n                   'Kondisi Perabotan': 'Kondisi',\n                   'Luas Bangunan': 'LB',\n                   'Luas Tanah': 'LT'}, inplace=True)\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 14161 entries, 0 to 17049\nData columns (total 12 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   price      14161 non-null  int64 \n 1   Kecamatan  14161 non-null  object\n 2   komplek    14161 non-null  object\n 3   Listrik    14161 non-null  int64 \n 4   Lantai     14161 non-null  object\n 5   KM         14161 non-null  int64 \n 6   KMP        14161 non-null  int64 \n 7   KP         14161 non-null  int64 \n 8   KT         14161 non-null  int64 \n 9   Kondisi    14161 non-null  object\n 10  LB         14161 non-null  int64 \n 11  LT         14161 non-null  int64 \ndtypes: int64(8), object(4)\nmemory usage: 1.4+ MB\n\n\n\ndf.Kecamatan.unique()\n\narray(['BSD City', 'Poris', 'Karang Tengah', 'Pondok Benda',\n       'Tangerang Kota', 'BSD Delatinos', 'Serua', 'Karawaci', 'Jombang',\n       'Cikupa', 'Curug', 'Suradita', 'Panongan', 'BSD Residence One',\n       'Kelapa Dua', 'Pondok Ranji', 'Ciledug', 'Pondok Cabe',\n       'Gading Serpong', 'Pagedangan', 'Legok', 'Pinang', 'Cikokol',\n       'Dadap', 'BSD Foresta', 'BSD Green Wich', 'Cireundeu',\n       'BSD Anggrek Loka', 'Cipondoh', 'BSD Nusaloka', 'BSD Puspita Loka',\n       'Gading Serpong Pondok Hijau Golf', 'BSD The Green', 'Cipadu',\n       'Larangan', 'BSD Giri Loka', 'Cikupa Citra Raya', 'Modernland',\n       'Benda', 'Rempoa', 'Alam Sutera', 'BSD', 'Cisauk', 'Graha Raya',\n       'Lippo Karawaci'], dtype=object)\n\n\n\n\n\n\n#Let's split the categorical and numerical columns into separate dataframes for easier analysis.\ndfnum = df._get_numeric_data()\ndfcat = df.drop(columns = dfnum.columns)\n\n\n\n\n#numerical columns describe.\ndfnum.describe()\n\n\n\n\n\n\n\n\nprice\nListrik\nKM\nKMP\nKP\nKT\nLB\nLT\n\n\n\n\ncount\n1.416100e+04\n14161.000000\n14161.000000\n14161.000000\n14161.000000\n14161.000000\n14161.000000\n1.416100e+04\n\n\nmean\n4.525861e+03\n3892.131205\n2.717958\n0.419603\n0.419603\n3.521715\n187.472848\n4.820822e+04\n\n\nstd\n1.518706e+05\n6346.367405\n2.124905\n0.535776\n0.535776\n2.272806\n209.443483\n5.714284e+06\n\n\nmin\n7.000000e+00\n-1300.000000\n1.000000\n0.000000\n0.000000\n1.000000\n1.000000\n7.000000e+00\n\n\n25%\n1.200000e+03\n2200.000000\n2.000000\n0.000000\n0.000000\n3.000000\n81.000000\n8.300000e+01\n\n\n50%\n2.000000e+03\n2200.000000\n2.000000\n0.000000\n0.000000\n3.000000\n135.000000\n1.260000e+02\n\n\n75%\n3.500000e+03\n4400.000000\n3.000000\n1.000000\n1.000000\n4.000000\n225.000000\n2.050000e+02\n\n\nmax\n1.800000e+07\n95000.000000\n99.000000\n10.000000\n10.000000\n99.000000\n8032.000000\n6.800000e+08\n\n\n\n\n\n\n\n\nAll columns above have outliers. ‘Listrik’ column has negative value which is caused by incorrect user input\n\n\ndf['Listrik'] = abs(df['Listrik'])\n\n\ndfcat.describe()\n\n\n\n\n\n\n\n\nKecamatan\nkomplek\nLantai\nKondisi\n\n\n\n\ncount\n14161\n14161\n14161\n14161\n\n\nunique\n45\n2\n4\n2\n\n\ntop\nPoris\ntidak\n2\nUnfurnished\n\n\nfreq\n503\n7852\n8818\n8014\n\n\n\n\n\n\n\n\nThere are 45 ‘Kecamatan’ used on this samples.\n\n\n\n\n\nfig, axarr = plt.subplots(2,4, figsize=(11, 8))\nfor col in dfnum.columns:\n    loc = dfnum.columns.get_loc(col)\n    y = 0 if loc &lt; 4 else 1\n    x = loc if loc &lt; 4 else loc - 4\n    sns.boxplot(y = dfnum[col], ax = axarr[y,x])\n    axarr[y, x].set_ylabel(None)\n    axarr[y, x].set_xlabel(col)\nplt.suptitle('Outliers checking on numeric columns', weight='bold')\nfig.tight_layout(pad=1)\nplt.show()\n\n\n\n\n\n#removing extreme values\ndf = df[(df.price &lt; 10000)]\ndf = df[(df['Listrik'] &lt; 20000)]\ndf = df[(df['KM'] &lt; 10)]\ndf = df[(df['KMP'] &lt;= 2)]\ndf = df[(df['KP'] &lt;= 2)]\ndf = df[(df['KT'] &lt;= 20)]\ndf = df[(df['LB'] &lt;= 1000)]\ndf = df[(df['LT'] &lt;= 1000)]\n\n\ndfnum = df._get_numeric_data()\ndfcat = df.drop(columns = dfnum.columns)\n\n\nfig, axarr = plt.subplots(2,4, figsize=(11, 8))\nfor col in dfnum.columns:\n    loc = dfnum.columns.get_loc(col)\n    y = 0 if loc &lt; 4 else 1\n    x = loc if loc &lt; 4 else loc - 4\n    sns.histplot(data=dfnum[col], \n                 color='skyblue', \n                 kde=True, \n                 edgecolor='none', \n                 ax=axarr[y, x])\nplt.suptitle('Distribution checking on numeric columns', weight='bold')\nfig.tight_layout(pad=1)\nplt.show()\n\n\n\n\n\nWe can clearly see that this data is right skewed.\n\n\nfig, axarr = plt.subplots(2,2, figsize=(11, 8))\nfor col in dfcat.columns:\n    loc = dfcat.columns.get_loc(col)\n    y = 0 if loc &lt; 2 else 1\n    x = loc if loc &lt; 2 else loc - 2\n    sns.countplot(x=df[col], color='green', ax = axarr[y,x], order=df[col].value_counts().iloc[:4].index)\n\nplt.suptitle('Countplot of categorical columns', weight='bold')\nfig.tight_layout(pad=1)\nplt.show()\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10,8))\nsns.heatmap(dfnum.corr(), annot=True, fmt='.2f')\nplt.title('Correlation plot', weight ='bold')\nplt.xticks(rotation = 0)\nplt.show()\n\n\n\n\n\nprice, LB, and LT, exhibit a strong positive correlation. Although this correlation is quite strong, it does not lead to multicollinearity issues.\n\n\n\n\n\n\ndf['KMR'] = df.KP + df.KT\ndf['KMD'] = df.KM + df.KMP\ndf['price/m'] = df.price / (df.LT + df.LB) \ndf['Lantai'] = df.Lantai.astype(int)\ndf.drop(columns=['KM','KMP','KP','KT'], inplace=True)\n\n\nI’ve developed a new feature named ‘price/m’ by dividing the price by the sum of LT and LB. This feature is more suitable for analysis compared to using just the price alone.\n\n\nsns.histplot(df.groupby('Kecamatan').size(), alpha=0.8)\nplt.xlabel('Kecamatan Frequency')\nplt.title('Kecamatan distribution count', weight='bold')\nplt.show()\n\n\n\n\n\nThe plot above illustrates the distribution of samples collected from each Kecamatan. The sample size varies, with the lowest being approximately 100 and the highest reaching 500. In cases of insufficient sample size, potential issues may arise, and further analysis is necessary to determine whether these samples are adequate.\n\n\nfig, axarr = plt.subplots(1,2, figsize=(10, 4))\ndata = df.groupby('Kecamatan').size().reset_index(drop=False).sort_values(0, ascending=False)\ndata.columns = ['Kecamatan','count']\nsns.barplot(data = data.head(15), x='count', y='Kecamatan', ax = axarr[0])\nsns.barplot(data = data.tail(15), x='count', y='Kecamatan', ax = axarr[1], palette='RdBu')\naxarr[0].set_title('Top 15 highest kecamatan count', weight='bold')\naxarr[1].set_title('Top 15 lowest kecamatan count', weight='bold')\naxarr[0].set_position([0.1, 0.1, 0.3, 0.8])\naxarr[1].set_position([0.6, 0.1, 0.3, 0.8])\n\n\n\n\n\ndf['Kecamatan'] = df.Kecamatan.str.replace('Gading Serpong Pondok Hijau Golf' , 'Gading Serpong PHG')\n\n\nfig, axarr = plt.subplots(1,2, figsize=(10, 4))\ndata = df.groupby('Kecamatan')[['price']].agg('median').reset_index().sort_values('price', ascending=False)\nsns.barplot(data = data.head(15), x='price', y='Kecamatan', ax = axarr[0], palette='plasma')\nsns.barplot(data = data.tail(15), x='price', y='Kecamatan', ax = axarr[1], palette='coolwarm')\nplt.title('harga per kecamatan')\naxarr[0].set_title('Top 15 highest kecamatan price', weight='bold')\naxarr[1].set_title('Top 15 lowest kecamatan price', weight='bold')\naxarr[0].set_position([0.1, 0.1, 0.3, 0.8])\naxarr[1].set_position([0.6, 0.1, 0.3, 0.8])\n\n\n\n\n\nBSD is one of the most expensive region in Tangerang\n\n\nfig, axarr = plt.subplots(1,2, figsize=(10, 4))\ndata = df.groupby('Kecamatan')[['price/m']].agg('median').reset_index().sort_values('price/m', ascending=False)\nsns.barplot(data = data.head(15), x='price/m', y='Kecamatan', ax = axarr[0], palette = 'YlGn')\nsns.barplot(data = data.tail(15), x='price/m', y='Kecamatan', ax = axarr[1], palette = 'YlGnBu')\nplt.title('harga meteran per kecamatan')\naxarr[0].set_title('Top 15 highest kecamatan price/m', weight='bold')\naxarr[1].set_title('Top 15 lowest kecamatan price/m', weight='bold')\naxarr[0].set_position([0.1, 0.1, 0.3, 0.8])\naxarr[1].set_position([0.6, 0.1, 0.3, 0.8])\n\n\n\n\n\nfrom the price/m, bsd is still one of highest price/m\n\n\ndf.Kondisi = df.Kondisi.map(value_mapping)\n\n\ndf['TK'] = df.KMR + df.KMD\ndf.drop(columns=['KMR','KMD'], inplace=True)\n\n\ndf['avg_price'] = df.groupby(['Kecamatan','Kondisi'])['price/m'].transform('median')\n\n\ndf['rstd'] = df.groupby(['Kecamatan','Kondisi'])['price/m'].transform(lambda x: x.std() * 100 / x.mean())\n\n\ndf1 = df[df.Kondisi == 1]\ndf2 = df[df.Kondisi == 2]\n\nfig, axarr = plt.subplots(1,2, figsize=(10, 4))\ndata = df1[['Kecamatan','rstd']].drop_duplicates().sort_values('rstd', ascending=False).head()\nsns.barplot(data = data, x='rstd', y='Kecamatan', ax = axarr[0])\nsns.histplot(data = df1.rstd.drop_duplicates(), ax = axarr[1])\nsns.histplot(data = df2.rstd.drop_duplicates(), ax = axarr[1], alpha = 0.5, legend='a')\naxarr[0].set_title('Top 5 highest kecamatan price variability', weight='bold')\naxarr[1].set_title('r-std.dev price/m per kecamatan', weight='bold')\naxarr[0].yaxis.set_tick_params(labelsize=10)\n\nhand2 = [mpatches.Rectangle((0, 0), 1, 1, color='blue', label='Unfurnished')]\nhand1 = [mpatches.Rectangle((0, 0), 1, 1, color='orange', label='Furnished')]\nlegend1 = plt.legend(handles = hand2, loc='upper right', labelcolor='blue')\nlegend2 = plt.legend(handles = hand1, loc='upper left', labelcolor='orange')\nplt.gca().add_artist(legend1)\nplt.show()\n\n\n\n\n\nR-std, which stands for relative standard deviation, is a valuable tool for assessing variability and making comparisons between datasets. In the displayed plot, we observe that the r-std is distributed within a substantial 30% range. When the price exhibits a high r-std, it typically indicates an insufficient number of samples or a failure to account for critical factors influencing price fluctuations.\n\n\n#create a variable to contain avg_price information from the data to be used by test data\navg = df[['Kecamatan','Kondisi','avg_price']].drop_duplicates()\n\n\n#remove columns that are no longer needed\ndf9 = df.copy()\ndf = df.drop(columns=['Kecamatan','price/m','rstd'])\n\n\n#encoding\ndf['komplek'] =  df[['komplek']].apply(lambda x: 1 if x.komplek == 'ya' else 0, axis = 1)\n\n\n#create a variable to contain result from all combination\nfinal_result = pd.DataFrame(index = ['mae','mape','rmse','r2'])\n\n\n\n\nFor the first combination, I will utilize all available features and apply linear regression.\n\ndfc1 = df.copy().reset_index(drop=True)\n\n\ndfc1.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 13428 entries, 0 to 13427\nData columns (total 9 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   price      13428 non-null  int64  \n 1   komplek    13428 non-null  int64  \n 2   Listrik    13428 non-null  int64  \n 3   Lantai     13428 non-null  int64  \n 4   Kondisi    13428 non-null  int64  \n 5   LB         13428 non-null  int64  \n 6   LT         13428 non-null  int64  \n 7   TK         13428 non-null  int64  \n 8   avg_price  13428 non-null  float64\ndtypes: float64(1), int64(8)\nmemory usage: 944.3 KB\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(dfc1.drop(columns='price'), dfc1.price.to_numpy(), test_size = 0.2, random_state=129)\nX_train = X_train.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)\n\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\ny_pred_test = model.predict(X_test)\ny_pred_train = model.predict(X_train)\n\n\nresult = pd.DataFrame()\nmae_test = mean_absolute_error(y_test, y_pred_test)\nmape_test = mean_absolute_percentage_error(y_test, y_pred_test)\nrmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\nr2_test = r2_score(y_test, y_pred_test)\n\nmae_train = mean_absolute_error(y_train, y_pred_train)\nmape_train = mean_absolute_percentage_error(y_train, y_pred_train)\nrmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\nr2_train = r2_score(y_train, y_pred_train)\n\nresult['test_OLS'] = [mae_test, mape_test, rmse_test, r2_test]\nresult['train_OLS'] = [mae_train, mape_train, rmse_train, r2_train]\nresult.index = ['mae','mape','rmse','r2']\n\n\nresult\n\n\n\n\n\n\n\n\ntest_OLS\ntrain_OLS\n\n\n\n\nmae\n532.715149\n541.468771\n\n\nmape\n0.278840\n0.329897\n\n\nrmse\n800.262693\n805.231411\n\n\nr2\n0.797440\n0.794544\n\n\n\n\n\n\n\n\nAbove is the result of the base model.\n\n\nfinal_result = pd.concat([final_result, result], axis = 1)\n\n\n\n\nIn this second combination, I intend to enhance model complexity by incorporating polynomial features. If overfitting concerns arise, I will address them by applying ridge regression.\n\ndfc2 = df.copy().reset_index(drop=True) \n\n\ndfc2.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 13428 entries, 0 to 13427\nData columns (total 9 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   price      13428 non-null  int64  \n 1   komplek    13428 non-null  int64  \n 2   Listrik    13428 non-null  int64  \n 3   Lantai     13428 non-null  int64  \n 4   Kondisi    13428 non-null  int64  \n 5   LB         13428 non-null  int64  \n 6   LT         13428 non-null  int64  \n 7   TK         13428 non-null  int64  \n 8   avg_price  13428 non-null  float64\ndtypes: float64(1), int64(8)\nmemory usage: 944.3 KB\n\n\n\nscaler = MinMaxScaler()\ndf1 = scaler.fit_transform(dfc2.drop(columns=['price']))\ndf1 = pd.DataFrame(df1, columns = dfc2.drop(columns=['price']).columns)\ndf1['price'] = dfc2.price\ndfc2 = df1.copy()\n\n\nX_train, X_test, y_train, y_test = train_test_split(dfc2.drop(columns='price'), dfc2.price.to_numpy(), test_size=0.2, random_state=129)\n\n\nresult = pd.DataFrame()\nfor x in [2,3,4]:\n    poly_features = PolynomialFeatures(degree=x)\n    X_poly = poly_features.fit_transform(X_train)\n    X_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(X_poly, y_train, test_size=0.2, random_state=129)\n\n    model = LinearRegression()\n    model.fit(X_train_poly, y_train_poly)\n\n    y_pred_test = model.predict(X_test_poly)\n    y_pred_train = model.predict(X_train_poly)\n\n    mae_test = mean_absolute_error(y_test_poly, y_pred_test)\n    rmse_test = np.sqrt(mean_squared_error(y_test_poly, y_pred_test))\n    r2_test = r2_score(y_test_poly, y_pred_test)\n\n    mae_train = mean_absolute_error(y_train_poly, y_pred_train)\n    rmse_train = np.sqrt(mean_squared_error(y_train_poly, y_pred_train))\n    r2_train = r2_score(y_train_poly, y_pred_train)\n\n    result[f'test_{x}'] = [mae_test, rmse_test, r2_test]\n    result[f'train_{x}'] = [mae_train, rmse_train, r2_train]\n\nresult.index = ['mae','rmse','r2']\nresult\n\n\n\n\n\n\n\n\ntest_2\ntrain_2\ntest_3\ntrain_3\ntest_4\ntrain_4\n\n\n\n\nmae\n472.915901\n481.615889\n472.519472\n464.954008\n501.537211\n442.182946\n\n\nrmse\n712.046550\n738.563833\n719.862027\n705.295298\n834.318214\n660.588687\n\n\nr2\n0.837082\n0.827754\n0.833486\n0.842922\n0.776326\n0.862204\n\n\n\n\n\n\n\n\nI tested a polynomial degree of 3, and the results indicate a minor degree of overfitting. However, when I increased it to degree 4, the model exhibited a severe overfitting issue.\n\n\nresult = pd.DataFrame()\nfor x in [3,4]:\n    poly_features = PolynomialFeatures(degree=x)\n    X_poly = poly_features.fit_transform(X_train)\n    X_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(X_poly, y_train, test_size=0.2, random_state=129)\n\n    for alpha in [0.01, 0.1, 1., 10]:\n        model = Ridge(alpha=alpha)\n        model.fit(X_train_poly, y_train_poly)\n\n        y_pred_test = model.predict(X_test_poly)\n        y_pred_train = model.predict(X_train_poly)\n\n        mae_test = mean_absolute_error(y_test_poly, y_pred_test)\n        rmse_test = np.sqrt(mean_squared_error(y_test_poly, y_pred_test))\n        r2_test = r2_score(y_test_poly, y_pred_test)\n\n        mae_train = mean_absolute_error(y_train_poly, y_pred_train)\n        rmse_train = np.sqrt(mean_squared_error(y_train_poly, y_pred_train))\n        r2_train = r2_score(y_train_poly, y_pred_train)\n\n        result[f'test_{x}_{alpha}'] = [mae_test, rmse_test, r2_test]\n        result[f'train_{x}_{alpha}'] = [mae_train, rmse_train, r2_train]\n\nresult.index = ['mae','rmse','r2']\nresult\n\n\n\n\n\n\n\n\ntest_3_0.01\ntrain_3_0.01\ntest_3_0.1\ntrain_3_0.1\ntest_3_1.0\ntrain_3_1.0\ntest_3_10\ntrain_3_10\ntest_4_0.01\ntrain_4_0.01\ntest_4_0.1\ntrain_4_0.1\ntest_4_1.0\ntrain_4_1.0\ntest_4_10\ntrain_4_10\n\n\n\n\nmae\n467.342704\n466.439950\n464.299064\n470.035509\n470.187015\n478.290378\n484.975821\n490.051383\n464.840133\n456.548113\n462.897803\n463.601116\n466.596826\n473.665648\n481.134291\n487.347859\n\n\nrmse\n704.461373\n708.507262\n698.403249\n717.934342\n707.437585\n731.792609\n726.156035\n749.558649\n706.885343\n690.605679\n695.774749\n704.905074\n700.618287\n723.246246\n720.228905\n743.215049\n\n\nr2\n0.840534\n0.841488\n0.843265\n0.837242\n0.839184\n0.830898\n0.830561\n0.822587\n0.839435\n0.849397\n0.844443\n0.843096\n0.842270\n0.834824\n0.833316\n0.825578\n\n\n\n\n\n\n\n\nI attempted to use ridge regression to tackle overfitting on highly polynomial features, but the outcome wasn’t superior to simply using a polynomial degree of 2 without any regularization.\n\n\npoly_features = PolynomialFeatures(degree=2)\nX_train = poly_features.fit_transform(X_train)\nX_test = poly_features.fit_transform(X_test)\n\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\ny_pred_test = model.predict(X_test)\ny_pred_train = model.predict(X_train)\n\n\nresult = pd.DataFrame()\nmae_test = mean_absolute_error(y_test, y_pred_test)\nmape_test = mean_absolute_percentage_error(y_test, y_pred_test)\nrmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\nr2_test = r2_score(y_test, y_pred_test)\n\nmae_train = mean_absolute_error(y_train, y_pred_train)\nmape_train = mean_absolute_percentage_error(y_train, y_pred_train)\nrmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\nr2_train = r2_score(y_train, y_pred_train)\n\nresult['test_PolyRidgeOLS'] = [mae_test, mape_test, rmse_test, r2_test]\nresult['train_PolyRidgeOLS'] = [mae_train, mape_train, rmse_train, r2_train]\nresult.index = ['mae','mape','rmse','r2']\n\n\nresult\n\n\n\n\n\n\n\n\ntest_PolyRidgeOLS\ntrain_PolyRidgeOLS\n\n\n\n\nmae\n471.090238\n478.917020\n\n\nmape\n0.227019\n0.276395\n\n\nrmse\n729.230240\n732.438213\n\n\nr2\n0.831804\n0.830011\n\n\n\n\n\n\n\n\nThe model is not overfitting and the result is better than the base model\n\n\nfinal_result = pd.concat([final_result, result], axis = 1)\n\n\n\n\nLet’s try to log transform skewed features and use linear regression.\n\ndfc3 = df.copy().reset_index(drop=True)\n\n\ncolumn = ['LB','LT','TK']\nfor col in column:\n    dfc3[col] = np.log(dfc3[col])\n\n\ndfc3.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 13428 entries, 0 to 13427\nData columns (total 9 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   price      13428 non-null  int64  \n 1   komplek    13428 non-null  int64  \n 2   Listrik    13428 non-null  int64  \n 3   Lantai     13428 non-null  int64  \n 4   Kondisi    13428 non-null  int64  \n 5   LB         13428 non-null  float64\n 6   LT         13428 non-null  float64\n 7   TK         13428 non-null  float64\n 8   avg_price  13428 non-null  float64\ndtypes: float64(4), int64(5)\nmemory usage: 944.3 KB\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(dfc3.drop(columns='price'), dfc3.price.to_numpy(), test_size = 0.2, random_state=129)\nX_train = X_train.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)\n\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\ny_pred_test = model.predict(X_test)\ny_pred_train = model.predict(X_train)\n\n\nresult = pd.DataFrame()\nmae_test = mean_absolute_error(y_test, y_pred_test)\nmape_test = mean_absolute_percentage_error(y_test, y_pred_test)\nrmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\nr2_test = r2_score(y_test, y_pred_test)\n\nmae_train = mean_absolute_error(y_train, y_pred_train)\nmape_train = mean_absolute_percentage_error(y_train, y_pred_train)\nrmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\nr2_train = r2_score(y_train, y_pred_train)\n\nresult['test_LogOLS'] = [mae_test, mape_test, rmse_test, r2_test]\nresult['train_LogOLS'] = [mae_train, mape_train, rmse_train, r2_train]\nresult.index = ['mae','mape','rmse','r2']\n\n\nresult\n\n\n\n\n\n\n\n\ntest_LogOLS\ntrain_LogOLS\n\n\n\n\nmae\n583.179251\n595.413840\n\n\nmape\n0.329967\n0.394927\n\n\nrmse\n835.080845\n841.121663\n\n\nr2\n0.779431\n0.775821\n\n\n\n\n\n\n\n\nThe results show a performance decline compared to using Linear Regression on the original, untransformed features.\n\n\nfinal_result = pd.concat([final_result, result], axis = 1)\n\n\n\n\nFor this combination, let’s try XGBRegressor\n\ndfc4 = df.copy()\n\n\ndfc4.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 13428 entries, 0 to 17049\nData columns (total 9 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   price      13428 non-null  int64  \n 1   komplek    13428 non-null  int64  \n 2   Listrik    13428 non-null  int64  \n 3   Lantai     13428 non-null  int64  \n 4   Kondisi    13428 non-null  int64  \n 5   LB         13428 non-null  int64  \n 6   LT         13428 non-null  int64  \n 7   TK         13428 non-null  int64  \n 8   avg_price  13428 non-null  float64\ndtypes: float64(1), int64(8)\nmemory usage: 1.0 MB\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(dfc4.drop(columns='price'), dfc4.price.to_numpy(), test_size = 0.2, random_state=129)\nX_train = X_train.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)\n\n\nxgb_regressor = XGBRegressor(\n    objective='reg:squarederror',  \n    eval_metric='rmse',           \n    random_state=42               \n)\n\n\nparam_grid = {  \n    'learning_rate': [0.001, 0.01],  \n    'n_estimators': [100, 400, 900, 1000],  \n    'max_depth': [3, 4],  \n    'eval_metric': ['rmse'],  \n}\n\n\ngrid_search = GridSearchCV(\n    estimator=xgb_regressor,\n    param_grid=param_grid,\n    scoring='neg_mean_squared_error',  \n    cv=5,                              \n    verbose=1,                         \n    n_jobs=-1                           \n)\n\n\ngrid_search.fit(X_train, y_train)\n\nFitting 5 folds for each of 16 candidates, totalling 80 fits\n\n\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n\n\nGridSearchCV(cv=5,\n             estimator=XGBRegressor(base_score=None, booster=None,\n                                    callbacks=None, colsample_bylevel=None,\n                                    colsample_bynode=None,\n                                    colsample_bytree=None, device=None,\n                                    early_stopping_rounds=None,\n                                    enable_categorical=False,\n                                    eval_metric='rmse', feature_types=None,\n                                    gamma=None, grow_policy=None,\n                                    importance_type=None,\n                                    interaction_constraints=None,\n                                    learning_rate=None...\n                                    max_depth=None, max_leaves=None,\n                                    min_child_weight=None, missing=nan,\n                                    monotone_constraints=None,\n                                    multi_strategy=None, n_estimators=None,\n                                    n_jobs=None, num_parallel_tree=None,\n                                    random_state=42, ...),\n             n_jobs=-1,\n             param_grid={'eval_metric': ['rmse'],\n                         'learning_rate': [0.001, 0.01], 'max_depth': [3, 4],\n                         'n_estimators': [100, 400, 900, 1000]},\n             scoring='neg_mean_squared_error', verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5,\n             estimator=XGBRegressor(base_score=None, booster=None,\n                                    callbacks=None, colsample_bylevel=None,\n                                    colsample_bynode=None,\n                                    colsample_bytree=None, device=None,\n                                    early_stopping_rounds=None,\n                                    enable_categorical=False,\n                                    eval_metric='rmse', feature_types=None,\n                                    gamma=None, grow_policy=None,\n                                    importance_type=None,\n                                    interaction_constraints=None,\n                                    learning_rate=None...\n                                    max_depth=None, max_leaves=None,\n                                    min_child_weight=None, missing=nan,\n                                    monotone_constraints=None,\n                                    multi_strategy=None, n_estimators=None,\n                                    n_jobs=None, num_parallel_tree=None,\n                                    random_state=42, ...),\n             n_jobs=-1,\n             param_grid={'eval_metric': ['rmse'],\n                         'learning_rate': [0.001, 0.01], 'max_depth': [3, 4],\n                         'n_estimators': [100, 400, 900, 1000]},\n             scoring='neg_mean_squared_error', verbose=1)estimator: XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric='rmse', feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=None, n_jobs=None,\n             num_parallel_tree=None, random_state=42, ...)XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric='rmse', feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=None, n_jobs=None,\n             num_parallel_tree=None, random_state=42, ...)\n\n\n\nbest_params_xgb = grid_search.best_params_\n\n\nbest_params_xgb\n\n{'eval_metric': 'rmse',\n 'learning_rate': 0.01,\n 'max_depth': 4,\n 'n_estimators': 1000}\n\n\n\nxgboost = XGBRegressor(**best_params_xgb, random_state=42)\n\n\nxgboost.fit(X_train, y_train)\n\nXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric='rmse', feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=4, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=1000, n_jobs=None,\n             num_parallel_tree=None, random_state=42, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric='rmse', feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=4, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=1000, n_jobs=None,\n             num_parallel_tree=None, random_state=42, ...)\n\n\n\ny_pred_test = xgboost.predict(X_test)\ny_pred_train = xgboost.predict(X_train)\n\n\nresult = pd.DataFrame()\nmae_test = mean_absolute_error(y_test, y_pred_test)\nmape_test = mean_absolute_percentage_error(y_test, y_pred_test)\nrmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\nr2_test = r2_score(y_test, y_pred_test)\n\nmae_train = mean_absolute_error(y_train, y_pred_train)\nmape_train = mean_absolute_percentage_error(y_train, y_pred_train)\nrmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\nr2_train = r2_score(y_train, y_pred_train)\n\nresult['test_XGB'] = [mae_test, mape_test, rmse_test, r2_test]\nresult['train_XGB'] = [mae_train, mape_train, rmse_train, r2_train]\nresult.index = ['mae','mape','rmse','r2']\n\n\nresult\n\n\n\n\n\n\n\n\ntest_XGB\ntrain_XGB\n\n\n\n\nmae\n438.290716\n419.337764\n\n\nmape\n0.213093\n0.253128\n\n\nrmse\n678.516114\n626.639712\n\n\nr2\n0.854384\n0.875573\n\n\n\n\n\n\n\n\nThe model is little overfit. But this is the best model so far\n\n\nfinal_result = pd.concat([final_result, result], axis = 1)\n\n\nxgb.plot_importance(xgboost)  # You can also use 'gain' or 'cover' for importance_type\nplt.show()\n\n\n\n\n\n\n\nI will use RandomForest on this combination. But using RandomForest model on this case is very computational expensive because of many continuous features.\n\ndfc5 = df.copy().reset_index(drop=True) #df[['LT','avg_price','distance','LB','Listrik','price']]\n\n\ndfc5.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 13428 entries, 0 to 13427\nData columns (total 9 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   price      13428 non-null  int64  \n 1   komplek    13428 non-null  int64  \n 2   Listrik    13428 non-null  int64  \n 3   Lantai     13428 non-null  int64  \n 4   Kondisi    13428 non-null  int64  \n 5   LB         13428 non-null  int64  \n 6   LT         13428 non-null  int64  \n 7   TK         13428 non-null  int64  \n 8   avg_price  13428 non-null  float64\ndtypes: float64(1), int64(8)\nmemory usage: 944.3 KB\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(dfc5.drop(columns='price'), dfc5.price.to_numpy(), test_size = 0.2, random_state=129)\nX_train = X_train.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)\n\n\nrandomforest = RandomForestRegressor(random_state=42)\n\n\nparam_grid = {\n    'n_estimators': [600],\n    'max_depth': [8, 7],\n    'min_samples_split': [10, 50, 100],\n}\n\n\ngrid_search = GridSearchCV(estimator=randomforest, param_grid=param_grid, cv=5, n_jobs=-1)\n\n\ngrid_search.fit(X_train, y_train)\n\nGridSearchCV(cv=5, estimator=RandomForestRegressor(random_state=42), n_jobs=-1,\n             param_grid={'max_depth': [8, 7],\n                         'min_samples_split': [10, 50, 100],\n                         'n_estimators': [600]})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5, estimator=RandomForestRegressor(random_state=42), n_jobs=-1,\n             param_grid={'max_depth': [8, 7],\n                         'min_samples_split': [10, 50, 100],\n                         'n_estimators': [600]})estimator: RandomForestRegressorRandomForestRegressor(random_state=42)RandomForestRegressorRandomForestRegressor(random_state=42)\n\n\n\nbest_params_rf = grid_search.best_params_\n\n\nbest_params_rf\n\n{'max_depth': 8, 'min_samples_split': 10, 'n_estimators': 600}\n\n\n\nrf = RandomForestRegressor(**best_params_rf, random_state=42)\n\n\nrf.fit(X_train, y_train)\n\nRandomForestRegressor(max_depth=8, min_samples_split=10, n_estimators=600,\n                      random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressorRandomForestRegressor(max_depth=8, min_samples_split=10, n_estimators=600,\n                      random_state=42)\n\n\n\ny_pred_test = rf.predict(X_test)\ny_pred_train = rf.predict(X_train)\n\n\nresult = pd.DataFrame()\nmae_test = mean_absolute_error(y_test, y_pred_test)\nmape_test = mean_absolute_percentage_error(y_test, y_pred_test)\nrmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\nr2_test = r2_score(y_test, y_pred_test)\n\nmae_train = mean_absolute_error(y_train, y_pred_train)\nmape_train = mean_absolute_percentage_error(y_train, y_pred_train)\nrmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\nr2_train = r2_score(y_train, y_pred_train)\n\nresult['test_RF'] = [mae_test, mape_test, rmse_test, r2_test]\nresult['train_RF'] = [mae_train, mape_train, rmse_train, r2_train]\nresult.index = ['mae','mape','rmse','r2']\n\n\nresult\n\n\n\n\n\n\n\n\ntest_RF\ntrain_RF\n\n\n\n\nmae\n438.359894\n399.306010\n\n\nmape\n0.216506\n0.243069\n\n\nrmse\n687.059730\n599.451504\n\n\nr2\n0.850694\n0.886136\n\n\n\n\n\n\n\n\nThe result is more or less the same with XGBoost model.\n\n\nfinal_result = pd.concat([final_result, result], axis = 1)\n\n\n\n\nFor this last combination, let’s see how AdaBoost perform.\n\ndfc6 = df.copy().reset_index(drop=True) #df[['LT','avg_price','distance','LB','Listrik','price']]\n\n\ndfc6.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 13428 entries, 0 to 13427\nData columns (total 9 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   price      13428 non-null  int64  \n 1   komplek    13428 non-null  int64  \n 2   Listrik    13428 non-null  int64  \n 3   Lantai     13428 non-null  int64  \n 4   Kondisi    13428 non-null  int64  \n 5   LB         13428 non-null  int64  \n 6   LT         13428 non-null  int64  \n 7   TK         13428 non-null  int64  \n 8   avg_price  13428 non-null  float64\ndtypes: float64(1), int64(8)\nmemory usage: 944.3 KB\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(dfc5.drop(columns='price'), dfc5.price.to_numpy(), test_size = 0.2, random_state=129)\nX_train = X_train.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)\n\n\nadaboost = AdaBoostRegressor(random_state = 42)\n\n\nparam_grid = {\n    'n_estimators': [50, 150, 250],\n    'learning_rate': [0.01, 0.1, 1, 0.001]\n}\n\n\ngrid_search = GridSearchCV(estimator=adaboost, param_grid=param_grid, cv=5, n_jobs=-1)\n\n\ngrid_search.fit(X_train, y_train)\n\nGridSearchCV(cv=5, estimator=AdaBoostRegressor(random_state=42), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 1, 0.001],\n                         'n_estimators': [50, 150, 250]})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5, estimator=AdaBoostRegressor(random_state=42), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 1, 0.001],\n                         'n_estimators': [50, 150, 250]})estimator: AdaBoostRegressorAdaBoostRegressor(random_state=42)AdaBoostRegressorAdaBoostRegressor(random_state=42)\n\n\n\nbest_params_ada = grid_search.best_params_\n\n\nbest_params_ada\n\n{'learning_rate': 0.1, 'n_estimators': 50}\n\n\n\nada = AdaBoostRegressor(**best_params_ada, random_state=42)\n\n\nada.fit(X_train, y_train)\n\nAdaBoostRegressor(learning_rate=0.1, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.AdaBoostRegressorAdaBoostRegressor(learning_rate=0.1, random_state=42)\n\n\n\ny_pred_test = ada.predict(X_test)\ny_pred_train = ada.predict(X_train)\n\n\nresult = pd.DataFrame()\nmae_test = mean_absolute_error(y_test, y_pred_test)\nmape_test = mean_absolute_percentage_error(y_test, y_pred_test)\nrmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\nr2_test = r2_score(y_test, y_pred_test)\n\nmae_train = mean_absolute_error(y_train, y_pred_train)\nmape_train = mean_absolute_percentage_error(y_train, y_pred_train)\nrmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\nr2_train = r2_score(y_train, y_pred_train)\n\nresult['test_ADA'] = [mae_test, mape_test, rmse_test, r2_test]\nresult['train_ADA'] = [mae_train, mape_train, rmse_train, r2_train]\nresult.index = ['mae','mape','rmse','r2']\n\n\nresult\n\n\n\n\n\n\n\n\ntest_ADA\ntrain_ADA\n\n\n\n\nmae\n574.532983\n580.901639\n\n\nmape\n0.329233\n0.398783\n\n\nrmse\n838.365578\n816.508925\n\n\nr2\n0.777692\n0.788748\n\n\n\n\n\n\n\n\nfinal_result = pd.concat([final_result, result], axis = 1)\n\n\n\n\nLet’s use the best combination\n\nfinal_result\n\n\n\n\n\n\n\n\ntest_OLS\ntrain_OLS\ntest_PolyRidgeOLS\ntrain_PolyRidgeOLS\ntest_LogOLS\ntrain_LogOLS\ntest_XGB\ntrain_XGB\ntest_RF\ntrain_RF\ntest_ADA\ntrain_ADA\n\n\n\n\nmae\n532.715149\n541.468771\n471.090238\n478.917020\n583.179251\n595.413840\n438.290716\n419.337764\n438.359894\n399.306010\n574.532983\n580.901639\n\n\nmape\n0.278840\n0.329897\n0.227019\n0.276395\n0.329967\n0.394927\n0.213093\n0.253128\n0.216506\n0.243069\n0.329233\n0.398783\n\n\nrmse\n800.262693\n805.231411\n729.230240\n732.438213\n835.080845\n841.121663\n678.516114\n626.639712\n687.059730\n599.451504\n838.365578\n816.508925\n\n\nr2\n0.797440\n0.794544\n0.831804\n0.830011\n0.779431\n0.775821\n0.854384\n0.875573\n0.850694\n0.886136\n0.777692\n0.788748\n\n\n\n\n\n\n\n\nBy seeing the results, 2 best models are XGBoost and RandomForrest. I’ll go with XGBoost because it way more less computational than RandomForest\n\n\n#choosing only the most important features for XGB\ndf = df[['LT','LB','Listrik','price','avg_price']]\n\nX_train, X_test, y_train, y_test = train_test_split(df.drop(columns='price'), df.price.to_numpy(), test_size = 0.2, random_state=123)\nX_train = X_train.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)\n\n\nbest_xgb_regressor = XGBRegressor(**best_params_xgb, random_state=42)\n\n\nbest_xgb_regressor.fit(X_train, y_train)\n\nXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric='rmse', feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=4, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=1000, n_jobs=None,\n             num_parallel_tree=None, random_state=42, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric='rmse', feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=4, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=1000, n_jobs=None,\n             num_parallel_tree=None, random_state=42, ...)\n\n\n\ny_pred_test = best_xgb_regressor.predict(X_test)\ny_pred_train = best_xgb_regressor.predict(X_train)\n\n\nresult = pd.DataFrame()\nmae_test = mean_absolute_error(y_test, y_pred_test)\nmape_test = mean_absolute_percentage_error(y_test, y_pred_test)\nrmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\nr2_test = r2_score(y_test, y_pred_test)\n\nmae_train = mean_absolute_error(y_train, y_pred_train)\nmape_train = mean_absolute_percentage_error(y_train, y_pred_train)\nrmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\nr2_train = r2_score(y_train, y_pred_train)\n\nresult['test_XGB'] = [mae_test, mape_test, rmse_test, r2_test]\nresult['train_XGB'] = [mae_train, mape_train, rmse_train, r2_train]\nresult.index = ['mae','mape','rmse','r2']\n\n\nresult\n\n\n\n\n\n\n\n\ntest_XGB\ntrain_XGB\n\n\n\n\nmae\n459.897612\n429.131317\n\n\nmape\n0.307846\n0.244483\n\n\nrmse\n699.862902\n643.969481\n\n\nr2\n0.846041\n0.868387\n\n\n\n\n\n\n\n\nThe results remain favorable and consistent\n\n\n\n\nHere, I will assess and interpret the model’s performance using a new set of sample data that I collected separately from the main dataset.\n\ntest = pd.read_csv('test.csv')\ntest = pd.merge(test, avg , on=['Kecamatan','Kondisi']).convert_dtypes()\n\n\nAdd the avg_price column to test data.\n\n\ntest = test[(test.price &lt; 10000) & (test.price &gt; 100)]\ntest = test[test.LB &lt; 900]\ntest = test[test.LT &lt; 500]\ntest = test[['LT','avg_price','LB','Listrik','price']]\n\n\nClean the test data\n\n\ntest_x = test[['LT','LB','Listrik','avg_price']]\ntest_y = test.price\n\n\ny_pred_test = best_xgb_regressor.predict(test_x)\ny_test = test_y\n\n\nresult = pd.DataFrame()\nmae_test = mean_absolute_error(y_test, y_pred_test)\nmape_test = mean_absolute_percentage_error(y_test, y_pred_test)\nrmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\nr2_test = r2_score(y_test, y_pred_test)\n\nmae_train = mean_absolute_error(y_train, y_pred_train)\nmape_train = mean_absolute_percentage_error(y_train, y_pred_train)\nrmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\nr2_train = r2_score(y_train, y_pred_train)\n\nresult['test_XGB'] = [mae_test, mape_test, rmse_test, r2_test]\nresult['train_XGB'] = [mae_train, mape_train, rmse_train, r2_train]\nresult.index = ['mae','mape','rmse','r2']\n\n\nresult\n\n\n\n\n\n\n\n\ntest_XGB\ntrain_XGB\n\n\n\n\nmae\n531.780306\n429.131317\n\n\nmape\n0.214570\n0.244483\n\n\nrmse\n885.063409\n643.969481\n\n\nr2\n0.785323\n0.868387\n\n\n\n\n\n\n\n\nThe model perform worse on scrapped test data.\n\n\n\n\nres = pd.DataFrame([list(y_train), list(y_pred_train)]).transpose()\nres.columns = ['test','pred']\nres['residual'] = res.test - res.pred\nres['residualp'] = abs(res.test - res.pred) * 100 / res.test\n\n\nfig = px.scatter(x=res['test'], y=res['residualp'])\nfig.update_layout( \n    height = 800,\n    title = 'Residual\\'s absolute percentage plot'\n)\nfig.add_hline(y=res['residualp'].mean(), line_dash=\"dot\", line_color=\"red\")\nfig\n\n\n                                                \n\n\n\nFrom the residual percentage above we can see that : 1. 7.5% of data is APE that is more than 50%. 2. 59% of data is APE that is lower than 20%. 3. 32.5% of data is ape that is 20% - 50%. 4. When the houses prices is 4 million, the model become worse as the price kept going up. 5. There are extreme APE error on 0 to 2k prices, this indicate that there are anomalies data inserted.\n\n\nfig = px.scatter(x=res['test'], y=res['residual'])\nfig.update_layout(  \n    height = 700,\n    title = 'Residuals plot'\n)\nfig.add_hline(y=0, line_dash=\"dot\", line_color=\"red\")\nfig\n\n\n                                                \n\n\n\nThe unequal variance in the data implies the presence of heteroscedasticity, and a higher price is associated with increased variability. This phenomenon is considered normal since more expensive houses involve a greater number of factors in determining their prices. Additionally, the model also consistently underpredicts as house prices increase. I think we need to gather more variables or predictors that influence house prices.\n\n\n\n\n\ndef pdp_ice_plot(model, df_test, column, clusters=True):\n    df_test = df_test.copy()\n    pdp_isolate = pdp.PDPIsolate(model = model, df = df_test, \n                      num_grid_points=20,\n                      model_features = df_test.columns, \n                      feature = column, feature_name=column, n_classes=0)\n    pdp_isolate.plot(\n                center=True, plot_lines=True,\n                cluster=clusters, n_cluster_centers=5,cluster_method='accurate',\n                plot_pts_dist=False, to_bins=True,\n                show_percentile=False, which_classes=None,\n                figsize=(10,6), dpi=300,\n                ncols=2, plot_params={\"pdp_hl\": True},\n                engine='matplotlib', template='plotly_white')\n    \n    plt.show()\n    return pdp_isolate\n\n\npdp_ice_plot(best_xgb_regressor, test_x, 'LT', clusters=False)\n\nobtain pred_func from the provided model.\n\n\n\n\n\nfindfont: Font family 'Arial' not found.\nfindfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\nfindfont: Font family 'Arial' not found.\nfindfont: Font family 'Arial' not found.\nfindfont: Font family 'Arial' not found.\nfindfont: Font family 'Arial' not found.\n\n\n\n\n\n&lt;pdpbox.pdp.PDPIsolate at 0x7f92d07a8a30&gt;\n\n\n\nThe Larger the LT, higher the price. But as the LT get higher, the price is also more disperse.\n\n\ntest1 = test_x.copy()\npdp_interact = pdp.PDPInteract(model=best_xgb_regressor, df=test1,\n                              num_grid_points=10,\n                              model_features = test1.columns, \n                              features=['LB','avg_price'], \n                              feature_names=['LB','avg_price'], n_classes=0)\n\nobtain pred_func from the provided model.\nobtain pred_func from the provided model.\nobtain pred_func from the provided model.\n\n\n\n\n\n\n\n\n\n\n\n\npdp_ice_plot(best_xgb_regressor, test_x, 'avg_price', clusters=True)\n\nobtain pred_func from the provided model.\n\n\n\n\n\nfindfont: Font family 'Arial' not found.\nfindfont: Font family 'Arial' not found.\nfindfont: Font family 'Arial' not found.\nfindfont: Font family 'Arial' not found.\nfindfont: Font family 'Arial' not found.\n\n\n\n\n\n&lt;pdpbox.pdp.PDPIsolate at 0x7f92c44449d0&gt;\n\n\n\nThe higher the avg_price, also the higher the price, but not all data is affected the same, some are highly affected, some are less affected.\n\n\nfig, axes = pdp_interact.plot(\n    plot_type=\"grid\",\n    to_bins=True,\n    plot_pdp=True,\n    show_percentile=True,\n    which_classes=None,\n    figsize=None,\n    dpi=300,\n    ncols=2,\n    plot_params=None,\n    engine=\"plotly\",\n    template=\"plotly_white\",\n)\nfig\n\n\n                                                \n\n\n\nBy combining the avg_price and the LT, you can see that low LT is not affected as much as it affect high LT. The price is more volatile on higher LT.\n\n\n\n\n\nI have concerns regarding the model’s accuracy, primarily because the residuals are still to be relatively high. This issue can be attributed to the limited sample size and the exclusion of critical factors influencing prices. Among these factors, ‘avg_price’ is of utmost importance. To enhance the accuracy of ‘avg_price,’ it’s imperative to expand the sample size and consider a broader spectrum of variables affecting prices. While I’ve used ‘Kecamatan’ and ‘Kondisi Perabotan’ in this context to estimate ‘avg_price,’ I believe these variables alone are insufficient. Moving forward, I plan to further develop this project by gathering additional data from various real estate websites to create a more robust and accurate model."
  },
  {
    "objectID": "ipynb/houses/house_tangerang.html#table-of-contents",
    "href": "ipynb/houses/house_tangerang.html#table-of-contents",
    "title": "Tangerang house price prediction",
    "section": "",
    "text": "• Introduction  • Data Preparation • Basic exploratory data analysis      • Descriptive statistic      • Univariate analysis      • Multivariate analysis  • Deep-dive exploratory data analysis  • Modelling     • Model building combination 1      • Model building combination 2      • Model building combination 3      • Model building combination 4      • Model building combination 5      • Model building combination 6  • Choosing the best model combination  • Model evaluation and interpretation      • Residuals plot      • PDP & ICE plots  • Conclusion"
  },
  {
    "objectID": "ipynb/houses/house_tangerang.html#introduction",
    "href": "ipynb/houses/house_tangerang.html#introduction",
    "title": "Tangerang house price prediction",
    "section": "",
    "text": "This project revolves around predicting house prices in Tangerang City, which encompasses more than 40 regions. The data for this project was gathered by scraping information from one of Indonesia’s largest online real estate listing platforms. After conducting this exploratory phase, I will deploy an application publicly to assist people in estimating house prices."
  },
  {
    "objectID": "ipynb/houses/house_tangerang.html#data-preparation",
    "href": "ipynb/houses/house_tangerang.html#data-preparation",
    "title": "Tangerang house price prediction",
    "section": "",
    "text": "Importing libraries, cleaning data and choosing features.\n\n#import libraries\nimport numpy as np\nimport pandas as pd\nimport ast\nimport re\nfrom sklearn.impute import KNNImputer\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\nimport warnings\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\nfrom sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\nfrom xgboost import XGBRegressor\nimport xgboost as xgb\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\nimport plotly.express as px\nimport matplotlib.patches as mpatches\nfrom pdpbox import pdp\n\n#set some library settings\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_columns', None)\nsns.set()\nsns.set_style('darkgrid')\nmpl.rcParams['figure.dpi'] = 150 \nplt.rc('legend',**{'fontsize':10})\n\n\n#read csv\ndf = pd.read_csv('tangerang.csv')\n\n\n#first look of data\ndf.head()\n\n\n\n\n\n\n\n\nharga\nalamat\nfasilitas\nspesifikasi\n\n\n\n\n0\nRp 2,6 Miliar\nBSD City, Tangerang\nTempat Jemuran,Keamanan 24 jam,Playground,Wast...\n[['Kamar Tidur', '3'], ['Kamar Mandi', '2'], [...\n\n\n1\nRp 20 Miliar\nBSD City, Tangerang\nNaN\n[['Kamar Tidur', '5'], ['Kamar Mandi', '5'], [...\n\n\n2\nRp 1,68 Miliar\nBSD City, Tangerang\nTaman,Tempat Jemuran,Keamanan 24 jam\n[['Kamar Tidur', '2'], ['Kamar Mandi', '2'], [...\n\n\n3\nRp 1,61 Miliar\nBSD City, Tangerang\nTaman,Tempat Jemuran,Keamanan 24 jam\n[['Kamar Tidur', '4'], ['Kamar Mandi', '3'], [...\n\n\n4\nRp 2,14 Miliar\nBSD City, Tangerang\nTaman,Tempat Jemuran,Keamanan 24 jam\n[['Kamar Tidur', '3'], ['Kamar Mandi', '3'], [...\n\n\n\n\n\n\n\n\n#change 'spesifikasi' to type list and explode it to become columns\ndf['spesifikasi'] = df.spesifikasi.apply(ast.literal_eval)\ndf = df.explode('spesifikasi')\n\n\n#split into 'keterangan' and 'qty'\ndf['keterangan'] = df.spesifikasi.str[0]\ndf['qty'] = df.spesifikasi.str[1]\n\n\ndf_columns = df[['harga','alamat','fasilitas']].reset_index().drop_duplicates('index').set_index('index')\ndf_pivot = df.pivot(columns='keterangan', values='qty').rename_axis(None, axis=1)\ndf = pd.concat([df_columns, df_pivot], axis = 1)\n\n\nCode above is about to transform ‘keterangan’ into several columns.\n\n\n#new transformed dataframe\ndf.head()\n\n\n\n\n\n\n\n\nharga\nalamat\nfasilitas\nCarport\nDapur\nDaya Listrik\nGarasi\nHadap\nHook\nID Iklan\nJumlah Lantai\nKamar Mandi\nKamar Mandi Pembantu\nKamar Pembantu\nKamar Tidur\nKondisi Perabotan\nKondisi Properti\nKonsep dan Gaya Rumah\nLebar Jalan\nLuas Bangunan\nLuas Tanah\nMaterial Bangunan\nMaterial Lantai\nNomor Lantai\nPemandangan\nPeriode Sewa\nRuang Makan\nRuang Tamu\nSertifikat\nSumber Air\nTahun Di Renovasi\nTahun Dibangun\nTerjangkau Internet\nTipe Properti\n\n\n\n\n0\nRp 2,6 Miliar\nBSD City, Tangerang\nTempat Jemuran,Keamanan 24 jam,Playground,Wast...\n1\nNaN\nNaN\nNaN\nNaN\nTidak\nhos14814642\n1\n2\n2\n1\n3\nNaN\nBagus\nMinimalis Modern\n3 Mobil\n91 m²\n91 m²\nBatako\nGranit\nNaN\nTaman Kota\nNaN\nYa\nYa\nLainnya (PPJB,Girik,Adat,dll)\nPAM atau PDAM\nNaN\nNaN\nYa\nRumah\n\n\n1\nRp 20 Miliar\nBSD City, Tangerang\nNaN\nNaN\n2\n7700 Watt\nNaN\nNaN\nTidak\nhos13101318\n2\n5\n1\n1\n5\nUnfurnished\nBagus\nNaN\nNaN\n465 m²\n396 m²\nNaN\nNaN\nNaN\nNaN\nNaN\nYa\nYa\nLainnya (PPJB,Girik,Adat,dll)\nPAM atau PDAM\nNaN\nNaN\nTidak\nRumah\n\n\n2\nRp 1,68 Miliar\nBSD City, Tangerang\nTaman,Tempat Jemuran,Keamanan 24 jam\n1\n1\n1300 Watt\nNaN\nUtara\nTidak\nhos14850708\n1\n2\nNaN\nNaN\n2\nUnfurnished\nBagus\nNaN\nNaN\n70 m²\n105 m²\nNaN\nNaN\nNaN\nNaN\nNaN\nYa\nYa\nHGB - Hak Guna Bangunan\nPAM atau PDAM\nNaN\nNaN\nTidak\nRumah\n\n\n3\nRp 1,61 Miliar\nBSD City, Tangerang\nTaman,Tempat Jemuran,Keamanan 24 jam\n1\n1\n2200 Watt\nNaN\nTimur Laut\nTidak\nhos14850789\n2\n3\nNaN\nNaN\n4\nUnfurnished\nBagus\nNaN\nNaN\n174 m²\n144 m²\nNaN\nNaN\nNaN\nNaN\nNaN\nYa\nYa\nSHM - Sertifikat Hak Milik\nPAM atau PDAM\nNaN\nNaN\nTidak\nRumah\n\n\n4\nRp 2,14 Miliar\nBSD City, Tangerang\nTaman,Tempat Jemuran,Keamanan 24 jam\n1\n1\n5500 Watt\nNaN\nUtara\nTidak\nhos14003997\n3\n3\nNaN\n1\n3\nUnfurnished\nBagus\nNaN\nNaN\n170 m²\n105 m²\nNaN\nNaN\nNaN\nNaN\nNaN\nYa\nYa\nSHM - Sertifikat Hak Milik\nPAM atau PDAM\nNaN\nNaN\nTidak\nRumah\n\n\n\n\n\n\n\n\n#remove duplicate data / 'iklan' (ads)\ndf = df.drop_duplicates('ID Iklan').reset_index(drop=True)\n\n\ndf.insert(3, 'komplek', df[['fasilitas']].fillna('none').apply(lambda z: 'ya' if any([x in z.fasilitas.lower() for x in ['lapangan','gym','jogging','playground','one gate system']]) else 'tidak', axis = 1))\ndf.insert(1, 'Kecamatan', df.alamat.str.split(',').str[0])\ndf.insert(2, 'Kota', df.alamat.str.split(',').str[1])\ndf.drop(columns=['alamat','fasilitas'], inplace=True)\n\n\nCreate 3 new columns: 1.’komplek’ = check if the house is in a ‘komplek’ or not. 2.’Kecamatan’ = ‘kecamatan’ of the address. 3.’Kota’ = ‘kota’ of the address.\n\n\n#ensure the selected city is Tangerang\ndf = df[df.Kota == ' Tangerang']\ndf.drop(columns = ['Kota'], inplace=True)\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 22667 entries, 0 to 22673\nData columns (total 34 columns):\n #   Column                 Non-Null Count  Dtype \n---  ------                 --------------  ----- \n 0   harga                  22667 non-null  object\n 1   Kecamatan              22667 non-null  object\n 2   komplek                22667 non-null  object\n 3   Carport                14104 non-null  object\n 4   Dapur                  12157 non-null  object\n 5   Daya Listrik           18287 non-null  object\n 6   Garasi                 6450 non-null   object\n 7   Hadap                  8528 non-null   object\n 8   Hook                   18764 non-null  object\n 9   ID Iklan               22667 non-null  object\n 10  Jumlah Lantai          20569 non-null  object\n 11  Kamar Mandi            21871 non-null  object\n 12  Kamar Mandi Pembantu   8945 non-null   object\n 13  Kamar Pembantu         9597 non-null   object\n 14  Kamar Tidur            21864 non-null  object\n 15  Kondisi Perabotan      16139 non-null  object\n 16  Kondisi Properti       19797 non-null  object\n 17  Konsep dan Gaya Rumah  9493 non-null   object\n 18  Lebar Jalan            12045 non-null  object\n 19  Luas Bangunan          22624 non-null  object\n 20  Luas Tanah             22658 non-null  object\n 21  Material Bangunan      7709 non-null   object\n 22  Material Lantai        8380 non-null   object\n 23  Nomor Lantai           0 non-null      object\n 24  Pemandangan            9987 non-null   object\n 25  Periode Sewa           1 non-null      object\n 26  Ruang Makan            11288 non-null  object\n 27  Ruang Tamu             18765 non-null  object\n 28  Sertifikat             22489 non-null  object\n 29  Sumber Air             13236 non-null  object\n 30  Tahun Di Renovasi      2856 non-null   object\n 31  Tahun Dibangun         8878 non-null   object\n 32  Terjangkau Internet    18761 non-null  object\n 33  Tipe Properti          22667 non-null  object\ndtypes: object(34)\nmemory usage: 6.1+ MB\n\n\n\n\nThere are 29 missing columns. After carefully reviewing each house one by one in the context, I categorized the missing columns into: \n\n\n1.Missing Completely At Random This means that the values are not input by the users due to accidents, forgetfulness, or oversight. These columns are:  Daya Listrik, Kamar Mandi, Kamar Tidur, Luas Bangunan, Luas Tanah, Sertifikat, Sumber Air, Hook\n\n\n2.Missing At Random This means that the values are not input by the users because these values can be obtained from other sources, such as photos or other information. These columns are:  Carport, Dapur, Garasi, Kamar Mandi Pembantu, Kamar Pembantu, Kondisi Perabotan, Kondisi Properti, Jumlah Lantai, Ruang Makan, Ruang Tamu\n\n\n3.Missing Not At Random This means that the values are not input by the users because they either do not know the value or they perceive it as unimportant to input the value. These columns are:  Konsep dan Gaya Rumah, Hadap, Lebar Jalan, Material Bangunan, Material Lantai, Nomor Lantai, Pemandangan, Periode Sewa, Tahun Di Renovasi, Tahun Dibangun, Terjangkau Internet ***\n\n\n#create a function to categorize multiple 'perabot' conditions into more general category\ndef perabot(kondisi):\n    kondisi = str(kondisi)\n    if kondisi.lower() in ['unfurnished','butuh renovasi']:\n        return 'Unfurnished'\n    elif kondisi.lower() in ['furnished','bagus','bagus sekali','baru','sudah renovasi','semi furnished']:\n        return 'Furnished'\n    else:\n        return np.nan\n\ndf['Kondisi Perabotan'] = df[['Kondisi Perabotan']].apply(lambda x: perabot(x['Kondisi Perabotan']), axis = 1)\n\n\n#filtered only houses and not other property\ndf = df[df['Tipe Properti'] == 'Rumah']\n\n\n#extract prices from column 'harga' into integer\ndef price_extract(price):\n    if \"Triliun\" in price:\n        numbers = re.findall(r'\\d+\\.\\d+|\\d+', price)\n        numbers = float(\".\".join(numbers))\n        return int(numbers * 1000000)\n    elif \"Miliar\" in price:\n        numbers = re.findall(r'\\d+\\.\\d+|\\d+', price)\n        numbers = float(\".\".join(numbers))\n        return int(numbers * 1000)\n    elif \"Juta\" in price:\n        numbers = re.findall(r'\\d+\\.\\d+|\\d+', price)\n        numbers = float(\".\".join(numbers))\n        return int(numbers * 1)      \n\ndf.insert(1, 'price',  df[['harga']].apply(lambda x: price_extract(x.harga), axis = 1))\ndf.drop(columns = ['harga'], inplace = True)\n\n\nHere, I converted the ‘price’ column into integers and divided it by one million to make it easier to view and analyze.\n\n\n#move 'ID Iklan' column to left\ndf.insert(3, 'ID',  df['ID Iklan'])\n\n\n#filter Sertifikat to SHM only\ndf = df[df.Sertifikat == 'SHM - Sertifikat Hak Milik']\n\n\n#drop useless columns\ndf.drop(columns=['Kondisi Properti','Sertifikat','Hook','ID Iklan','Lebar Jalan','Dapur','Tipe Properti','Terjangkau Internet','Sumber Air','Ruang Tamu','Ruang Makan','Carport','Garasi','Tahun Di Renovasi','Tahun Dibangun','Hadap','Konsep dan Gaya Rumah','Material Bangunan','Material Lantai','Nomor Lantai','Pemandangan','Periode Sewa'], inplace = True, errors='ignore')\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 17746 entries, 3 to 22673\nData columns (total 13 columns):\n #   Column                Non-Null Count  Dtype \n---  ------                --------------  ----- \n 0   price                 17746 non-null  int64 \n 1   Kecamatan             17746 non-null  object\n 2   komplek               17746 non-null  object\n 3   ID                    17746 non-null  object\n 4   Daya Listrik          14640 non-null  object\n 5   Jumlah Lantai         16312 non-null  object\n 6   Kamar Mandi           17110 non-null  object\n 7   Kamar Mandi Pembantu  6714 non-null   object\n 8   Kamar Pembantu        7296 non-null   object\n 9   Kamar Tidur           17103 non-null  object\n 10  Kondisi Perabotan     12942 non-null  object\n 11  Luas Bangunan         17719 non-null  object\n 12  Luas Tanah            17743 non-null  object\ndtypes: int64(1), object(12)\nmemory usage: 1.9+ MB\n\n\n\ndf = df[~df['Luas Tanah'].isna()].reset_index(drop=True)\ndf = df[~((df['Kamar Mandi'].isna()) & (df['Kamar Mandi Pembantu'].isna()) & (df['Kamar Pembantu'].isna()) & (df['Kamar Tidur'].isna()))].reset_index(drop=True)\n\n\nI removed rows that lacked critical information, such as ‘Luas Tanah’ and the total number of ‘Kamar’.\n\n\nfor kol in ['Kamar Mandi','Kamar Tidur']:\n    df[kol] = df[kol].fillna(1)\n\ndf['Luas Bangunan'] = df['Luas Bangunan'].fillna(df['Luas Tanah'])\ndf['Kamar Mandi Pembantu'] = df['Kamar Mandi Pembantu'].fillna(0)\ndf['Kamar Pembantu'] = df['Kamar Mandi Pembantu'].fillna(0)\n\n\nThe columns mentioned above have a small number of missing values. I filled in the missing values for ‘Kamar Mandi’ and ‘Kamar Tidur’ with ‘1’ because there is at least one of each of them in a house. For ‘Kamar & Kamar Mandi Pembantu,’ I filled in ‘0’.\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 17149 entries, 0 to 17148\nData columns (total 13 columns):\n #   Column                Non-Null Count  Dtype \n---  ------                --------------  ----- \n 0   price                 17149 non-null  int64 \n 1   Kecamatan             17149 non-null  object\n 2   komplek               17149 non-null  object\n 3   ID                    17149 non-null  object\n 4   Daya Listrik          14208 non-null  object\n 5   Jumlah Lantai         15731 non-null  object\n 6   Kamar Mandi           17149 non-null  object\n 7   Kamar Mandi Pembantu  17149 non-null  object\n 8   Kamar Pembantu        17149 non-null  object\n 9   Kamar Tidur           17149 non-null  object\n 10  Kondisi Perabotan     12890 non-null  object\n 11  Luas Bangunan         17149 non-null  object\n 12  Luas Tanah            17149 non-null  object\ndtypes: int64(1), object(12)\nmemory usage: 1.7+ MB\n\n\n\n#If a LB is bigger than LT, I assume that the houses has more than 1 floor.\ndef conditions(x):\n    if pd.isnull(x['Jumlah Lantai']):\n        if x['Luas Bangunan'] &gt; x['Luas Tanah']:\n            return '2'\n        else:\n            return '1'\n    else:\n        return x['Jumlah Lantai']\n\ndf['Jumlah Lantai'] = df[['Jumlah Lantai','Luas Bangunan','Luas Tanah']].apply(lambda x: conditions(x) , axis = 1 )\n\n\n#I replaced certain ambiguous values in the 'Daya Listrik' column with 'NaN' so it can be imputed with KNN Imputer\ncondition = ((df['Daya Listrik'] == 'Lainnya Watt') | (df['Daya Listrik'] == 'lainnya Watt'))\ndf.loc[condition, 'Daya Listrik'] = np.nan\n\n\n#change data types\ndf['Daya Listrik'] = df['Daya Listrik'].str.replace('Watt','').astype('Float64')\ndf['Luas Bangunan'] = df['Luas Bangunan'].str.replace('m²','').astype(int)\ndf['Luas Tanah'] = df['Luas Tanah'].str.replace('m²','').astype(int)\n\n\n#impute with KNN\ndf['avg_bangunan'] = df.groupby('Daya Listrik')['Luas Bangunan'].transform('median').fillna(df['Luas Bangunan'])\nlistrik_impute = pd.DataFrame(KNNImputer(n_neighbors=1).fit_transform(df[['Daya Listrik','avg_bangunan']]))\ndf['Daya Listrik'] = listrik_impute[0]\ndf.drop(columns=['avg_bangunan'], inplace=True)\n\n\n#I selected only the 'Kecamatan' with more than 100 samples to ensure a reliable result\ndf = df[ df.groupby('Kecamatan')['price'].transform('count') &gt; 100]\n\n\ndf['price/m'] = df.price / (df['Luas Tanah'] + df['Luas Bangunan'])\n\n\n#create dictionary to map 'Kondisi Perabotan'\nvalue_mapping = {\n    'Unfurnished': 1,\n    'Furnished' : 2,\n}\n\nreverse_mapping = {\n    1.0 : 'Unfurnished',\n    2.0 : 'Furnished',\n}\n\n\ndf['Kondisi Perabotan'] = df['Kondisi Perabotan'].map(value_mapping)\n\n\nfor kec in df.Kecamatan.unique() :\n    df1 = df[df.Kecamatan == kec][['Kondisi Perabotan','price/m']].copy().reset_index(drop=True)\n    df1['price/m'] = df1.groupby(['Kondisi Perabotan'])['price/m'].transform('median').fillna(df1['price/m'])\n\n    imputer = KNNImputer(n_neighbors = 1)\n    imputer.fit(df1[['Kondisi Perabotan','price/m']])\n    missing_values = df.loc[df[df.Kecamatan == kec].index,['Kondisi Perabotan','price/m']]\n    df.loc[df[df.Kecamatan == kec].index,['Kondisi Perabotan','price/m']] = imputer.transform(missing_values)\n        \ndf.reset_index(drop=True, inplace=True)\ndf['Kondisi Perabotan'] = df['Kondisi Perabotan'].map(reverse_mapping)\n\n\nTo impute missing values for ‘Kondisi Perabotan’, first gather the ‘price/m’ data for each ‘Kecamatan’. Once we have this data, we can observe that ‘Kondisi Perabotan’ prices vary significantly, with furnished properties being considerably more expensive than unfurnished ones. Then, we will use the KNN Imputer to fill in the missing values with the nearest neighbors’ values.\n\n\ndf.drop(columns = ['price/m'], inplace = True)\n\n\n#change data types\ndf = df.convert_dtypes(convert_string = False)\nfor column in ['Kamar Mandi','Kamar Mandi Pembantu','Kamar Pembantu','Kamar Tidur','price','Daya Listrik','Luas Bangunan','Luas Tanah']:\n    df[column] = df[column].astype(int)\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 17050 entries, 0 to 17049\nData columns (total 13 columns):\n #   Column                Non-Null Count  Dtype \n---  ------                --------------  ----- \n 0   price                 17050 non-null  int64 \n 1   Kecamatan             17050 non-null  object\n 2   komplek               17050 non-null  object\n 3   ID                    17050 non-null  object\n 4   Daya Listrik          17050 non-null  int64 \n 5   Jumlah Lantai         17050 non-null  object\n 6   Kamar Mandi           17050 non-null  int64 \n 7   Kamar Mandi Pembantu  17050 non-null  int64 \n 8   Kamar Pembantu        17050 non-null  int64 \n 9   Kamar Tidur           17050 non-null  int64 \n 10  Kondisi Perabotan     17050 non-null  object\n 11  Luas Bangunan         17050 non-null  int64 \n 12  Luas Tanah            17050 non-null  int64 \ndtypes: int64(8), object(5)\nmemory usage: 1.7+ MB\n\n\n\ndf.drop(columns = 'ID', inplace = True)\ndf.duplicated().sum()\n\n2877\n\n\n\nAfter getting rid of the ‘ID’ column, you’ll notice there are still lots of duplicates left. It seems like the same ads are being posted multiple times. Let’s go ahead and clean those up.\n\n\ndf = df.drop_duplicates()\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 14173 entries, 0 to 17049\nData columns (total 12 columns):\n #   Column                Non-Null Count  Dtype \n---  ------                --------------  ----- \n 0   price                 14173 non-null  int64 \n 1   Kecamatan             14173 non-null  object\n 2   komplek               14173 non-null  object\n 3   Daya Listrik          14173 non-null  int64 \n 4   Jumlah Lantai         14173 non-null  object\n 5   Kamar Mandi           14173 non-null  int64 \n 6   Kamar Mandi Pembantu  14173 non-null  int64 \n 7   Kamar Pembantu        14173 non-null  int64 \n 8   Kamar Tidur           14173 non-null  int64 \n 9   Kondisi Perabotan     14173 non-null  object\n 10  Luas Bangunan         14173 non-null  int64 \n 11  Luas Tanah            14173 non-null  int64 \ndtypes: int64(8), object(4)\nmemory usage: 1.4+ MB\n\n\n\ndf = df[df['Jumlah Lantai'].astype(int) &lt;= 4]\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 14161 entries, 0 to 17049\nData columns (total 12 columns):\n #   Column                Non-Null Count  Dtype \n---  ------                --------------  ----- \n 0   price                 14161 non-null  int64 \n 1   Kecamatan             14161 non-null  object\n 2   komplek               14161 non-null  object\n 3   Daya Listrik          14161 non-null  int64 \n 4   Jumlah Lantai         14161 non-null  object\n 5   Kamar Mandi           14161 non-null  int64 \n 6   Kamar Mandi Pembantu  14161 non-null  int64 \n 7   Kamar Pembantu        14161 non-null  int64 \n 8   Kamar Tidur           14161 non-null  int64 \n 9   Kondisi Perabotan     14161 non-null  object\n 10  Luas Bangunan         14161 non-null  int64 \n 11  Luas Tanah            14161 non-null  int64 \ndtypes: int64(8), object(4)\nmemory usage: 1.4+ MB\n\n\n\ndf5 = df.copy()\n\n\n#rename columns\ndf.rename(columns={'Daya Listrik': 'Listrik', \n                   'Jumlah Lantai': 'Lantai',\n                   'Kamar Mandi': 'KM',\n                   'Kamar Mandi Pembantu': 'KMP',\n                   'Kamar Pembantu': 'KP',\n                   'Kamar Tidur': 'KT',\n                   'Kondisi Perabotan': 'Kondisi',\n                   'Luas Bangunan': 'LB',\n                   'Luas Tanah': 'LT'}, inplace=True)\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 14161 entries, 0 to 17049\nData columns (total 12 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   price      14161 non-null  int64 \n 1   Kecamatan  14161 non-null  object\n 2   komplek    14161 non-null  object\n 3   Listrik    14161 non-null  int64 \n 4   Lantai     14161 non-null  object\n 5   KM         14161 non-null  int64 \n 6   KMP        14161 non-null  int64 \n 7   KP         14161 non-null  int64 \n 8   KT         14161 non-null  int64 \n 9   Kondisi    14161 non-null  object\n 10  LB         14161 non-null  int64 \n 11  LT         14161 non-null  int64 \ndtypes: int64(8), object(4)\nmemory usage: 1.4+ MB\n\n\n\ndf.Kecamatan.unique()\n\narray(['BSD City', 'Poris', 'Karang Tengah', 'Pondok Benda',\n       'Tangerang Kota', 'BSD Delatinos', 'Serua', 'Karawaci', 'Jombang',\n       'Cikupa', 'Curug', 'Suradita', 'Panongan', 'BSD Residence One',\n       'Kelapa Dua', 'Pondok Ranji', 'Ciledug', 'Pondok Cabe',\n       'Gading Serpong', 'Pagedangan', 'Legok', 'Pinang', 'Cikokol',\n       'Dadap', 'BSD Foresta', 'BSD Green Wich', 'Cireundeu',\n       'BSD Anggrek Loka', 'Cipondoh', 'BSD Nusaloka', 'BSD Puspita Loka',\n       'Gading Serpong Pondok Hijau Golf', 'BSD The Green', 'Cipadu',\n       'Larangan', 'BSD Giri Loka', 'Cikupa Citra Raya', 'Modernland',\n       'Benda', 'Rempoa', 'Alam Sutera', 'BSD', 'Cisauk', 'Graha Raya',\n       'Lippo Karawaci'], dtype=object)"
  },
  {
    "objectID": "ipynb/houses/house_tangerang.html#basic-exploratory-data-analysis",
    "href": "ipynb/houses/house_tangerang.html#basic-exploratory-data-analysis",
    "title": "Tangerang house price prediction",
    "section": "",
    "text": "#Let's split the categorical and numerical columns into separate dataframes for easier analysis.\ndfnum = df._get_numeric_data()\ndfcat = df.drop(columns = dfnum.columns)\n\n\n\n\n#numerical columns describe.\ndfnum.describe()\n\n\n\n\n\n\n\n\nprice\nListrik\nKM\nKMP\nKP\nKT\nLB\nLT\n\n\n\n\ncount\n1.416100e+04\n14161.000000\n14161.000000\n14161.000000\n14161.000000\n14161.000000\n14161.000000\n1.416100e+04\n\n\nmean\n4.525861e+03\n3892.131205\n2.717958\n0.419603\n0.419603\n3.521715\n187.472848\n4.820822e+04\n\n\nstd\n1.518706e+05\n6346.367405\n2.124905\n0.535776\n0.535776\n2.272806\n209.443483\n5.714284e+06\n\n\nmin\n7.000000e+00\n-1300.000000\n1.000000\n0.000000\n0.000000\n1.000000\n1.000000\n7.000000e+00\n\n\n25%\n1.200000e+03\n2200.000000\n2.000000\n0.000000\n0.000000\n3.000000\n81.000000\n8.300000e+01\n\n\n50%\n2.000000e+03\n2200.000000\n2.000000\n0.000000\n0.000000\n3.000000\n135.000000\n1.260000e+02\n\n\n75%\n3.500000e+03\n4400.000000\n3.000000\n1.000000\n1.000000\n4.000000\n225.000000\n2.050000e+02\n\n\nmax\n1.800000e+07\n95000.000000\n99.000000\n10.000000\n10.000000\n99.000000\n8032.000000\n6.800000e+08\n\n\n\n\n\n\n\n\nAll columns above have outliers. ‘Listrik’ column has negative value which is caused by incorrect user input\n\n\ndf['Listrik'] = abs(df['Listrik'])\n\n\ndfcat.describe()\n\n\n\n\n\n\n\n\nKecamatan\nkomplek\nLantai\nKondisi\n\n\n\n\ncount\n14161\n14161\n14161\n14161\n\n\nunique\n45\n2\n4\n2\n\n\ntop\nPoris\ntidak\n2\nUnfurnished\n\n\nfreq\n503\n7852\n8818\n8014\n\n\n\n\n\n\n\n\nThere are 45 ‘Kecamatan’ used on this samples.\n\n\n\n\n\nfig, axarr = plt.subplots(2,4, figsize=(11, 8))\nfor col in dfnum.columns:\n    loc = dfnum.columns.get_loc(col)\n    y = 0 if loc &lt; 4 else 1\n    x = loc if loc &lt; 4 else loc - 4\n    sns.boxplot(y = dfnum[col], ax = axarr[y,x])\n    axarr[y, x].set_ylabel(None)\n    axarr[y, x].set_xlabel(col)\nplt.suptitle('Outliers checking on numeric columns', weight='bold')\nfig.tight_layout(pad=1)\nplt.show()\n\n\n\n\n\n#removing extreme values\ndf = df[(df.price &lt; 10000)]\ndf = df[(df['Listrik'] &lt; 20000)]\ndf = df[(df['KM'] &lt; 10)]\ndf = df[(df['KMP'] &lt;= 2)]\ndf = df[(df['KP'] &lt;= 2)]\ndf = df[(df['KT'] &lt;= 20)]\ndf = df[(df['LB'] &lt;= 1000)]\ndf = df[(df['LT'] &lt;= 1000)]\n\n\ndfnum = df._get_numeric_data()\ndfcat = df.drop(columns = dfnum.columns)\n\n\nfig, axarr = plt.subplots(2,4, figsize=(11, 8))\nfor col in dfnum.columns:\n    loc = dfnum.columns.get_loc(col)\n    y = 0 if loc &lt; 4 else 1\n    x = loc if loc &lt; 4 else loc - 4\n    sns.histplot(data=dfnum[col], \n                 color='skyblue', \n                 kde=True, \n                 edgecolor='none', \n                 ax=axarr[y, x])\nplt.suptitle('Distribution checking on numeric columns', weight='bold')\nfig.tight_layout(pad=1)\nplt.show()\n\n\n\n\n\nWe can clearly see that this data is right skewed.\n\n\nfig, axarr = plt.subplots(2,2, figsize=(11, 8))\nfor col in dfcat.columns:\n    loc = dfcat.columns.get_loc(col)\n    y = 0 if loc &lt; 2 else 1\n    x = loc if loc &lt; 2 else loc - 2\n    sns.countplot(x=df[col], color='green', ax = axarr[y,x], order=df[col].value_counts().iloc[:4].index)\n\nplt.suptitle('Countplot of categorical columns', weight='bold')\nfig.tight_layout(pad=1)\nplt.show()\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10,8))\nsns.heatmap(dfnum.corr(), annot=True, fmt='.2f')\nplt.title('Correlation plot', weight ='bold')\nplt.xticks(rotation = 0)\nplt.show()\n\n\n\n\n\nprice, LB, and LT, exhibit a strong positive correlation. Although this correlation is quite strong, it does not lead to multicollinearity issues."
  },
  {
    "objectID": "ipynb/houses/house_tangerang.html#deep-dive-exploratory-data-analysis",
    "href": "ipynb/houses/house_tangerang.html#deep-dive-exploratory-data-analysis",
    "title": "Tangerang house price prediction",
    "section": "",
    "text": "df['KMR'] = df.KP + df.KT\ndf['KMD'] = df.KM + df.KMP\ndf['price/m'] = df.price / (df.LT + df.LB) \ndf['Lantai'] = df.Lantai.astype(int)\ndf.drop(columns=['KM','KMP','KP','KT'], inplace=True)\n\n\nI’ve developed a new feature named ‘price/m’ by dividing the price by the sum of LT and LB. This feature is more suitable for analysis compared to using just the price alone.\n\n\nsns.histplot(df.groupby('Kecamatan').size(), alpha=0.8)\nplt.xlabel('Kecamatan Frequency')\nplt.title('Kecamatan distribution count', weight='bold')\nplt.show()\n\n\n\n\n\nThe plot above illustrates the distribution of samples collected from each Kecamatan. The sample size varies, with the lowest being approximately 100 and the highest reaching 500. In cases of insufficient sample size, potential issues may arise, and further analysis is necessary to determine whether these samples are adequate.\n\n\nfig, axarr = plt.subplots(1,2, figsize=(10, 4))\ndata = df.groupby('Kecamatan').size().reset_index(drop=False).sort_values(0, ascending=False)\ndata.columns = ['Kecamatan','count']\nsns.barplot(data = data.head(15), x='count', y='Kecamatan', ax = axarr[0])\nsns.barplot(data = data.tail(15), x='count', y='Kecamatan', ax = axarr[1], palette='RdBu')\naxarr[0].set_title('Top 15 highest kecamatan count', weight='bold')\naxarr[1].set_title('Top 15 lowest kecamatan count', weight='bold')\naxarr[0].set_position([0.1, 0.1, 0.3, 0.8])\naxarr[1].set_position([0.6, 0.1, 0.3, 0.8])\n\n\n\n\n\ndf['Kecamatan'] = df.Kecamatan.str.replace('Gading Serpong Pondok Hijau Golf' , 'Gading Serpong PHG')\n\n\nfig, axarr = plt.subplots(1,2, figsize=(10, 4))\ndata = df.groupby('Kecamatan')[['price']].agg('median').reset_index().sort_values('price', ascending=False)\nsns.barplot(data = data.head(15), x='price', y='Kecamatan', ax = axarr[0], palette='plasma')\nsns.barplot(data = data.tail(15), x='price', y='Kecamatan', ax = axarr[1], palette='coolwarm')\nplt.title('harga per kecamatan')\naxarr[0].set_title('Top 15 highest kecamatan price', weight='bold')\naxarr[1].set_title('Top 15 lowest kecamatan price', weight='bold')\naxarr[0].set_position([0.1, 0.1, 0.3, 0.8])\naxarr[1].set_position([0.6, 0.1, 0.3, 0.8])\n\n\n\n\n\nBSD is one of the most expensive region in Tangerang\n\n\nfig, axarr = plt.subplots(1,2, figsize=(10, 4))\ndata = df.groupby('Kecamatan')[['price/m']].agg('median').reset_index().sort_values('price/m', ascending=False)\nsns.barplot(data = data.head(15), x='price/m', y='Kecamatan', ax = axarr[0], palette = 'YlGn')\nsns.barplot(data = data.tail(15), x='price/m', y='Kecamatan', ax = axarr[1], palette = 'YlGnBu')\nplt.title('harga meteran per kecamatan')\naxarr[0].set_title('Top 15 highest kecamatan price/m', weight='bold')\naxarr[1].set_title('Top 15 lowest kecamatan price/m', weight='bold')\naxarr[0].set_position([0.1, 0.1, 0.3, 0.8])\naxarr[1].set_position([0.6, 0.1, 0.3, 0.8])\n\n\n\n\n\nfrom the price/m, bsd is still one of highest price/m\n\n\ndf.Kondisi = df.Kondisi.map(value_mapping)\n\n\ndf['TK'] = df.KMR + df.KMD\ndf.drop(columns=['KMR','KMD'], inplace=True)\n\n\ndf['avg_price'] = df.groupby(['Kecamatan','Kondisi'])['price/m'].transform('median')\n\n\ndf['rstd'] = df.groupby(['Kecamatan','Kondisi'])['price/m'].transform(lambda x: x.std() * 100 / x.mean())\n\n\ndf1 = df[df.Kondisi == 1]\ndf2 = df[df.Kondisi == 2]\n\nfig, axarr = plt.subplots(1,2, figsize=(10, 4))\ndata = df1[['Kecamatan','rstd']].drop_duplicates().sort_values('rstd', ascending=False).head()\nsns.barplot(data = data, x='rstd', y='Kecamatan', ax = axarr[0])\nsns.histplot(data = df1.rstd.drop_duplicates(), ax = axarr[1])\nsns.histplot(data = df2.rstd.drop_duplicates(), ax = axarr[1], alpha = 0.5, legend='a')\naxarr[0].set_title('Top 5 highest kecamatan price variability', weight='bold')\naxarr[1].set_title('r-std.dev price/m per kecamatan', weight='bold')\naxarr[0].yaxis.set_tick_params(labelsize=10)\n\nhand2 = [mpatches.Rectangle((0, 0), 1, 1, color='blue', label='Unfurnished')]\nhand1 = [mpatches.Rectangle((0, 0), 1, 1, color='orange', label='Furnished')]\nlegend1 = plt.legend(handles = hand2, loc='upper right', labelcolor='blue')\nlegend2 = plt.legend(handles = hand1, loc='upper left', labelcolor='orange')\nplt.gca().add_artist(legend1)\nplt.show()\n\n\n\n\n\nR-std, which stands for relative standard deviation, is a valuable tool for assessing variability and making comparisons between datasets. In the displayed plot, we observe that the r-std is distributed within a substantial 30% range. When the price exhibits a high r-std, it typically indicates an insufficient number of samples or a failure to account for critical factors influencing price fluctuations.\n\n\n#create a variable to contain avg_price information from the data to be used by test data\navg = df[['Kecamatan','Kondisi','avg_price']].drop_duplicates()\n\n\n#remove columns that are no longer needed\ndf9 = df.copy()\ndf = df.drop(columns=['Kecamatan','price/m','rstd'])\n\n\n#encoding\ndf['komplek'] =  df[['komplek']].apply(lambda x: 1 if x.komplek == 'ya' else 0, axis = 1)\n\n\n#create a variable to contain result from all combination\nfinal_result = pd.DataFrame(index = ['mae','mape','rmse','r2'])"
  },
  {
    "objectID": "ipynb/houses/house_tangerang.html#modelling-combination-1",
    "href": "ipynb/houses/house_tangerang.html#modelling-combination-1",
    "title": "Tangerang house price prediction",
    "section": "",
    "text": "For the first combination, I will utilize all available features and apply linear regression.\n\ndfc1 = df.copy().reset_index(drop=True)\n\n\ndfc1.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 13428 entries, 0 to 13427\nData columns (total 9 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   price      13428 non-null  int64  \n 1   komplek    13428 non-null  int64  \n 2   Listrik    13428 non-null  int64  \n 3   Lantai     13428 non-null  int64  \n 4   Kondisi    13428 non-null  int64  \n 5   LB         13428 non-null  int64  \n 6   LT         13428 non-null  int64  \n 7   TK         13428 non-null  int64  \n 8   avg_price  13428 non-null  float64\ndtypes: float64(1), int64(8)\nmemory usage: 944.3 KB\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(dfc1.drop(columns='price'), dfc1.price.to_numpy(), test_size = 0.2, random_state=129)\nX_train = X_train.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)\n\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\ny_pred_test = model.predict(X_test)\ny_pred_train = model.predict(X_train)\n\n\nresult = pd.DataFrame()\nmae_test = mean_absolute_error(y_test, y_pred_test)\nmape_test = mean_absolute_percentage_error(y_test, y_pred_test)\nrmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\nr2_test = r2_score(y_test, y_pred_test)\n\nmae_train = mean_absolute_error(y_train, y_pred_train)\nmape_train = mean_absolute_percentage_error(y_train, y_pred_train)\nrmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\nr2_train = r2_score(y_train, y_pred_train)\n\nresult['test_OLS'] = [mae_test, mape_test, rmse_test, r2_test]\nresult['train_OLS'] = [mae_train, mape_train, rmse_train, r2_train]\nresult.index = ['mae','mape','rmse','r2']\n\n\nresult\n\n\n\n\n\n\n\n\ntest_OLS\ntrain_OLS\n\n\n\n\nmae\n532.715149\n541.468771\n\n\nmape\n0.278840\n0.329897\n\n\nrmse\n800.262693\n805.231411\n\n\nr2\n0.797440\n0.794544\n\n\n\n\n\n\n\n\nAbove is the result of the base model.\n\n\nfinal_result = pd.concat([final_result, result], axis = 1)"
  },
  {
    "objectID": "ipynb/houses/house_tangerang.html#modelling-combination-2",
    "href": "ipynb/houses/house_tangerang.html#modelling-combination-2",
    "title": "Tangerang house price prediction",
    "section": "",
    "text": "In this second combination, I intend to enhance model complexity by incorporating polynomial features. If overfitting concerns arise, I will address them by applying ridge regression.\n\ndfc2 = df.copy().reset_index(drop=True) \n\n\ndfc2.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 13428 entries, 0 to 13427\nData columns (total 9 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   price      13428 non-null  int64  \n 1   komplek    13428 non-null  int64  \n 2   Listrik    13428 non-null  int64  \n 3   Lantai     13428 non-null  int64  \n 4   Kondisi    13428 non-null  int64  \n 5   LB         13428 non-null  int64  \n 6   LT         13428 non-null  int64  \n 7   TK         13428 non-null  int64  \n 8   avg_price  13428 non-null  float64\ndtypes: float64(1), int64(8)\nmemory usage: 944.3 KB\n\n\n\nscaler = MinMaxScaler()\ndf1 = scaler.fit_transform(dfc2.drop(columns=['price']))\ndf1 = pd.DataFrame(df1, columns = dfc2.drop(columns=['price']).columns)\ndf1['price'] = dfc2.price\ndfc2 = df1.copy()\n\n\nX_train, X_test, y_train, y_test = train_test_split(dfc2.drop(columns='price'), dfc2.price.to_numpy(), test_size=0.2, random_state=129)\n\n\nresult = pd.DataFrame()\nfor x in [2,3,4]:\n    poly_features = PolynomialFeatures(degree=x)\n    X_poly = poly_features.fit_transform(X_train)\n    X_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(X_poly, y_train, test_size=0.2, random_state=129)\n\n    model = LinearRegression()\n    model.fit(X_train_poly, y_train_poly)\n\n    y_pred_test = model.predict(X_test_poly)\n    y_pred_train = model.predict(X_train_poly)\n\n    mae_test = mean_absolute_error(y_test_poly, y_pred_test)\n    rmse_test = np.sqrt(mean_squared_error(y_test_poly, y_pred_test))\n    r2_test = r2_score(y_test_poly, y_pred_test)\n\n    mae_train = mean_absolute_error(y_train_poly, y_pred_train)\n    rmse_train = np.sqrt(mean_squared_error(y_train_poly, y_pred_train))\n    r2_train = r2_score(y_train_poly, y_pred_train)\n\n    result[f'test_{x}'] = [mae_test, rmse_test, r2_test]\n    result[f'train_{x}'] = [mae_train, rmse_train, r2_train]\n\nresult.index = ['mae','rmse','r2']\nresult\n\n\n\n\n\n\n\n\ntest_2\ntrain_2\ntest_3\ntrain_3\ntest_4\ntrain_4\n\n\n\n\nmae\n472.915901\n481.615889\n472.519472\n464.954008\n501.537211\n442.182946\n\n\nrmse\n712.046550\n738.563833\n719.862027\n705.295298\n834.318214\n660.588687\n\n\nr2\n0.837082\n0.827754\n0.833486\n0.842922\n0.776326\n0.862204\n\n\n\n\n\n\n\n\nI tested a polynomial degree of 3, and the results indicate a minor degree of overfitting. However, when I increased it to degree 4, the model exhibited a severe overfitting issue.\n\n\nresult = pd.DataFrame()\nfor x in [3,4]:\n    poly_features = PolynomialFeatures(degree=x)\n    X_poly = poly_features.fit_transform(X_train)\n    X_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(X_poly, y_train, test_size=0.2, random_state=129)\n\n    for alpha in [0.01, 0.1, 1., 10]:\n        model = Ridge(alpha=alpha)\n        model.fit(X_train_poly, y_train_poly)\n\n        y_pred_test = model.predict(X_test_poly)\n        y_pred_train = model.predict(X_train_poly)\n\n        mae_test = mean_absolute_error(y_test_poly, y_pred_test)\n        rmse_test = np.sqrt(mean_squared_error(y_test_poly, y_pred_test))\n        r2_test = r2_score(y_test_poly, y_pred_test)\n\n        mae_train = mean_absolute_error(y_train_poly, y_pred_train)\n        rmse_train = np.sqrt(mean_squared_error(y_train_poly, y_pred_train))\n        r2_train = r2_score(y_train_poly, y_pred_train)\n\n        result[f'test_{x}_{alpha}'] = [mae_test, rmse_test, r2_test]\n        result[f'train_{x}_{alpha}'] = [mae_train, rmse_train, r2_train]\n\nresult.index = ['mae','rmse','r2']\nresult\n\n\n\n\n\n\n\n\ntest_3_0.01\ntrain_3_0.01\ntest_3_0.1\ntrain_3_0.1\ntest_3_1.0\ntrain_3_1.0\ntest_3_10\ntrain_3_10\ntest_4_0.01\ntrain_4_0.01\ntest_4_0.1\ntrain_4_0.1\ntest_4_1.0\ntrain_4_1.0\ntest_4_10\ntrain_4_10\n\n\n\n\nmae\n467.342704\n466.439950\n464.299064\n470.035509\n470.187015\n478.290378\n484.975821\n490.051383\n464.840133\n456.548113\n462.897803\n463.601116\n466.596826\n473.665648\n481.134291\n487.347859\n\n\nrmse\n704.461373\n708.507262\n698.403249\n717.934342\n707.437585\n731.792609\n726.156035\n749.558649\n706.885343\n690.605679\n695.774749\n704.905074\n700.618287\n723.246246\n720.228905\n743.215049\n\n\nr2\n0.840534\n0.841488\n0.843265\n0.837242\n0.839184\n0.830898\n0.830561\n0.822587\n0.839435\n0.849397\n0.844443\n0.843096\n0.842270\n0.834824\n0.833316\n0.825578\n\n\n\n\n\n\n\n\nI attempted to use ridge regression to tackle overfitting on highly polynomial features, but the outcome wasn’t superior to simply using a polynomial degree of 2 without any regularization.\n\n\npoly_features = PolynomialFeatures(degree=2)\nX_train = poly_features.fit_transform(X_train)\nX_test = poly_features.fit_transform(X_test)\n\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\ny_pred_test = model.predict(X_test)\ny_pred_train = model.predict(X_train)\n\n\nresult = pd.DataFrame()\nmae_test = mean_absolute_error(y_test, y_pred_test)\nmape_test = mean_absolute_percentage_error(y_test, y_pred_test)\nrmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\nr2_test = r2_score(y_test, y_pred_test)\n\nmae_train = mean_absolute_error(y_train, y_pred_train)\nmape_train = mean_absolute_percentage_error(y_train, y_pred_train)\nrmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\nr2_train = r2_score(y_train, y_pred_train)\n\nresult['test_PolyRidgeOLS'] = [mae_test, mape_test, rmse_test, r2_test]\nresult['train_PolyRidgeOLS'] = [mae_train, mape_train, rmse_train, r2_train]\nresult.index = ['mae','mape','rmse','r2']\n\n\nresult\n\n\n\n\n\n\n\n\ntest_PolyRidgeOLS\ntrain_PolyRidgeOLS\n\n\n\n\nmae\n471.090238\n478.917020\n\n\nmape\n0.227019\n0.276395\n\n\nrmse\n729.230240\n732.438213\n\n\nr2\n0.831804\n0.830011\n\n\n\n\n\n\n\n\nThe model is not overfitting and the result is better than the base model\n\n\nfinal_result = pd.concat([final_result, result], axis = 1)"
  },
  {
    "objectID": "ipynb/houses/house_tangerang.html#modelling-combination-3",
    "href": "ipynb/houses/house_tangerang.html#modelling-combination-3",
    "title": "Tangerang house price prediction",
    "section": "",
    "text": "Let’s try to log transform skewed features and use linear regression.\n\ndfc3 = df.copy().reset_index(drop=True)\n\n\ncolumn = ['LB','LT','TK']\nfor col in column:\n    dfc3[col] = np.log(dfc3[col])\n\n\ndfc3.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 13428 entries, 0 to 13427\nData columns (total 9 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   price      13428 non-null  int64  \n 1   komplek    13428 non-null  int64  \n 2   Listrik    13428 non-null  int64  \n 3   Lantai     13428 non-null  int64  \n 4   Kondisi    13428 non-null  int64  \n 5   LB         13428 non-null  float64\n 6   LT         13428 non-null  float64\n 7   TK         13428 non-null  float64\n 8   avg_price  13428 non-null  float64\ndtypes: float64(4), int64(5)\nmemory usage: 944.3 KB\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(dfc3.drop(columns='price'), dfc3.price.to_numpy(), test_size = 0.2, random_state=129)\nX_train = X_train.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)\n\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\ny_pred_test = model.predict(X_test)\ny_pred_train = model.predict(X_train)\n\n\nresult = pd.DataFrame()\nmae_test = mean_absolute_error(y_test, y_pred_test)\nmape_test = mean_absolute_percentage_error(y_test, y_pred_test)\nrmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\nr2_test = r2_score(y_test, y_pred_test)\n\nmae_train = mean_absolute_error(y_train, y_pred_train)\nmape_train = mean_absolute_percentage_error(y_train, y_pred_train)\nrmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\nr2_train = r2_score(y_train, y_pred_train)\n\nresult['test_LogOLS'] = [mae_test, mape_test, rmse_test, r2_test]\nresult['train_LogOLS'] = [mae_train, mape_train, rmse_train, r2_train]\nresult.index = ['mae','mape','rmse','r2']\n\n\nresult\n\n\n\n\n\n\n\n\ntest_LogOLS\ntrain_LogOLS\n\n\n\n\nmae\n583.179251\n595.413840\n\n\nmape\n0.329967\n0.394927\n\n\nrmse\n835.080845\n841.121663\n\n\nr2\n0.779431\n0.775821\n\n\n\n\n\n\n\n\nThe results show a performance decline compared to using Linear Regression on the original, untransformed features.\n\n\nfinal_result = pd.concat([final_result, result], axis = 1)"
  },
  {
    "objectID": "ipynb/houses/house_tangerang.html#modelling-combination-4",
    "href": "ipynb/houses/house_tangerang.html#modelling-combination-4",
    "title": "Tangerang house price prediction",
    "section": "",
    "text": "For this combination, let’s try XGBRegressor\n\ndfc4 = df.copy()\n\n\ndfc4.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 13428 entries, 0 to 17049\nData columns (total 9 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   price      13428 non-null  int64  \n 1   komplek    13428 non-null  int64  \n 2   Listrik    13428 non-null  int64  \n 3   Lantai     13428 non-null  int64  \n 4   Kondisi    13428 non-null  int64  \n 5   LB         13428 non-null  int64  \n 6   LT         13428 non-null  int64  \n 7   TK         13428 non-null  int64  \n 8   avg_price  13428 non-null  float64\ndtypes: float64(1), int64(8)\nmemory usage: 1.0 MB\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(dfc4.drop(columns='price'), dfc4.price.to_numpy(), test_size = 0.2, random_state=129)\nX_train = X_train.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)\n\n\nxgb_regressor = XGBRegressor(\n    objective='reg:squarederror',  \n    eval_metric='rmse',           \n    random_state=42               \n)\n\n\nparam_grid = {  \n    'learning_rate': [0.001, 0.01],  \n    'n_estimators': [100, 400, 900, 1000],  \n    'max_depth': [3, 4],  \n    'eval_metric': ['rmse'],  \n}\n\n\ngrid_search = GridSearchCV(\n    estimator=xgb_regressor,\n    param_grid=param_grid,\n    scoring='neg_mean_squared_error',  \n    cv=5,                              \n    verbose=1,                         \n    n_jobs=-1                           \n)\n\n\ngrid_search.fit(X_train, y_train)\n\nFitting 5 folds for each of 16 candidates, totalling 80 fits\n\n\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(dtype):\n/home/vertikal/data_science/exploration/houses-explore/.venv/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n\n\nGridSearchCV(cv=5,\n             estimator=XGBRegressor(base_score=None, booster=None,\n                                    callbacks=None, colsample_bylevel=None,\n                                    colsample_bynode=None,\n                                    colsample_bytree=None, device=None,\n                                    early_stopping_rounds=None,\n                                    enable_categorical=False,\n                                    eval_metric='rmse', feature_types=None,\n                                    gamma=None, grow_policy=None,\n                                    importance_type=None,\n                                    interaction_constraints=None,\n                                    learning_rate=None...\n                                    max_depth=None, max_leaves=None,\n                                    min_child_weight=None, missing=nan,\n                                    monotone_constraints=None,\n                                    multi_strategy=None, n_estimators=None,\n                                    n_jobs=None, num_parallel_tree=None,\n                                    random_state=42, ...),\n             n_jobs=-1,\n             param_grid={'eval_metric': ['rmse'],\n                         'learning_rate': [0.001, 0.01], 'max_depth': [3, 4],\n                         'n_estimators': [100, 400, 900, 1000]},\n             scoring='neg_mean_squared_error', verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5,\n             estimator=XGBRegressor(base_score=None, booster=None,\n                                    callbacks=None, colsample_bylevel=None,\n                                    colsample_bynode=None,\n                                    colsample_bytree=None, device=None,\n                                    early_stopping_rounds=None,\n                                    enable_categorical=False,\n                                    eval_metric='rmse', feature_types=None,\n                                    gamma=None, grow_policy=None,\n                                    importance_type=None,\n                                    interaction_constraints=None,\n                                    learning_rate=None...\n                                    max_depth=None, max_leaves=None,\n                                    min_child_weight=None, missing=nan,\n                                    monotone_constraints=None,\n                                    multi_strategy=None, n_estimators=None,\n                                    n_jobs=None, num_parallel_tree=None,\n                                    random_state=42, ...),\n             n_jobs=-1,\n             param_grid={'eval_metric': ['rmse'],\n                         'learning_rate': [0.001, 0.01], 'max_depth': [3, 4],\n                         'n_estimators': [100, 400, 900, 1000]},\n             scoring='neg_mean_squared_error', verbose=1)estimator: XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric='rmse', feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=None, n_jobs=None,\n             num_parallel_tree=None, random_state=42, ...)XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric='rmse', feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=None, n_jobs=None,\n             num_parallel_tree=None, random_state=42, ...)\n\n\n\nbest_params_xgb = grid_search.best_params_\n\n\nbest_params_xgb\n\n{'eval_metric': 'rmse',\n 'learning_rate': 0.01,\n 'max_depth': 4,\n 'n_estimators': 1000}\n\n\n\nxgboost = XGBRegressor(**best_params_xgb, random_state=42)\n\n\nxgboost.fit(X_train, y_train)\n\nXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric='rmse', feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=4, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=1000, n_jobs=None,\n             num_parallel_tree=None, random_state=42, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric='rmse', feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=4, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=1000, n_jobs=None,\n             num_parallel_tree=None, random_state=42, ...)\n\n\n\ny_pred_test = xgboost.predict(X_test)\ny_pred_train = xgboost.predict(X_train)\n\n\nresult = pd.DataFrame()\nmae_test = mean_absolute_error(y_test, y_pred_test)\nmape_test = mean_absolute_percentage_error(y_test, y_pred_test)\nrmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\nr2_test = r2_score(y_test, y_pred_test)\n\nmae_train = mean_absolute_error(y_train, y_pred_train)\nmape_train = mean_absolute_percentage_error(y_train, y_pred_train)\nrmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\nr2_train = r2_score(y_train, y_pred_train)\n\nresult['test_XGB'] = [mae_test, mape_test, rmse_test, r2_test]\nresult['train_XGB'] = [mae_train, mape_train, rmse_train, r2_train]\nresult.index = ['mae','mape','rmse','r2']\n\n\nresult\n\n\n\n\n\n\n\n\ntest_XGB\ntrain_XGB\n\n\n\n\nmae\n438.290716\n419.337764\n\n\nmape\n0.213093\n0.253128\n\n\nrmse\n678.516114\n626.639712\n\n\nr2\n0.854384\n0.875573\n\n\n\n\n\n\n\n\nThe model is little overfit. But this is the best model so far\n\n\nfinal_result = pd.concat([final_result, result], axis = 1)\n\n\nxgb.plot_importance(xgboost)  # You can also use 'gain' or 'cover' for importance_type\nplt.show()"
  },
  {
    "objectID": "ipynb/houses/house_tangerang.html#modelling-combination-5",
    "href": "ipynb/houses/house_tangerang.html#modelling-combination-5",
    "title": "Tangerang house price prediction",
    "section": "",
    "text": "I will use RandomForest on this combination. But using RandomForest model on this case is very computational expensive because of many continuous features.\n\ndfc5 = df.copy().reset_index(drop=True) #df[['LT','avg_price','distance','LB','Listrik','price']]\n\n\ndfc5.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 13428 entries, 0 to 13427\nData columns (total 9 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   price      13428 non-null  int64  \n 1   komplek    13428 non-null  int64  \n 2   Listrik    13428 non-null  int64  \n 3   Lantai     13428 non-null  int64  \n 4   Kondisi    13428 non-null  int64  \n 5   LB         13428 non-null  int64  \n 6   LT         13428 non-null  int64  \n 7   TK         13428 non-null  int64  \n 8   avg_price  13428 non-null  float64\ndtypes: float64(1), int64(8)\nmemory usage: 944.3 KB\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(dfc5.drop(columns='price'), dfc5.price.to_numpy(), test_size = 0.2, random_state=129)\nX_train = X_train.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)\n\n\nrandomforest = RandomForestRegressor(random_state=42)\n\n\nparam_grid = {\n    'n_estimators': [600],\n    'max_depth': [8, 7],\n    'min_samples_split': [10, 50, 100],\n}\n\n\ngrid_search = GridSearchCV(estimator=randomforest, param_grid=param_grid, cv=5, n_jobs=-1)\n\n\ngrid_search.fit(X_train, y_train)\n\nGridSearchCV(cv=5, estimator=RandomForestRegressor(random_state=42), n_jobs=-1,\n             param_grid={'max_depth': [8, 7],\n                         'min_samples_split': [10, 50, 100],\n                         'n_estimators': [600]})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5, estimator=RandomForestRegressor(random_state=42), n_jobs=-1,\n             param_grid={'max_depth': [8, 7],\n                         'min_samples_split': [10, 50, 100],\n                         'n_estimators': [600]})estimator: RandomForestRegressorRandomForestRegressor(random_state=42)RandomForestRegressorRandomForestRegressor(random_state=42)\n\n\n\nbest_params_rf = grid_search.best_params_\n\n\nbest_params_rf\n\n{'max_depth': 8, 'min_samples_split': 10, 'n_estimators': 600}\n\n\n\nrf = RandomForestRegressor(**best_params_rf, random_state=42)\n\n\nrf.fit(X_train, y_train)\n\nRandomForestRegressor(max_depth=8, min_samples_split=10, n_estimators=600,\n                      random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressorRandomForestRegressor(max_depth=8, min_samples_split=10, n_estimators=600,\n                      random_state=42)\n\n\n\ny_pred_test = rf.predict(X_test)\ny_pred_train = rf.predict(X_train)\n\n\nresult = pd.DataFrame()\nmae_test = mean_absolute_error(y_test, y_pred_test)\nmape_test = mean_absolute_percentage_error(y_test, y_pred_test)\nrmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\nr2_test = r2_score(y_test, y_pred_test)\n\nmae_train = mean_absolute_error(y_train, y_pred_train)\nmape_train = mean_absolute_percentage_error(y_train, y_pred_train)\nrmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\nr2_train = r2_score(y_train, y_pred_train)\n\nresult['test_RF'] = [mae_test, mape_test, rmse_test, r2_test]\nresult['train_RF'] = [mae_train, mape_train, rmse_train, r2_train]\nresult.index = ['mae','mape','rmse','r2']\n\n\nresult\n\n\n\n\n\n\n\n\ntest_RF\ntrain_RF\n\n\n\n\nmae\n438.359894\n399.306010\n\n\nmape\n0.216506\n0.243069\n\n\nrmse\n687.059730\n599.451504\n\n\nr2\n0.850694\n0.886136\n\n\n\n\n\n\n\n\nThe result is more or less the same with XGBoost model.\n\n\nfinal_result = pd.concat([final_result, result], axis = 1)"
  },
  {
    "objectID": "ipynb/houses/house_tangerang.html#modelling-combination-6",
    "href": "ipynb/houses/house_tangerang.html#modelling-combination-6",
    "title": "Tangerang house price prediction",
    "section": "",
    "text": "For this last combination, let’s see how AdaBoost perform.\n\ndfc6 = df.copy().reset_index(drop=True) #df[['LT','avg_price','distance','LB','Listrik','price']]\n\n\ndfc6.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 13428 entries, 0 to 13427\nData columns (total 9 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   price      13428 non-null  int64  \n 1   komplek    13428 non-null  int64  \n 2   Listrik    13428 non-null  int64  \n 3   Lantai     13428 non-null  int64  \n 4   Kondisi    13428 non-null  int64  \n 5   LB         13428 non-null  int64  \n 6   LT         13428 non-null  int64  \n 7   TK         13428 non-null  int64  \n 8   avg_price  13428 non-null  float64\ndtypes: float64(1), int64(8)\nmemory usage: 944.3 KB\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(dfc5.drop(columns='price'), dfc5.price.to_numpy(), test_size = 0.2, random_state=129)\nX_train = X_train.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)\n\n\nadaboost = AdaBoostRegressor(random_state = 42)\n\n\nparam_grid = {\n    'n_estimators': [50, 150, 250],\n    'learning_rate': [0.01, 0.1, 1, 0.001]\n}\n\n\ngrid_search = GridSearchCV(estimator=adaboost, param_grid=param_grid, cv=5, n_jobs=-1)\n\n\ngrid_search.fit(X_train, y_train)\n\nGridSearchCV(cv=5, estimator=AdaBoostRegressor(random_state=42), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 1, 0.001],\n                         'n_estimators': [50, 150, 250]})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5, estimator=AdaBoostRegressor(random_state=42), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 1, 0.001],\n                         'n_estimators': [50, 150, 250]})estimator: AdaBoostRegressorAdaBoostRegressor(random_state=42)AdaBoostRegressorAdaBoostRegressor(random_state=42)\n\n\n\nbest_params_ada = grid_search.best_params_\n\n\nbest_params_ada\n\n{'learning_rate': 0.1, 'n_estimators': 50}\n\n\n\nada = AdaBoostRegressor(**best_params_ada, random_state=42)\n\n\nada.fit(X_train, y_train)\n\nAdaBoostRegressor(learning_rate=0.1, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.AdaBoostRegressorAdaBoostRegressor(learning_rate=0.1, random_state=42)\n\n\n\ny_pred_test = ada.predict(X_test)\ny_pred_train = ada.predict(X_train)\n\n\nresult = pd.DataFrame()\nmae_test = mean_absolute_error(y_test, y_pred_test)\nmape_test = mean_absolute_percentage_error(y_test, y_pred_test)\nrmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\nr2_test = r2_score(y_test, y_pred_test)\n\nmae_train = mean_absolute_error(y_train, y_pred_train)\nmape_train = mean_absolute_percentage_error(y_train, y_pred_train)\nrmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\nr2_train = r2_score(y_train, y_pred_train)\n\nresult['test_ADA'] = [mae_test, mape_test, rmse_test, r2_test]\nresult['train_ADA'] = [mae_train, mape_train, rmse_train, r2_train]\nresult.index = ['mae','mape','rmse','r2']\n\n\nresult\n\n\n\n\n\n\n\n\ntest_ADA\ntrain_ADA\n\n\n\n\nmae\n574.532983\n580.901639\n\n\nmape\n0.329233\n0.398783\n\n\nrmse\n838.365578\n816.508925\n\n\nr2\n0.777692\n0.788748\n\n\n\n\n\n\n\n\nfinal_result = pd.concat([final_result, result], axis = 1)"
  },
  {
    "objectID": "ipynb/houses/house_tangerang.html#choosing-the-best-combination",
    "href": "ipynb/houses/house_tangerang.html#choosing-the-best-combination",
    "title": "Tangerang house price prediction",
    "section": "",
    "text": "Let’s use the best combination\n\nfinal_result\n\n\n\n\n\n\n\n\ntest_OLS\ntrain_OLS\ntest_PolyRidgeOLS\ntrain_PolyRidgeOLS\ntest_LogOLS\ntrain_LogOLS\ntest_XGB\ntrain_XGB\ntest_RF\ntrain_RF\ntest_ADA\ntrain_ADA\n\n\n\n\nmae\n532.715149\n541.468771\n471.090238\n478.917020\n583.179251\n595.413840\n438.290716\n419.337764\n438.359894\n399.306010\n574.532983\n580.901639\n\n\nmape\n0.278840\n0.329897\n0.227019\n0.276395\n0.329967\n0.394927\n0.213093\n0.253128\n0.216506\n0.243069\n0.329233\n0.398783\n\n\nrmse\n800.262693\n805.231411\n729.230240\n732.438213\n835.080845\n841.121663\n678.516114\n626.639712\n687.059730\n599.451504\n838.365578\n816.508925\n\n\nr2\n0.797440\n0.794544\n0.831804\n0.830011\n0.779431\n0.775821\n0.854384\n0.875573\n0.850694\n0.886136\n0.777692\n0.788748\n\n\n\n\n\n\n\n\nBy seeing the results, 2 best models are XGBoost and RandomForrest. I’ll go with XGBoost because it way more less computational than RandomForest\n\n\n#choosing only the most important features for XGB\ndf = df[['LT','LB','Listrik','price','avg_price']]\n\nX_train, X_test, y_train, y_test = train_test_split(df.drop(columns='price'), df.price.to_numpy(), test_size = 0.2, random_state=123)\nX_train = X_train.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)\n\n\nbest_xgb_regressor = XGBRegressor(**best_params_xgb, random_state=42)\n\n\nbest_xgb_regressor.fit(X_train, y_train)\n\nXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric='rmse', feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=4, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=1000, n_jobs=None,\n             num_parallel_tree=None, random_state=42, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric='rmse', feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=4, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=1000, n_jobs=None,\n             num_parallel_tree=None, random_state=42, ...)\n\n\n\ny_pred_test = best_xgb_regressor.predict(X_test)\ny_pred_train = best_xgb_regressor.predict(X_train)\n\n\nresult = pd.DataFrame()\nmae_test = mean_absolute_error(y_test, y_pred_test)\nmape_test = mean_absolute_percentage_error(y_test, y_pred_test)\nrmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\nr2_test = r2_score(y_test, y_pred_test)\n\nmae_train = mean_absolute_error(y_train, y_pred_train)\nmape_train = mean_absolute_percentage_error(y_train, y_pred_train)\nrmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\nr2_train = r2_score(y_train, y_pred_train)\n\nresult['test_XGB'] = [mae_test, mape_test, rmse_test, r2_test]\nresult['train_XGB'] = [mae_train, mape_train, rmse_train, r2_train]\nresult.index = ['mae','mape','rmse','r2']\n\n\nresult\n\n\n\n\n\n\n\n\ntest_XGB\ntrain_XGB\n\n\n\n\nmae\n459.897612\n429.131317\n\n\nmape\n0.307846\n0.244483\n\n\nrmse\n699.862902\n643.969481\n\n\nr2\n0.846041\n0.868387\n\n\n\n\n\n\n\n\nThe results remain favorable and consistent"
  },
  {
    "objectID": "ipynb/houses/house_tangerang.html#model-evaluation-interpretation",
    "href": "ipynb/houses/house_tangerang.html#model-evaluation-interpretation",
    "title": "Tangerang house price prediction",
    "section": "",
    "text": "Here, I will assess and interpret the model’s performance using a new set of sample data that I collected separately from the main dataset.\n\ntest = pd.read_csv('test.csv')\ntest = pd.merge(test, avg , on=['Kecamatan','Kondisi']).convert_dtypes()\n\n\nAdd the avg_price column to test data.\n\n\ntest = test[(test.price &lt; 10000) & (test.price &gt; 100)]\ntest = test[test.LB &lt; 900]\ntest = test[test.LT &lt; 500]\ntest = test[['LT','avg_price','LB','Listrik','price']]\n\n\nClean the test data\n\n\ntest_x = test[['LT','LB','Listrik','avg_price']]\ntest_y = test.price\n\n\ny_pred_test = best_xgb_regressor.predict(test_x)\ny_test = test_y\n\n\nresult = pd.DataFrame()\nmae_test = mean_absolute_error(y_test, y_pred_test)\nmape_test = mean_absolute_percentage_error(y_test, y_pred_test)\nrmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\nr2_test = r2_score(y_test, y_pred_test)\n\nmae_train = mean_absolute_error(y_train, y_pred_train)\nmape_train = mean_absolute_percentage_error(y_train, y_pred_train)\nrmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\nr2_train = r2_score(y_train, y_pred_train)\n\nresult['test_XGB'] = [mae_test, mape_test, rmse_test, r2_test]\nresult['train_XGB'] = [mae_train, mape_train, rmse_train, r2_train]\nresult.index = ['mae','mape','rmse','r2']\n\n\nresult\n\n\n\n\n\n\n\n\ntest_XGB\ntrain_XGB\n\n\n\n\nmae\n531.780306\n429.131317\n\n\nmape\n0.214570\n0.244483\n\n\nrmse\n885.063409\n643.969481\n\n\nr2\n0.785323\n0.868387\n\n\n\n\n\n\n\n\nThe model perform worse on scrapped test data.\n\n\n\n\nres = pd.DataFrame([list(y_train), list(y_pred_train)]).transpose()\nres.columns = ['test','pred']\nres['residual'] = res.test - res.pred\nres['residualp'] = abs(res.test - res.pred) * 100 / res.test\n\n\nfig = px.scatter(x=res['test'], y=res['residualp'])\nfig.update_layout( \n    height = 800,\n    title = 'Residual\\'s absolute percentage plot'\n)\nfig.add_hline(y=res['residualp'].mean(), line_dash=\"dot\", line_color=\"red\")\nfig\n\n\n                                                \n\n\n\nFrom the residual percentage above we can see that : 1. 7.5% of data is APE that is more than 50%. 2. 59% of data is APE that is lower than 20%. 3. 32.5% of data is ape that is 20% - 50%. 4. When the houses prices is 4 million, the model become worse as the price kept going up. 5. There are extreme APE error on 0 to 2k prices, this indicate that there are anomalies data inserted.\n\n\nfig = px.scatter(x=res['test'], y=res['residual'])\nfig.update_layout(  \n    height = 700,\n    title = 'Residuals plot'\n)\nfig.add_hline(y=0, line_dash=\"dot\", line_color=\"red\")\nfig\n\n\n                                                \n\n\n\nThe unequal variance in the data implies the presence of heteroscedasticity, and a higher price is associated with increased variability. This phenomenon is considered normal since more expensive houses involve a greater number of factors in determining their prices. Additionally, the model also consistently underpredicts as house prices increase. I think we need to gather more variables or predictors that influence house prices.\n\n\n\n\n\ndef pdp_ice_plot(model, df_test, column, clusters=True):\n    df_test = df_test.copy()\n    pdp_isolate = pdp.PDPIsolate(model = model, df = df_test, \n                      num_grid_points=20,\n                      model_features = df_test.columns, \n                      feature = column, feature_name=column, n_classes=0)\n    pdp_isolate.plot(\n                center=True, plot_lines=True,\n                cluster=clusters, n_cluster_centers=5,cluster_method='accurate',\n                plot_pts_dist=False, to_bins=True,\n                show_percentile=False, which_classes=None,\n                figsize=(10,6), dpi=300,\n                ncols=2, plot_params={\"pdp_hl\": True},\n                engine='matplotlib', template='plotly_white')\n    \n    plt.show()\n    return pdp_isolate\n\n\npdp_ice_plot(best_xgb_regressor, test_x, 'LT', clusters=False)\n\nobtain pred_func from the provided model.\n\n\n\n\n\nfindfont: Font family 'Arial' not found.\nfindfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\nfindfont: Font family 'Arial' not found.\nfindfont: Font family 'Arial' not found.\nfindfont: Font family 'Arial' not found.\nfindfont: Font family 'Arial' not found.\n\n\n\n\n\n&lt;pdpbox.pdp.PDPIsolate at 0x7f92d07a8a30&gt;\n\n\n\nThe Larger the LT, higher the price. But as the LT get higher, the price is also more disperse.\n\n\ntest1 = test_x.copy()\npdp_interact = pdp.PDPInteract(model=best_xgb_regressor, df=test1,\n                              num_grid_points=10,\n                              model_features = test1.columns, \n                              features=['LB','avg_price'], \n                              feature_names=['LB','avg_price'], n_classes=0)\n\nobtain pred_func from the provided model.\nobtain pred_func from the provided model.\nobtain pred_func from the provided model.\n\n\n\n\n\n\n\n\n\n\n\n\npdp_ice_plot(best_xgb_regressor, test_x, 'avg_price', clusters=True)\n\nobtain pred_func from the provided model.\n\n\n\n\n\nfindfont: Font family 'Arial' not found.\nfindfont: Font family 'Arial' not found.\nfindfont: Font family 'Arial' not found.\nfindfont: Font family 'Arial' not found.\nfindfont: Font family 'Arial' not found.\n\n\n\n\n\n&lt;pdpbox.pdp.PDPIsolate at 0x7f92c44449d0&gt;\n\n\n\nThe higher the avg_price, also the higher the price, but not all data is affected the same, some are highly affected, some are less affected.\n\n\nfig, axes = pdp_interact.plot(\n    plot_type=\"grid\",\n    to_bins=True,\n    plot_pdp=True,\n    show_percentile=True,\n    which_classes=None,\n    figsize=None,\n    dpi=300,\n    ncols=2,\n    plot_params=None,\n    engine=\"plotly\",\n    template=\"plotly_white\",\n)\nfig\n\n\n                                                \n\n\n\nBy combining the avg_price and the LT, you can see that low LT is not affected as much as it affect high LT. The price is more volatile on higher LT."
  },
  {
    "objectID": "ipynb/houses/house_tangerang.html#conclusion",
    "href": "ipynb/houses/house_tangerang.html#conclusion",
    "title": "Tangerang house price prediction",
    "section": "",
    "text": "I have concerns regarding the model’s accuracy, primarily because the residuals are still to be relatively high. This issue can be attributed to the limited sample size and the exclusion of critical factors influencing prices. Among these factors, ‘avg_price’ is of utmost importance. To enhance the accuracy of ‘avg_price,’ it’s imperative to expand the sample size and consider a broader spectrum of variables affecting prices. While I’ve used ‘Kecamatan’ and ‘Kondisi Perabotan’ in this context to estimate ‘avg_price,’ I believe these variables alone are insufficient. Moving forward, I plan to further develop this project by gathering additional data from various real estate websites to create a more robust and accurate model."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Vertikal Willis",
    "section": "",
    "text": "Hi! Welcome to my blog!\n\n\n\n\n\nEverything you need to know about me.\n\n\nABOUT\n\n\n\n\n\n\n\n\nAll of my data science projects and portfolios.\n\n\nPORTFOLIOS\n\n\n\n\n\n\n\n\nTalks about things related to data science.\n\n\nTALKS"
  },
  {
    "objectID": "talks/errors_type/index.html",
    "href": "talks/errors_type/index.html",
    "title": "The importance of types of errors",
    "section": "",
    "text": "When dealing with imbalanced datasets, if you don’t include the classification costs of type 1 and type 2 errors you are most likely will get sub-optimal result. Because usually in imbalanced datasets the positive class is more important than the negative class (type 2 error is more severe than type 1 error) and by default all machine learning models are treating both positive class and negative class equally. In this post, I will explained how important to determine the cost of type 1 and type 2 error and tuned your model based on it.\n\nIntroduction of Type 1 and Type 2 errors\n\n\nWe’re advised against relying on accuracy when dealing with imbalanced datasets. This is because the model mostly predicts the more common class and can give a misleading result. However, if we think about it logically, we might not be too concerned about which class the model prefers. What really matters is that the model minimizes its error predictions as much as possible or gets the highest number of accuracy, right? We’re also told to use techniques like undersampling or oversampling to balance the data so that the model can treat both classes more equally. So, what’s the actual reason for doing all of this? The answer is because of the differences in the costs/effects between Type 1 and Type 2 errors.\nType 1 error, commonly known as false positive, is a situation where the model predicts a positive output, but in reality, it’s negative (for example: the model predicts someone has COVID, but in truth, that person doesn’t have COVID).\nType 2 error, often referred to as false negative, is a situation where the model predicts a negative output, but in reality, it’s positive (for example: the model predicts someone doesn’t have COVID, but in truth, that person has COVID).\nIf the consequences of Type 1 and Type 2 errors are the same (equal misclassification cost) in severe imbalanced dataset, then high accuracy may indicate a good model, and there’s no need to go through the trouble of using under/oversampling techniques because the objective is to minimize both False Positives (FP) and False Negatives (FN) as much as possible. But if it is the case then it’s also no use to build a predictive model, just predict everything is negative class.\nHowever, in real-life scenarios, in most cases of imbalanced datasets, Type 1 and Type 2 errors have different consequences where Type 2 error’s impact is always more severe than Type 1 error (for instance, in COVID cases). If Type 2 error is more critical than Type 1 error, then the positive class becomes more important than the negative class. As a result, we cannot rely on accuracy because accuracy evaluates the model’s performance as a whole, whereas we want to assess the model’s performance specifically on the positive class. Some metrics used to evaluate the model’s performance on the positive class are recall and precision.\n \nLooking at the formulas for recall and precision above, you can see that both of them share the same numerator, “True positive.” This is because the main goal of these metrics is to specifically measure the model’s performance in detecting the positive class.\nRecall / Sensitivity : How sensitive is the model in capturing/predicting the positive class. Precision : How accurate are the positive predictions made by a model.\nThe relationship between Recall and Precision is a trade-off, meaning that if you want the model to be more inclined to detect the positive class (higher recall), the model will also make more mistakes in predicting the positive class (lower precision), and vice versa.\n\nWhy SMOTE and other sampling techniques actually only solve half of your imbalanced classification problem\n\n\nAs a recap, we now understand that in the case of an imbalanced dataset, the positive class is more important than the negative class because Type 2 error is more severe than Type 1 error.\nWith an imbalanced dataset, the model tends to predict the majority class, which is often referred to as the negative class. When the dataset is balanced using sampling techniques, the model doesn’t lean towards either class (balanced). However, remember that our goal is to make the model more inclined towards the positive class, not the negative class or treating both classes equally. This is why balancing the dataset doesn’t completely solve the problem but it does help.\n\nDon’t use default decision threshold\n\n\nMost classification models such as decision trees, random forests, AdaBoost, and gradient boosting don’t directly output results as classes, but rather as probabilities. The decision threshold is commonly set at 0.5 or 50%, which means that if the output is 60%, the result is the positive class, and if the output is 45%, the result is the negative class, and so on. Generally, in the case of imbalanced datasets, we aim to achieve a higher recall while sacrificing some precision (because the positive class is more important than the negative class). Take a look at the example below\n\n \n\nHere I use a simple data, where the y-axis is the predictor (feature) and the x-axis is the model output probability. \n\n \n\nThe red line marks the decision threshold, and the blue area shows where the model predicts the positive class. Notice that as we lower the decision threshold, the model starts predicting the positive class more often. And if you set the threshold to 0, the model will predict everything a positive class. In other words, reducing the decision threshold leads to an increase in recall while also causing a decrease in precision.\nIn imbalanced datasets, it’s understood that a lower threshold is often needed. However, the question remains: What is the ideal threshold?\n\nDetermining the best threshold\n\n\nTo determine the best threshold or the most optimal balance between recall and precision, first you must define the ratio between FP and FN. How severe are the consequences of FN compared to FP? Is mistakenly detecting someone without COVID who actually has it worse than detecting 10 people with COVID who are actually COVID-free?  So, it’s not enough to just establish which is worse between FP and FN, we must also decide on their cost/consequences ratio. In this example, I’ll assume an FN:FP ratio of 3:1, meaning the consequences of false negatives are three times more severe than false positives.  Let’s calculate the misclassification costs on all threshold : \n\nWrong classification cost = False Negative * 3 + False positive\n\n\n \n\nThe threshold with the lowest misclassification costs is the best threshold which 0.1 and 0.2 in this case."
  },
  {
    "objectID": "talks/residuals_plot/index.html",
    "href": "talks/residuals_plot/index.html",
    "title": "Evaluate your regression model with residuals plot",
    "section": "",
    "text": "In this instance, I’d like to discuss regression model evaluation. We typically use metrics like MAE, MSE, MAPE, and RMSE to evaluate models. There’s nothing wrong with that, but these metrics can’t provide a detailed evaluation of the model and because this metric use ‘mean’ as aggregation method, it can easily biased (by outliers or by anomalies). it’s important to complement these metrics with other evaluation techniques, like residual plots, to get a more comprehensive and nuanced understanding of how well the model is performing.\nLet’s assume that we build a polynomial regression model with a degree of freedom of 2, and we are attempting to evaluate this model on test data. The results obtained are as follows:\nR² = 88.8%  Mae = 396.735  Mape = 17.9%  Rmse = 591.20 \nAnd the distribution of the ground truth values for y is:\n \nBased on the above-mentioned evaluation metrics, an R² value of 88.8% is considered good. The values for MAE and MAPE are also relatively favorable. However, the slightly high RMSE indicates that there are some large residual values, which are influencing the RMSE.\nNow, let’s create a residual absolute percentage plot. The y-axis represents the absolute percentage error (error based on percentage), and the x-axis represents the ground truth values.\n \nFrom the plot above, you can see that there is a significant percentage error, but this error is only present for x &lt; 4k, with the majority of it occurring for x &lt; 2k. This suggests the presence of data anomalies but further investigation is needed to make sure it is really anomaly.\nNow, let’s zoom in on the plot up to a 40% percentage error limit.\n \nBased on the plot above, the calculation of the error frequencies is as follows:\n0 - 5% = 22.8% of data  5 - 10% = 20.13% of data  10 - 15% = 16.19% of data  15 - 20% = 13.14% of data  20 - 30% = 18.65% of data  30 - 40% = 9.01% of data \nNow, the main question is, what is the maximum acceptable percentage error limit? The answer to this question depends on each specific case. Let’s assume that the maximum acceptable error limit is below 15%. This means that the model can only meet our expectations to the extent of 59.12% (22.8% + 20.13% + 16.19%), while the remaining 41% represents predictions with errors exceeding 15%. In my opinion, using evaluation methods like this is better than solely relying on MAPE.\nNext, let’s evaluate the residual plot using the actual residual values.\n \nBy looking at the plot above, we can see that the residuals have unequal variance (heteroscedasticity). Additionally, there is a pattern where, as x increases, the model becomes more consistent in underpredicting (as indicated by the red line). These two signs indicate that the model is not performing well.\nThe example I provided above is a simple illustration of how a residual plot can help us to do better model evaluation. Thanks and have a good day!"
  },
  {
    "objectID": "talks/missing_values/index.html",
    "href": "talks/missing_values/index.html",
    "title": "Missing values handling",
    "section": "",
    "text": "On this occasion, I would like to discuss missing values. There are various methods for imputing missing values, such as using descriptive statistics like the mean, median, mode, and employing techniques like the KNN Imputer. However, it is essential to first gain a deep understanding of the domain-specific context and the data itself. Before imputing missing values, we must first classify them into three categories, which are:\n\nMissing Completely At Random (MCAR)\nThis means that missing values are truly missing without any discernible pattern or relation to other variables or factors. They are simply missing. For example, when people do not input certain values in a form due to carelessness or negligence. This type of missing values should not be many.\n\n\nMissing At Random (MAR)\nThis means that the missing values are not entirely missing. First, it could be because the values are 0 or None. Second, they may have a relationship with or can be inferred from other columns or variables. For example, when people sell an iPhone online but don’t fill in the ‘Operating system’ field because iPhones always run on iOS and can never be Android. A second example would be when people sell their houses online but don’t specify the number of floors because it can be visually observed in the photos.\n\n\nMissing Not At Random (MNAR)\nThis means that the missing values are not related to other variables within the dataset but are instead associated with external factors. For example, individuals with significant debt may choose not to tell it. Imputing this type of missing values is more challenging due to the influence of external factors.\nAfter classifying them, let’s proceed to the practical demonstration of how to impute missing values.\n\n\nExample 1\nLet’s say you want to retrieve employee information from the servers to build a salary prediction model.\n\n\n\n\n\n\n\n\n\nName\nAge\nPosition\nSalary\n\n\n\n\nSarah Connor\n35\nAccounting\n50k\n\n\nJohn Connor\n32\nMarketing\n35k\n\n\nKyle Reese\n28\nIT\n45k\n\n\nVertikal\n\nIT\n45k\n\n\n\nYou can see that there’s a missing value in the ‘Age’ column under the ‘vertikal’ name. Since this data is derived from employee information stored on the server, it indicates that the missing value must be MCAR and not MAR or MNAR. Let’s take into consideration that this missing value is relatively small, so an appropriate imputation method would be to use the mean or median of the ‘Age’ column. If the missing values are high, then it’s better to report it to database administrator because by context, ‘Age’ has no relation to other columns and thus only can be imputed by itself.\n\n\nExample 2\nImagine you are conducting data collection on an online vehicle listing website.\n\n\n\n\n\n\n\n\n\n\nBrand\nType\nTransmission\nColor\nPrice\n\n\n\n\nToyota\nInnova\nManual\nWhite\n200Jt\n\n\nHonda\nBrio\nAutomatic\nOrange\n125Jt\n\n\nHonda\nBrio\nManual\nBlack\n110Jt\n\n\nSuzuki\nErtiga\nManual\nRed\n140Jt\n\n\nSuzuki\nErtiga\n\nWhite\n142Jt\n\n\nSuzuki\nErtiga\nAutomatic\nBlack\n165Jt\n\n\n\nThere’s a missing value in the ‘Transmission’ column, and upon investigation, it’s apparent that users mentioned the transmission type in the title rather than in the ‘Vehicle Transmission Type’ field. In this case, the missing value is of type MAR. Three possible approaches to impute this missing value are: directly inspecting the ad, re-scraping the data to extract ‘title’ information and apply text mining, or using a KNN Imputer based on the contextual knowledge that the same vehicle with automatic transmission is typically more expensive than manual transmission. The third method, KNN Imputer, is preferable as it take less time and resources consuming compared to other methods.\n\n\nExample 3\nLet’s say you’re thinking about surveying gym-goers for some fitness insights.\n\n\n\n\n\n\n\n\n\nGender\nWeight\nAge\nMembership\n\n\n\n\nMale\n95Kg\n25\n6M\n\n\nFemale\n\n23\n1Y\n\n\nMale\n85Kg\n35\n6M\n\n\nMale\n80Kg\n40\n2Y\n\n\nFemale\n\n28\n6M\n\n\nMale\n102Kg\n38\n3M\n\n\nFemale\n55Kg\n41\n1Y\n\n\n\nThere’s a missing column in the ‘Weight’ column, and initially, you might think that these missing values are MCAR. However, you come to realize that the missing values mostly occur for individuals with a gender of ‘Female.’ This could be due to that females may be more reluctant to disclose their weight when they are overweight. Based on this pattern, you can consider these missing values as type MNAR. If there are only a few missing values, you can impute the weight for females by using the ‘max’ value for weight among females. However, if the missing values are more extensive, it’s essential to validate the assumption first before proceeding with imputation.\n\n\nExample 4\nLet’s imagine you want to build a machine learning model to predict whether an email is spam or not.\n\n\n\n\n\n\n\n\n\nDomain\nWords_count\nFrom\nSpam\n\n\n\n\nPublic\n235\nIND\nYes\n\n\nPublic\n238\nUSA\nYes\n\n\nPublic\n255\nIND\nYes\n\n\nPublic\n267\nDMK\nNo\n\n\nPublic\n310\nEUR\nNo\n\n\n\n320\nAUS\nNo\n\n\n\nLet’s consider that you have 10,000 rows of data above, and 98% of the data falls within the public domain. In such cases, you don’t need to impute the missing values, as the column has high cardinality, as it is likely to be dropped later in the analysis. Therefore, before considering imputing missing values, choose the features that you want to include in your analysis first to avoid unnecessary effort and redundancy.\n\n\nExample 5\nLook at the dataset below:\n\n\n\n\n\n\n\n\n\nName\nAge\nPosition\nSalary\n\n\n\n\nSarah Connor\n35\nUnknown\n50k\n\n\nJohn Connor\n32\nMarketing\n35k\n\n\nKyle Reese\n28\nUnknown\n45k\n\n\nVertikal\n32\nIT\n45k\n\n\nJohn Cena\n33\nMarketing\n35k\n\n\nTom\n28\nIT\n45k\n\n\n\nYou may not observe any missing values in the data, but take a closer look at the ‘Position’ column with ‘Unknown’ value. It should still be considered a form of missing data. It’s a good reminder that missing values can take various forms and may not always be represented as NULL or NaN. In this case, ‘Unknown’ essentially signifies that the specific position information is not available, making it a type of missing value. It’s crucial to remain vigilant and flexible in identifying and handling missing data in various formats.\n\n\nConclusions\nUnderstanding the data is crucial before you start imputing missing values. Carelessly applying imputation techniques without a clear understanding of the data context can lead to incorrect results, which could have significant consequences."
  },
  {
    "objectID": "portfolios.html",
    "href": "portfolios.html",
    "title": "Vertikal Willis",
    "section": "",
    "text": "Building a data automation pipeline with Apache Airflow\n\n\n\n\n\n\n\ndata-visualization\n\n\nairflow\n\n\npipelines\n\n\n\n\nEnd-to-end Data Automation: Data Scraping, Visualization, and Model Monitoring with Airflow\n\n\n\n\n\n\nOct 31, 2023\n\n\nVertikal\n\n\n\n\n\n\n  \n\n\n\n\nAdvanced Power BI dashboard\n\n\n\n\n\n\n\ndata-visualization\n\n\nPower_BI\n\n\ndashboard\n\n\nDAX\n\n\n\n\nBuild a sales analytics dashboard using Power BI and DAX.\n\n\n\n\n\n\nOct 15, 2023\n\n\nVertikal\n\n\n\n\n\n\n  \n\n\n\n\nTangerang house price prediction\n\n\n\n\n\n\n\nregression\n\n\nmachine-learning\n\n\ndata-science\n\n\nweb-scrap\n\n\n\n\nPredicting houses in Tangerang with more than 40 regions based on a real dataset scraped from one of the largest online house listings in Indonesia.\n\n\n\n\n\n\nOct 10, 2023\n\n\nVertikal\n\n\n\n\n\n\n  \n\n\n\n\nTelco customer churn analysis\n\n\n\n\n\n\n\nclassification\n\n\nmachine-learning\n\n\ndata-science\n\n\n\n\nAdvanced analysis, model building and model deployment on telco customer churn dataset.\n\n\n\n\n\n\nAug 3, 2023\n\n\nVertikal\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "ipynb/telco/final-project-original.html",
    "href": "ipynb/telco/final-project-original.html",
    "title": "Table Of Contents",
    "section": "",
    "text": "• Introduction  • Data Introduction  • Data Preparation (Import libraries, data cleaning & data wrangling) • Basic exploratory data analysis      * Descriptive statistic      * Univariate analysis      * Multivariate analysis  • Deep exploratory data analysis      * Services and internet services analysis      * Monthly charges analysis      * Customer analysis      * Benefits analysis      * Churn analysis  • Modelling     * Features selection and encoding      * Splits data and define custom metrics      * Model building combination 1      * Model building combination 2      * Model building combination 3 \n• Model interpretation     * PDP and ICE plots      * Checking prediction’s confidence using confidence interval"
  },
  {
    "objectID": "ipynb/telco/final-project-original.html#data-preparation-import-libraries-data-cleaning-data-wrangling",
    "href": "ipynb/telco/final-project-original.html#data-preparation-import-libraries-data-cleaning-data-wrangling",
    "title": "Table Of Contents",
    "section": "Data Preparation (Import libraries, data cleaning & data wrangling)",
    "text": "Data Preparation (Import libraries, data cleaning & data wrangling)\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import gaussian_kde\nimport matplotlib as mpl\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.metrics import confusion_matrix\nfrom pdpbox import pdp, info_plots\nmpl.rcParams['figure.dpi'] = 300 #set figure dpi\nsns.set() #set figure styling\n\n\n#dataset link\ngithub = 'https://raw.githubusercontent.com/vertikalwil/Data-Analyst/main/telco.csv'\n\n\n#import dataset\ndf = pd.read_csv('telco.csv')\ndf.head()\n\n\n\n\n\n\n\n\ncustomerID\ngender\nSeniorCitizen\nPartner\nDependents\ntenure\nPhoneService\nMultipleLines\nInternetService\nOnlineSecurity\n...\nTechSupport\nStreamingTV\nStreamingMovies\nContract\nPaperlessBilling\nPaymentMethod\nMonthlyCharges\nTotalCharges\nChurn\nHobby\n\n\n\n\n0\n7590-VHVEG\nFemale\n0\nYes\nNo\n135\nNo\nNo phone service\nDSL\nNo\n...\nNo\nNo\nNo\nMonth-to-month\nYes\nElectronic check\n29.85\n29.85\nNo\nSwimming\n\n\n1\n5575-GNVDE\nMale\n0\nNo\nNo\n34\nYes\nNo\nDSL\nYes\n...\nNo\nNo\nNo\nOne year\nNo\nMailed check\n56.95\n1889.5\nNo\nRunning\n\n\n2\n3668-QPYBK\nMale\n0\nNo\nNo\n140\nYes\nNo\nDSL\nYes\n...\nNo\nNo\nNo\nMonth-to-month\nYes\nMailed check\n53.85\n7560\nYes\nHiking\n\n\n3\n7795-CFOCW\nMale\n0\nNo\nNo\n136\nNo\nNo phone service\nDSL\nYes\n...\nYes\nNo\nNo\nOne year\nNo\nBank transfer (automatic)\n42.45\n1840.75\nNo\nSwimming\n\n\n4\n9237-HQITU\nFemale\n0\nNo\nNo\n2\nYes\nNo\nFiber optic\nNo\n...\nNo\nNo\nNo\nMonth-to-month\nYes\nElectronic check\n70.70\n151.65\nYes\nRunning\n\n\n\n\n5 rows × 22 columns\n\n\n\n\n#take a look at the dataframe\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 7043 entries, 0 to 7042\nData columns (total 22 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   customerID        7043 non-null   object \n 1   gender            7043 non-null   object \n 2   SeniorCitizen     7043 non-null   int64  \n 3   Partner           7043 non-null   object \n 4   Dependents        7043 non-null   object \n 5   tenure            7043 non-null   int64  \n 6   PhoneService      7043 non-null   object \n 7   MultipleLines     7043 non-null   object \n 8   InternetService   7043 non-null   object \n 9   OnlineSecurity    7043 non-null   object \n 10  OnlineBackup      7043 non-null   object \n 11  DeviceProtection  6627 non-null   object \n 12  TechSupport       7043 non-null   object \n 13  StreamingTV       7043 non-null   object \n 14  StreamingMovies   7043 non-null   object \n 15  Contract          6798 non-null   object \n 16  PaperlessBilling  7043 non-null   object \n 17  PaymentMethod     7043 non-null   object \n 18  MonthlyCharges    7043 non-null   float64\n 19  TotalCharges      4859 non-null   object \n 20  Churn             7043 non-null   object \n 21  Hobby             4201 non-null   object \ndtypes: float64(1), int64(2), object(19)\nmemory usage: 1.2+ MB\n\n\n\nThere are 4 columns with missing values which are DeviceProtection, Contract, TotalCharges and Hobby.\n\n\n#checking percentage of missing values\nmissingkolom = ['DeviceProtection','Contract','TotalCharges','Hobby']\nfor x in missingkolom:\n    print(f'Missing value of column {x} (%) : {round(df[x].isna().sum()/len(df) * 100,2)}')\n\nMissing value of column DeviceProtection (%) : 5.91\nMissing value of column Contract (%) : 3.48\nMissing value of column TotalCharges (%) : 31.01\nMissing value of column Hobby (%) : 40.35\n\n\n\n#impute missing values with univariate imputation by value proportion\ndf['DeviceProtection'] = df['DeviceProtection'].fillna(\n                             pd.Series(np.random.choice(['No','Yes','No internet service'], \n                             p = list(df['DeviceProtection'].value_counts(normalize=True)), size=len(df))))\n\ndf['Contract'] = df['Contract'].fillna(\n                     pd.Series(np.random.choice(['Month-to-month','Two year','One year'], \n                     p = list(df['Contract'].value_counts(normalize=True)), size=len(df))))\n\n\nHere I impute DeviceProtection and Contract with univariate imputation by value proportion for the following reasons: 1.The missing values is not that much (&lt;10%). 2.The columns don’t have any relationship with other columns so that multivariate imputation is not possible. 3.Using proportion is more precise in this case rather than use ‘mode’.\n\n\n#delete column Hobby\ndf.drop(columns=['Hobby'],inplace=True)\n\n\nReasons to delete: 1.Missing values is too many. 2.By business context, Hobby doesn’t give enough useful informations. 3.Cannot be imputed by multivariate imputation.\n\n\n#impute TotalCharges from tenure and MonthlyCharges\ndf['TotalCharges'] = df['TotalCharges'].fillna(df['tenure'] * df['MonthlyCharges'])\n\n\nEven this column has so many missing values, I decided to impute it with multivariate imputation because: 1.By business context, TotalCharges is more or less tenure * MonthlyCharges. 2.So even the missing values are high, it can still be imputed with a strong justification.\n\n\n#there's a space in the total charges column.\nfor x in df.TotalCharges:\n    try:\n        float(x)\n    except:\n        print(f'Unable to convert to float with this value : {x}')\n\nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \n\n\n\ndf[df.TotalCharges == ' '].head()\n\n\n\n\n\n\n\n\ncustomerID\ngender\nSeniorCitizen\nPartner\nDependents\ntenure\nPhoneService\nMultipleLines\nInternetService\nOnlineSecurity\n...\nDeviceProtection\nTechSupport\nStreamingTV\nStreamingMovies\nContract\nPaperlessBilling\nPaymentMethod\nMonthlyCharges\nTotalCharges\nChurn\n\n\n\n\n488\n4472-LVYGI\nFemale\n0\nYes\nYes\n0\nNo\nNo phone service\nDSL\nYes\n...\nYes\nYes\nYes\nNo\nMonth-to-month\nYes\nBank transfer (automatic)\n52.55\n\nNo\n\n\n753\n3115-CZMZD\nMale\n0\nNo\nYes\n0\nYes\nNo\nNo\nNo internet service\n...\nNo internet service\nNo internet service\nNo internet service\nNo internet service\nTwo year\nNo\nMailed check\n20.25\n\nNo\n\n\n936\n5709-LVOEQ\nFemale\n0\nYes\nYes\n0\nYes\nNo\nDSL\nYes\n...\nNo\nNo\nYes\nYes\nTwo year\nNo\nMailed check\n80.85\n\nNo\n\n\n1082\n4367-NUYAO\nMale\n0\nYes\nYes\n0\nYes\nYes\nNo\nNo internet service\n...\nYes\nNo internet service\nNo internet service\nNo internet service\nTwo year\nNo\nMailed check\n25.75\n\nNo\n\n\n1334\n1768-ZAIFU\nFemale\n1\nNo\nNo\n0\nNo\nNo phone service\nDSL\nNo\n...\nNo\nNo\nNo\nNo\nMonth-to-month\nYes\nElectronic check\n25.20\n\nYes\n\n\n\n\n5 rows × 21 columns\n\n\n\n\nWhen the tenure value is 0, the TotalCharges value is empty (‘space’). This is because customers who have just joined (less than a month) have not been charged yet, resulting in a TotalCharges value of 0. Since there are only 12 rows with this condition, I will delete them.\n\n\n#drop rows that has empty TotalCharges.\ndf = df.drop(df.index[df.TotalCharges == ' ']).reset_index(drop=True)\n\n\n#check for duplicate data, if True then there's no duplicate.\ndf.customerID.nunique() == len(df) \n\nTrue\n\n\n\n#feature engineered 2 new features for the sake of easier analysis.\ndf['Services'] = df[['PhoneService','InternetService']].apply(\n                     lambda x: 'Both' if list(x).count('No') == 0 else\n                     'Internet Only' if x[0] == 'No' else 'Phone Only', axis=1)\n\ndf['TotalBenefits'] = df.loc[:,'OnlineSecurity':'StreamingMovies']\\\n                          .apply(lambda x: list(x).count('Yes'), axis=1)\n\n\nNew features explanation:  1.Services = Combined values of PhoneService and InternetService. 2.TotalBenefits = Sum of benefits taken on OnlineSecurity until StreamingMovies.\n\n\n#Change values of 1 and 0 to 'Yes' and 'No'\ndf['SeniorCitizen'] = df.SeniorCitizen.apply(lambda x: 'Yes' if x == 1 else 'No')\n\n\nChange numerical value to strings for simpler and consistent analysis.\n\n\n#drop useless column\ndf.drop(columns=['customerID'], inplace=True)\n\n\n#change columns object data type to numerical\ndf.tenure = df.tenure.astype('int64')\ndf.MonthlyCharges = df.MonthlyCharges.astype('float64')\ndf.TotalCharges = df.TotalCharges.astype('float64')\n\n\n#checking values of real totalcharges and calculated totalcharges\ndf['TotalChargesDiff'] = df[['tenure','MonthlyCharges','TotalCharges']].apply(\n                             lambda x: round(abs(1 - (x[0] * x[1] / x[2])) * 100, 3), axis=1)\n\n\nHere, I have created a new column called TotalChargesDiff to check the differences (%) between the actual TotalCharges value (from the dataset) and the calculated TotalCharges value (obtained by multiplying tenure with MonthlyCharges). If the difference is above 40%, I will consider those rows as invalid because the values of tenure and MonthlyCharges cannot be trusted.\n\n\ndf['TotalChargesDiff'].sort_values(ascending=False).head(10)\n\n0       13400.000\n5        1357.404\n18        788.048\n19        330.214\n3         213.633\n128        73.511\n47         72.615\n4631       64.286\n5802       63.380\n20         58.263\nName: TotalChargesDiff, dtype: float64\n\n\n\nYou can observe that some data points have a TotalChargesDiff that reaches hundreds or even thousands.\n\n\n#removing rows that have &gt; 40% TotalChargesDiff.\ndf = df[df.TotalChargesDiff &lt; 40].reset_index(drop=True)\ndf.drop(columns=['TotalChargesDiff'], inplace=True)\n\n\ndef numericategoric(df):\n    num = len(df._get_numeric_data().columns)\n    cat = len(df.columns) - num\n    print(\"TotalNumericalData = \" + str(num))\n    print(\"TotalCategoricalData = \" + str(cat))\n    print(\"Numerical = \" + str(list(df._get_numeric_data().columns )))\n    print(\"Categorical = \" + str(list(df.drop(df._get_numeric_data().columns, axis=1).columns)))\n\n\nnumericategoric(df)\n\nTotalNumericalData = 4\nTotalCategoricalData = 18\nNumerical = ['tenure', 'MonthlyCharges', 'TotalCharges', 'TotalBenefits']\nCategorical = ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'Churn', 'Services']\n\n\n\nShow numerical and categorical columns\n\n\n#assign categorical and numerical columns on different dataframe for easier analysis.\ndfnum = df._get_numeric_data()\ndfcat = df.drop(columns = dfnum.columns)"
  },
  {
    "objectID": "ipynb/telco/final-project-original.html#basic-exploratory-data-analysis.",
    "href": "ipynb/telco/final-project-original.html#basic-exploratory-data-analysis.",
    "title": "Table Of Contents",
    "section": "Basic Exploratory Data Analysis.",
    "text": "Basic Exploratory Data Analysis.\n\n\nDescriptive Statistics\n\n#numerical columns describe.\ndfnum.describe()\n\n\n\n\n\n\n\n\ntenure\nMonthlyCharges\nTotalCharges\nTotalBenefits\n\n\n\n\ncount\n7012.000000\n7012.000000\n7012.000000\n7012.000000\n\n\nmean\n32.506560\n64.732760\n2286.410207\n2.041358\n\n\nstd\n24.564234\n30.109753\n2265.759401\n1.835792\n\n\nmin\n1.000000\n12.000000\n13.500000\n0.000000\n\n\n25%\n9.000000\n35.450000\n402.437500\n0.000000\n\n\n50%\n29.000000\n70.300000\n1397.250000\n2.000000\n\n\n75%\n56.000000\n89.850000\n3784.125000\n3.000000\n\n\nmax\n140.000000\n118.750000\n8684.800000\n6.000000\n\n\n\n\n\n\n\n\n1.All columns seems to have a normal min-max values. Nothing weird here. 2.Average tenure is about 30 months which is pretty low. 3.Average MonthlyCharge is about 65-70 USD which is pretty good. 4.Out of 6 benefits available, the average taken by customer is around 2, which is pretty low.\n\n\ndfcat.describe()\n\n\n\n\n\n\n\n\ngender\nSeniorCitizen\nPartner\nDependents\nPhoneService\nMultipleLines\nInternetService\nOnlineSecurity\nOnlineBackup\nDeviceProtection\nTechSupport\nStreamingTV\nStreamingMovies\nContract\nPaperlessBilling\nPaymentMethod\nChurn\nServices\n\n\n\n\ncount\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n\n\nunique\n2\n2\n2\n2\n2\n3\n3\n3\n3\n3\n3\n3\n3\n3\n2\n4\n2\n3\n\n\ntop\nMale\nNo\nNo\nNo\nYes\nNo\nFiber optic\nNo\nNo\nNo\nNo\nNo\nNo\nMonth-to-month\nYes\nElectronic check\nNo\nBoth\n\n\nfreq\n3542\n5874\n3624\n4920\n6336\n3372\n3087\n3486\n3075\n3072\n3459\n2798\n2769\n3851\n4155\n2354\n5155\n4821\n\n\n\n\n\n\n\n\n1.Male and Female has the same proportion. 2.Most of customers is non SeniorCitizen with no Partner and No Dependents. 3.Favorite InternetService is Fiber optic. 4.Majority of customers is subscribed with ‘Month-to-month’ Contract. 5.5155 out of 7012 is non-Churn customers which make this dataset is imbalanced. 6.All these columns have low cardiality values.\n\n\nfor col in dfcat.columns:\n    print(f'Value counts for column {col}:')\n    print(df[col].value_counts())\n    print('---'*10)\n    print('\\n')\n\nValue counts for column gender:\nMale      3542\nFemale    3470\nName: gender, dtype: int64\n------------------------------\n\n\nValue counts for column SeniorCitizen:\nNo     5874\nYes    1138\nName: SeniorCitizen, dtype: int64\n------------------------------\n\n\nValue counts for column Partner:\nNo     3624\nYes    3388\nName: Partner, dtype: int64\n------------------------------\n\n\nValue counts for column Dependents:\nNo     4920\nYes    2092\nName: Dependents, dtype: int64\n------------------------------\n\n\nValue counts for column PhoneService:\nYes    6336\nNo      676\nName: PhoneService, dtype: int64\n------------------------------\n\n\nValue counts for column MultipleLines:\nNo                  3372\nYes                 2964\nNo phone service     676\nName: MultipleLines, dtype: int64\n------------------------------\n\n\nValue counts for column InternetService:\nFiber optic    3087\nDSL            2410\nNo             1515\nName: InternetService, dtype: int64\n------------------------------\n\n\nValue counts for column OnlineSecurity:\nNo                     3486\nYes                    2011\nNo internet service    1515\nName: OnlineSecurity, dtype: int64\n------------------------------\n\n\nValue counts for column OnlineBackup:\nNo                     3075\nYes                    2422\nNo internet service    1515\nName: OnlineBackup, dtype: int64\n------------------------------\n\n\nValue counts for column DeviceProtection:\nNo                     3072\nYes                    2416\nNo internet service    1524\nName: DeviceProtection, dtype: int64\n------------------------------\n\n\nValue counts for column TechSupport:\nNo                     3459\nYes                    2038\nNo internet service    1515\nName: TechSupport, dtype: int64\n------------------------------\n\n\nValue counts for column StreamingTV:\nNo                     2798\nYes                    2699\nNo internet service    1515\nName: StreamingTV, dtype: int64\n------------------------------\n\n\nValue counts for column StreamingMovies:\nNo                     2769\nYes                    2728\nNo internet service    1515\nName: StreamingMovies, dtype: int64\n------------------------------\n\n\nValue counts for column Contract:\nMonth-to-month    3851\nTwo year          1697\nOne year          1464\nName: Contract, dtype: int64\n------------------------------\n\n\nValue counts for column PaperlessBilling:\nYes    4155\nNo     2857\nName: PaperlessBilling, dtype: int64\n------------------------------\n\n\nValue counts for column PaymentMethod:\nElectronic check             2354\nMailed check                 1599\nBank transfer (automatic)    1539\nCredit card (automatic)      1520\nName: PaymentMethod, dtype: int64\n------------------------------\n\n\nValue counts for column Churn:\nNo     5155\nYes    1857\nName: Churn, dtype: int64\n------------------------------\n\n\nValue counts for column Services:\nBoth             4821\nPhone Only       1515\nInternet Only     676\nName: Services, dtype: int64\n------------------------------\n\n\n\n\n\nShow all value counts for each categorical columns.\n\n\n\nUnivariate Analysis\n\nfig, axarr = plt.subplots(1,4, figsize=(10, 4))\nfor x in dfnum.columns:\n    axarr[dfnum.columns.get_loc(x)].boxplot(df[x],patch_artist=True)\n    axarr[dfnum.columns.get_loc(x)].set_xlabel(x)\nplt.suptitle('Outliers checking on numeric columns')\nfig.tight_layout(pad=1)\nplt.show()\n\n\n\n\n\nWill drop outlier in tenure.\n\n\n#drop outlier in tenure\ndf = df[df.tenure &lt; 125]\n\n\n#plot distribution for numerical columns\nfig, axarr = plt.subplots(1,4, figsize=(10, 4))\nfor x in dfnum.columns:\n    sns.histplot(data=dfnum[x], \n                 color='skyblue', \n                 kde=True, \n                 edgecolor='none', \n                 ax=axarr[dfnum.columns.get_loc(x)])\nplt.suptitle('Distribution plot', weight='bold')\nfig.tight_layout(pad=1)\n\n\n\n\n\nAbove plots are distribution plots on all numerical columns. 1.tenure and MonthlyCharges have a ‘U-shaped’ distribution. 2.TotalCharges has a positive-skew distribution.\n\n\n#count plot for categorical columns\nplt.figure(figsize=(15,12))\n\nfeatures = dfcat.columns\nfor i in np.arange(1, len(features)+1):\n    plt.subplot(5, len(features)//3 - 2, i)\n    sns.countplot(x=df[features[i-1]], color='green')\n    plt.xticks(rotation=10)\n    plt.xlabel(features[i-1])\nplt.suptitle('CountPlot', size=19, weight='bold')\nplt.tight_layout(pad = 1)\n\n\n\n\n\nHere I count plotted all categorical columns.\n\n\n\nMultivariate Analysis\n\n#count plots against 'churn'\nplt.figure(figsize=(15,12))\n\nfeatures = dfcat.columns\nfor i in np.arange(1, len(features)+1):\n    plt.subplot(5, len(features)//3 - 2, i)\n    sns.countplot(data=df, x=df[features[i-1]], hue='Churn')\n    plt.legend(prop={'size': 8})\n    plt.xticks(rotation=10)\n    plt.xlabel(features[i-1])\nplt.suptitle('CountPlot vs Churned', size=19, weight='bold')\nplt.tight_layout(pad = 1)\n\n\n\n\n\nI added Churn count into the categorical plots. 1.You can see for column Gender, the values and Churn count is pretty equal which make this column will have a very low predictive power. 2.For InternetService, fiber optic has way higher in churn probability compare to DSL. 3.Same ways also applied on Month-to-month Contract and Electronic-check PaymentMethod.\n\n\n#distribution plots against 'churn'\nfig, axarr = plt.subplots(1,4, figsize=(10, 4))\nfor x in dfnum.columns:\n    sns.histplot(data=df, \n                 x = dfnum[x],\n                 color='skyblue', \n                 kde=True, \n                 edgecolor='none', \n                 ax=axarr[dfnum.columns.get_loc(x)], \n                 hue='Churn')\n    \nplt.suptitle(\"Distribution plot\", weight='bold')\nfig.tight_layout(pad=1)\n\n\n\n\n\nLet’s also compare Churn distribution on numerical columns. Customers tend to churn when the tenure is low and not churn when the MonthlyCharges is very low. I will do further analysis about these columns later.\n\n\n#change binary column into numerical\ndfcorr = df.copy()\nbinary = ['gender','SeniorCitizen','Partner','Dependents','PhoneService','PaperlessBilling','Churn']\nvalue_mapping = {\n    'No': 0,\n    'Yes' : 1,\n    'Male' : 1,\n    'Female' : 0\n}\n\nfor col in binary:\n    dfcorr[col] = dfcorr[col].map(value_mapping).astype('int64')\n\n\nplt.figure(figsize=(10,8))\nsns.heatmap(dfcorr.corr(), annot=True, fmt='.2f')\nplt.show()\n\n\n\n\n\nCorrelation! 1.TotalCharges and tenure have a high positive correlation (causation : the longer the customers subscribed, the more they paid). 2.TotalBenefits also has a strong correlation with MonthlyCharges and TotalCharges (causation : more benefits taken also make the MonthlyCharges higher). Note : Correlation doesn’t indicate causation. Understanding the specific context, industry knowledge, and conducting further analysis or experiments can help determine if there is a causal relationship between the variables or if other factors are influencing the observed correlations."
  },
  {
    "objectID": "ipynb/telco/final-project-original.html#deep-dive-exploratory-data-analysis",
    "href": "ipynb/telco/final-project-original.html#deep-dive-exploratory-data-analysis",
    "title": "Table Of Contents",
    "section": "Deep-Dive Exploratory Data Analysis",
    "text": "Deep-Dive Exploratory Data Analysis\n\n\n#create a function to plot churn probability for numerical columns.\ndef prob_plot(df,colom,x):\n    means = df[colom].mean()\n    medians = df[colom].median()\n    data = df[df.Churn == 'Yes'][colom].astype('float64')\n    data1 = df[df.Churn == 'No'][colom].astype('float64')\n    \n    kde = gaussian_kde(data)\n    kde1 = gaussian_kde(data1)\n    dist_space = np.linspace( min(data), max(data), 200)\n    dist_space1 = np.linspace( min(data1), max(data1), 200)\n    axarr[x].plot( dist_space, kde(dist_space), label='Churned', color='orange' )\n    axarr[x].plot( dist_space1, kde1(dist_space1), label='Not churn', color='blue')\n    axarr[x].axvline(x = means, linestyle = '--', color='g', label='Mean')\n    axarr[x].axvline(x = medians, linestyle = '--', color='r', label='Median')\n    axarr[x].set_title('Probability', fontweight='bold', size=12)\n    axarr[x].set(ylabel = 'Probability', xlabel = colom)\n    axarr[x].legend()\n\n\nServices & InternetService analysis\n\n#count plot for Services.\nfig, axarr = plt.subplots(1,2, figsize=(12, 6))\nsns.countplot(data=df[df.Services == 'Both'], \n              x='InternetService', \n              ax=axarr[0])\n\nsns.countplot(data=df[df.Services == 'Internet Only'], \n              x='InternetService', \n              ax=axarr[1])\n\naxarr[0].set_title(\"Both phone service & internet service\", weight='bold')\naxarr[1].set_title(\"Internet service only\", weight='bold')\n\nplt.suptitle(\"Comparison of internet services on product services\")\nplt.tight_layout(pad=1)\nplt.show()\n\n\n\n\n\nIt is observed that customers tend to prefer Fiber optic over DSL for ‘phone & internet service’. However, when considering ‘internet service only’ without phone, there is no option for Fiber optic available. This suggests that in order to utilize Fiber optic, a phone connection (or phone service) is required.\n\n\n#plot churn probability for 'services' & 'internet services'\nfig, axarr = plt.subplots(1,2, figsize=(12, 6))\n\ndf.groupby('Services')['Churn'].agg(lambda x: list(x).count('Yes') * 100 / len(list(x)))\\\n                               .plot(kind='bar', width=0.3,rot=True, ax=axarr[0])\n\ndf[df.Services != 'Phone Only'].groupby(['InternetService'])['Churn'].agg(lambda x: list(x).count('Yes') * 100 / len(x))\\\n                               .plot(kind='bar', width=0.3,rot=True, ax=axarr[1])\n\naxarr[0].set_title('Services churn probability', weight='bold')\naxarr[1].set_title('Internet service churn probability', weight='bold')\n\nplt.tight_layout(pad=1)\nplt.show()\n\n\n\n\n\nCustomers that used ‘Both’ services and InternetService fiber optic tends to churn more.\n\n\n\nMonthlyCharges analysis\n\n#checking InternetService price.\ndf_filtered = df[(df.Services == 'Both') & (df.TotalBenefits == 0) & (df.MultipleLines == 'No')].copy()\ndf_filtered.groupby('InternetService')['MonthlyCharges'].mean()\n\nInternetService\nDSL            44.963824\nFiber optic    70.083186\nName: MonthlyCharges, dtype: float64\n\n\n\nThe price of Fiber optic is higher, around 25 USD, compared to DSL. However, it’s important to keep in mind that these prices include a phone service with a single line.\n\n\ndf_filtered = df[(df.Services == 'Phone Only') & (df.TotalBenefits == 0)].copy()\ndf_filtered.groupby('MultipleLines')['MonthlyCharges'].mean()\n\nMultipleLines\nNo     19.953819\nYes    24.973716\nName: MonthlyCharges, dtype: float64\n\n\n\nplt.figure(figsize=(10,5))\nsns.histplot(data=df_filtered, \n             x='MonthlyCharges', \n             hue='MultipleLines', \n             multiple='stack')\n\nplt.title('Phone service distribution', weight='bold')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nWe can see that the price for a ‘phone service’ with a single line is around 20 USD, while the price for a ‘phone service’ with multiple lines is around 25 USD. This also means that the price for DSL is around 25 USD, while the price for Fiber optic is around 50 USD, which is twice as much as DSL.\n\n\nfig, axarr = plt.subplots(1,2, figsize=(12, 6))\nmc_dist = sns.histplot(data=df, \n                       x = 'MonthlyCharges',\n                       hue='Churn', \n                       ax=axarr[0], \n                       multiple='stack')\n\naxarr[0].set_title('Distribution', fontweight='bold', size=12)\n\nprob_plot(df,'MonthlyCharges',1)\naxarr[1].legend(loc='upper right')\nplt.show()\n\n\n\n\n\nAt a MonthlyCharges range of approximately +- 20 USD, the ratio of non-churn customers is very high. It is known that products within this price range are typically ‘phone service only’. However, between the price range of 60 - 100 USD, the churn probability increases significantly. I am planning to conduct further analysis specifically for customers within this price range.\n\n\ndf_filtered = df[(df.InternetService == 'Fiber optic') & (df.Services == 'Both') & (df.MultipleLines == 'No')]\ndf_agg = df_filtered.groupby('TotalBenefits')['MonthlyCharges'].agg('mean').reset_index()\ndf_agg['MonthlyCharges'] = df_agg.MonthlyCharges.round()\n\n\nfig, axarr = plt.subplots(1,2, figsize=(12, 6))\nsns.scatterplot(data=df_filtered, \n                x='MonthlyCharges', \n                y='TotalBenefits', \n                s=35, \n                ax=axarr[0])\n\nsns.barplot(df_agg, x = 'TotalBenefits', y = 'MonthlyCharges')\naxarr[0].set_title('MonthlyCharges vs Totalbenefits', weight='bold')\naxarr[1].set_title('Average MonthlyCharges vs Totalbenefits', weight='bold')\nfig.tight_layout(pad = 1)\nplt.show()\n\n\n\n\n\nHere, you can observe that as more TotalBenefits are taken, the MonthlyCharges also increase. On the left plot, you can see that there are 5 outlier data points, which will be removed later.\n\n\ndf_fil = df[(df.InternetService == 'Fiber optic') & (df.Services == 'Both')\\\n              & (df.MultipleLines == 'No') & (df.TotalBenefits == 1)]\n\nbenefits = ['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']\n\ndf_benefit = pd.DataFrame()\nfor x in benefits:\n    df_value = pd.DataFrame([x, \n                      df_fil[df_fil[x] == 'Yes']['MonthlyCharges'].min(), \n                      df_fil[df_fil[x] == 'Yes']['MonthlyCharges'].max(), \n                      df_fil[df_fil[x] == 'Yes']['MonthlyCharges'].mean(), \n                      df_fil[df_fil[x] == 'Yes']['MonthlyCharges'].median()\n                     ]).transpose()\n    \n    df_benefit = pd.concat([df_benefit, df_value])\ndf_benefit.columns = ['Benefit','MinCharges','MaxCharges','MeanCharges','MedianCharges']    \n\n\ndf_benefit\n\n\n\n\n\n\n\n\nBenefit\nMinCharges\nMaxCharges\nMeanCharges\nMedianCharges\n\n\n\n\n0\nOnlineSecurity\n73.2\n80.3\n74.952857\n75.0\n\n\n0\nOnlineBackup\n72.75\n76.65\n74.708511\n74.65\n\n\n0\nDeviceProtection\n69.55\n76.65\n74.258889\n74.8\n\n\n0\nTechSupport\n73.85\n76.55\n75.045455\n74.7\n\n\n0\nStreamingTV\n77.65\n81.9\n79.746825\n79.75\n\n\n0\nStreamingMovies\n12.0\n86.45\n79.16746\n80.0\n\n\n\n\n\n\n\n\nAbove are the prices of benefits with Fiber optic and a single line phone connection. You can see that StreamingTV and StreamingMovies are more expensive compared to other benefits, approximately +- 5 USD.\n\nNow that we know the prices of every product, here’s a recap:\nDSL = approximately 25 USD. Fiber optic = approximately 50 USD. Phone service (single line) = approximately 20 USD. Phone service (multiple lines) = approximately 25 USD. OnlineSecurity - TechSupport = approximately 5 USD. StreamingTV - StreamingMovies = approximately 10 USD.\nWith this data, we can perform a simple ‘anomaly detection’ by manually calculating the MonthlyCharges and comparing them with the actual MonthlyCharges, similar to how we calculated the TotalChargesDiff above.\n\n#checking for MonthlyCharges values with the calculated one (similar with checking TotalCharges difference).\ndef MonthlyChargesDiff(x):\n    estimation = 0\n    if x['PhoneService'] == 'Yes':\n        estimation += 20\n    if x['MultipleLines'] == 'Yes':\n        estimation += 5\n    if x['InternetService'] == 'DSL':\n        estimation += 25\n    if x['InternetService'] == 'Fiber optic':\n        estimation += 50\n        \n    if (x['StreamingTV'] == 'Yes') & (x['StreamingMovies'] == 'Yes'):\n        estimation += 20 + (x['TotalBenefits'] - 2) * 5\n    elif (x['StreamingTV'] == 'Yes') | (x['StreamingMovies'] == 'Yes'):\n        estimation += 10 + (x['TotalBenefits'] - 1) * 5\n    else:\n        estimation += x['TotalBenefits'] * 5\n        \n    return abs(1 - (estimation / x['MonthlyCharges'])) * 100\n    \n   \n\n\ndf['MonthlyChargesEstimationDifference'] = df.apply(MonthlyChargesDiff, axis=1)\n\n\ndf[df.MonthlyChargesEstimationDifference &gt; 40][['MonthlyCharges','MonthlyChargesEstimationDifference']]\n\n\n\n\n\n\n\n\nMonthlyCharges\nMonthlyChargesEstimationDifference\n\n\n\n\n12\n29.00\n296.551724\n\n\n389\n12.00\n733.333333\n\n\n666\n12.00\n566.666667\n\n\n859\n26.41\n278.644453\n\n\n1439\n18.26\n447.645126\n\n\n2185\n21.63\n362.320851\n\n\n4090\n31.26\n219.897633\n\n\n5848\n15.00\n466.666667\n\n\n6718\n21.00\n304.761905\n\n\n\n\n\n\n\n\nYou can see that there are 9 rows with extreme MonthlyCharges values. These are considered as ‘anomalies’, so let’s remove them.\n\n\n#remove MonthlyCharges extreme values.\ndf = df[df.MonthlyChargesEstimationDifference &lt; 40].reset_index(drop=True)\n\n\n\nCustomer analysis\n\n#creating a function to engineered a new feature.\ndef statuss(x):\n    x = list(x)\n    if (x[0] == 'Yes') & (x[1] == 'Yes'):\n        return 'Both'\n    elif (x[0] == 'Yes') & (x[1] == 'No'):\n        return 'Partner Only'\n    elif (x[0] == 'No') & (x[1] == 'Yes'):\n        return 'Dependent Only'\n    else:\n        return 'Single'\n\n\ndf['Status'] = df[['Partner','Dependents']].apply(statuss, axis=1)\n\n\nI have created a new feature called ‘Status’. This feature is derived from the columns Partner and Dependents. 1.If customers have both Partner and Dependents, it will be labeled as ‘Both’. 2.If customers have Partner but no Dependents, it will be labeled as ‘Partner Only’. 3.If customers have Dependents but no Partner, it will be labeled as ‘Dependent Only’. 4.If customers have neither Partner nor Dependents, it will be labeled as ‘Single’.\n\n\nplt.figure(figsize=(10,5))\nsns.countplot(df.sort_values('Status', ascending=True), x='Status')\nplt.title('Status Count', size=16, weight='bold')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nMajority of customers are Single.\n\n\nfig, axarr = plt.subplots(1,3, figsize=(15, 6))\nk = ['tenure','MonthlyCharges','TotalBenefits']\nfor x in k:\n    sns.barplot(data=df.groupby(['Status'])[[x]].mean().reset_index(), \n                x='Status', \n                y=x, \n                ax=axarr[k.index(x)],\n                palette=['grey', 'g','m','b'])\n    \n    axarr[k.index(x)].set_title(f'{x} average', weight='bold', size=15)\n    \nfig.tight_layout()\nplt.show()\n\n\n\n\n\nCustomers labeled as ‘Partner Only’ are considered the best since they have the longest tenure and the highest MonthlyCharges. The second-best group is ‘Both’, although these customers may not have MonthlyCharges as high as those in the ‘Single’ group, their tenure is almost double that of the ‘Single’ group.\n\n\nplt.figure(figsize=(10,5))\nsns.barplot(data=df.groupby('Status')[['Churn']].agg(lambda x: list(x).count('Yes') / len(x)).reset_index(), \n            x='Status', \n            y='Churn')\n\nplt.title('Churn Probability', weight='bold', size=16)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nSingle customers have the highest churn probability.\n\n\ndf.SeniorCitizen.value_counts(normalize=True)\n\nNo     0.837903\nYes    0.162097\nName: SeniorCitizen, dtype: float64\n\n\n\nThe majority of customers are young people.\n\n\ndf_status = df.groupby('SeniorCitizen')[['Status']]\ndf_status = df_status.agg(Single = ('Status', lambda x: list(x).count('Single') * 100 / len(x)), \n                          PartnerOnly = ('Status', lambda x: list(x).count('Partner Only') * 100  / len(x)), \n                          Both = ('Status', lambda x: list(x).count('Both') * 100  / len(x)), \n                          DependentOnly = ('Status', lambda x: list(x).count('Dependent Only') * 100  / len(x)))\ndf_status = df_status.reset_index().melt(id_vars='SeniorCitizen')\ndf_status = df_status.rename(columns={'variable':'Status'})\n\n\nplt.figure(figsize=(10,5))\n\nsns.barplot(data=df_status, \n            x ='SeniorCitizen', \n            y='value', \n            hue='Status')\n\nplt.title(\"Status comparison between senior citizen\", size=15, weight='bold')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nFor non-Senior citizens, ‘Single’ customers have the highest frequency, followed by ‘Both’. For Senior citizens, ‘Single’ is also the highest category, but the difference with ‘PartnerOnly’ is not as significant. From the plots above, we can also conclude that young people tend to have dependents more than older people.\n\n\ndf_status = df.groupby('Status')[['OnlineSecurity','OnlineBackup',\n                                  'DeviceProtection','TechSupport','StreamingTV','StreamingMovies']]\\\n                                  .agg(lambda x: list(x).count('Yes'))\n    \ndf_status['total'] = df_status.apply('sum',axis=1)\n\nfor x in df_status.drop(columns='total').columns:\n    df_status[x] = (df_status[x] * 100 / df_status.total).round()\ndf_status.drop(columns='total', inplace=True)\n\n\ndf_status.plot(kind='bar', rot=0)\nplt.title('Benefit count comparison between status (%)', size=12, weight='bold')\nplt.legend(bbox_to_anchor=(1, 1))\nplt.show()\n\n\n\n\n\nSingle and PartnerOnly customers tend to prefer entertainment benefits such as StreamingTV and StreamingMovies compared to other customers. Additionally, these customers show a lower preference for using TechSupport and OnlineSecurity.\n\n\n\nBenefits analysis\n\nfig, axarr = plt.subplots(1,2, figsize=(12, 6))\nsns.countplot(df[df.Services != 'Phone Only'], \n              x = 'TotalBenefits', ax=axarr[0])\n\nsns.barplot(df[['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']]\\\n            .apply(lambda x: list(x).count('Yes')).reset_index(), x = 'index', y = 0, ax=axarr[1])\n\naxarr[0].set_title('TotalBenefits Count', size=12, weight='bold')\naxarr[0].set(ylabel=None)\n\naxarr[1].set(ylabel=None)\naxarr[1].set_title('Benefits Count', size=12, weight='bold')\naxarr[1].tick_params(axis='x', rotation=25)\nfig.tight_layout()\nplt.show()\n\n\n\n\n\nThe average number of TotalBenefits taken by customers is around 3, with StreamingTV and StreamingMovies being the most popular choices.\n\n\ndf_total = pd.DataFrame()\nfor x in ['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']:\n    total = list(df[df[x] == 'Yes']['Churn']).count('Yes') / len(df[df[x] == 'Yes']['Churn'])\n    df_total = pd.concat([df_total, pd.DataFrame([x],[total])])\n\n\nfig, axarr = plt.subplots(1,2, figsize=(12, 6))\nsns.barplot(df.groupby('TotalBenefits')[['Churn']]\\\n            .agg(lambda x: list(x).count('Yes') * 100 / len(x)).round().reset_index(), \n            x='TotalBenefits', \n            y='Churn', ax=axarr[0])\n\nsns.barplot(df_total.reset_index(), \n            x=0, \n            y='index', \n            ax=axarr[1])\n\naxarr[0].set_title('TotalBenefits Churn probability', size=12, weight='bold')\naxarr[0].set(ylabel=None)\n\naxarr[1].set(ylabel=None, xlabel='Benefits')\naxarr[1].set_title('Benefits Churn probability', size=12, weight='bold')\naxarr[1].tick_params(axis='x', rotation=25)\nfig.tight_layout()\nplt.show()\n\n\n\n\n\nWhile StreamingTV and StreamingMovies are the most favored choices, the churn probability associated with them is also the highest.\n\n\n\nChurn analysis\n\nfig, axarr = plt.subplots(1, figsize=(10, 6))\nsns.scatterplot(data=df, \n                x = 'MonthlyCharges', \n                y = 'tenure', \n                hue ='Churn', \n                s=20)\n\nplt.fill_between((68 , 97),20, alpha=0.2, color='green')\nplt.fill_between((68 , 97),52.5, 72.5, alpha=0.2, color='blue')\n\nplt.title('MonthlyCharges vs tenure vs Churn', size = 15, weight = 'bold')\nplt.show()\n\n\n\n\n\nHere, I have created two areas, denoted by green and blue, both focusing on MonthlyCharges in the range of 70 - 95 USD. This price range corresponds to the highest churn probability. The green area represents customers with low tenure and is predominantly occupied by churned customers, while the blue area represents customers with high tenure and is predominantly occupied by non-churned customers.\n\n\nplt.subplots(1, figsize=(15, 8))\nsns.scatterplot(data=df, \n                x = 'MonthlyCharges', \n                y = 'tenure', \n                s=35, \n                hue='TotalBenefits', \n                style='InternetService', \n                palette='coolwarm')\n\nplt.fill_between((68 , 97),20, alpha=0.15, color='green')\nplt.fill_between((68 , 97),52.5, 72.5, alpha=0.15, color='blue')\n\nplt.title('MonthlyCharges vs tenure vs TotalBenefits vs InternetService', size = 15, weight = 'bold')\nplt.show()\n\n\n\n\n\nStill on the same plot, I have added TotalBenefits and InternetService. It can be observed that in the green area, Fiber optic is the dominant InternetService with low TotalBenefits. On the other hand, the blue area is dominated by DSL with high TotalBenefits. This indicates that customers, at the same price point, tend to choose DSL with high TotalBenefits rather than Fiber optic with low TotalBenefits. Note that Fiber optic prices are doubled than DSL.\n\nWith the observed pattern above, we can create an important new feature, which we will refer to as ‘FO_LB’ (Fiber optic_Low benefit). I will assign a value of ‘1’ to indicate that the internet service is Fiber optic and the Totalbenefits taken are less than or equal to 3. For other cases, I will assign ‘0’.\n\ndf['FO_LB'] = df[['InternetService','TotalBenefits']].apply(\n    lambda x: 1 if (x['InternetService'] == 'Fiber optic') & (x['TotalBenefits'] &lt;= 3) else 0, axis=1)"
  },
  {
    "objectID": "ipynb/telco/final-project-original.html#modelling",
    "href": "ipynb/telco/final-project-original.html#modelling",
    "title": "Table Of Contents",
    "section": "Modelling",
    "text": "Modelling\n\nOne important thing to address before we proceed is considering the types of errors to make this project as realistic as possible. Typically, there are two types of errors: false positive (FP) and false negative (FN). However, in this project, I will introduce three types of errors.\n1.FP: False positive  2.FN1: False negative for customers with MonthlyCharges below 95 USD  3.FN2: False negative for customers with MonthlyCharges above 95 USD (VIP customers)  Let’s agree on the misclassification ratio, which is FP:FN1:FN2 = 1:3:5 \nIt’s important to note that this dataset is imbalanced, meaning there is a significant difference in the number of samples between the classes.\nBased on these problems, we can set up our model’s parameters as follows: 1.Hyperparameter tuning. 2.Decision threshold tuning. 3.Oversampling data using SMOTE. 4.Applying weights to the models. I will be using Random Forest, XGBoost, and Logistic Regression.\nMetrics: Custom scoring based on sample misclassification. Precision. Recall. F1_score. Once the models are evaluated using these metrics, I will interpret the best model.\n\nFeature selection & encoding\n\ndf1 = df.copy()\n\n\ndf = df1.copy()\n\n\n#drop columns\ndf.drop(columns = ['Services','MonthlyChargesEstimationDifference','Status','PaperlessBilling','PaymentMethod'], inplace=True)\n\n\nI have dropped the columns from ‘Services’ to ‘Status’ as these columns were engineered features created for simpler exploratory data analysis (EDA). Additionally, I have also dropped the ‘PaperlessBilling’ and ‘PaymentMethod’ columns because, in the business context, these columns are considered irrelevant for determining customer churn since they represent optional ‘features’ for customers.\n\n\n#converting 'No internet service' to 'No' in benefit columns.\nKolomBenefit = ['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']\n\nfor x in KolomBenefit:\n    df[x] = df[x].apply(lambda x: 'No' if x == 'No internet service' else x)\n\n\n#converting 'No phone service' to 'No'\ndf['MultipleLines'] = df[x].apply(lambda x: 'No' if x == 'No phone service' else x)\n\n\n#dict to mapping string to numerical.\nvalue_mapping = {\n    'No': 0,\n    'Yes' : 1,\n    'Male' : 1,\n    'Female' : 0\n}\n\n\n#binary encoding\nbinary = list(df.drop(columns=['tenure','InternetService','MonthlyCharges','TotalCharges','TotalBenefits','Contract','FO_LB']).columns)\n\nfor col in binary:\n    df[col] = df[col].map(value_mapping).astype('int64')\n\n\n#label encoding\ndf['Contract'] = df['Contract'].apply(lambda x: 1 if x == 'Month-to-month' else 2 if x == 'One year' else 3) \n\n\n#one hot encoding\ndf = pd.get_dummies(df, columns=['InternetService'])\n\n\n#feature selection\ndf = df[['Contract','tenure','InternetService_Fiber optic','MonthlyCharges','FO_LB','InternetService_No','Churn']]\ndf = df.rename(columns={\n                'InternetService_Fiber optic':'Fiber_optic',\n                'InternetService_No':'No_internet'})\n\n\nHere I only choose a feature that have a strong predictive power (by using feature of importances)\n\n\n\nSplits data and define custom function\n\n#split train-test\nX_train, X_test, y_train, y_test = train_test_split(df.drop(columns='Churn'), df.Churn.to_numpy(), test_size = 0.2, random_state=123)\nX_train = X_train.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)\n\n\n#custom function to do threshold tuning and custom metrics (used in GridSearchCV)\ndef my_scorer_2(clf, X, y_true, thres = np.arange(0.1,1,0.1)):\n    result_dict = {}\n    for threshold in np.atleast_1d(thres):\n        y_pred = (clf.predict_proba(X)[:,1] &gt; threshold).astype(int)\n        X_segment = (X['MonthlyCharges'] &gt; 95).to_numpy().astype(int)\n        y_stack = np.column_stack((X_segment, y_pred, y_true))\n        y_stack_reg, y_stack_vip = y_stack[y_stack[:,0] == 0], y_stack[y_stack[:,0] == 1]\n        cm_reg = confusion_matrix(y_stack_reg[:,2], y_stack_reg[:,1])\n        cm_vip = confusion_matrix(y_stack_vip[:,2], y_stack_vip[:,1])\n        fn_reg, fn_vip = cm_reg[1][0], cm_vip[1][0]\n        fp = cm_reg[0][1] + cm_vip[0][1]\n        loss_score = (fp * 1) + (fn_reg * 3) + (fn_vip * 5)\n        result_dict[threshold] = np.array([loss_score, metrics.precision_score(y_true, y_pred, zero_division = 0), \n                                           metrics.recall_score(y_true, y_pred), metrics.f1_score(y_true, y_pred)])\n        \n    result_np = np.array([np.insert(value, 0, key) for key, value in result_dict.items()])\n    best_np = result_np[result_np[:,1] == np.min(result_np[:,1])][0]\n    return best_np\n\ndef my_scorer_threshold(clf, X, y_true):\n    return my_scorer_2(clf, X, y_true)[0]\n\ndef my_scorer_ls(clf, X, y_true):\n    return my_scorer_2(clf, X, y_true)[1]\n\ndef my_scorer_precision(clf, X, y_true):\n    return my_scorer_2(clf, X, y_true)[2]\n\ndef my_scorer_recall(clf, X, y_true):\n    return my_scorer_2(clf, X, y_true)[3]\n\ndef my_scorer_f1(clf, X, y_true):\n    return my_scorer_2(clf, X, y_true)[4]\n        \n        \n\n\n#Grid scoring parameter\ngrid_scoring = {\n    'threshold': my_scorer_threshold,\n    'loss_score': my_scorer_ls,\n    'precision': my_scorer_precision,\n    'recall': my_scorer_recall,\n    'f1': my_scorer_f1\n}\n\n\n#define weight by missclassification cost which is FP:FN1:FN2 = 1:3:5\nX_segment = (X_train['MonthlyCharges'] &gt; 95).to_numpy().astype(int)\narr_weight = np.column_stack((X_segment, y_train))\nweight = np.apply_along_axis(lambda x: 1 if x[1] == 0 else 5 if x[0] == 1 else 3 , axis=1, arr=arr_weight)\n\n\n\nModel building 1 / Hyperparameter tuning + threshold tuning\n\n#RANDOM FOREST MODELLING\n\n#define parameter for tuning\nparam_grid_rf = {\n    'n_estimators': [250 , 400],\n    'max_depth': [10, 25, 50],\n    'min_samples_split': [25, 50, 70, 120],\n    'min_samples_leaf': [50, 75, 120],\n    'bootstrap' : [True, False]\n}\n\n#run grid search cv\nrf = GridSearchCV(estimator = RandomForestClassifier(),\n                  param_grid = param_grid_rf,\n                  cv=5,\n                  scoring = grid_scoring, refit=False, n_jobs = -1)\n\n#fit grid search cv with train data\nrf.fit(X_train, y_train)\n\n#selecting the best parameter and metrics\ngrid_result = pd.DataFrame(rf.cv_results_)\ngrid_result = grid_result[grid_result['mean_test_loss_score'] == grid_result['mean_test_loss_score'].min()][['params','mean_test_threshold','mean_test_loss_score','mean_test_precision','mean_test_recall','mean_test_f1']]\n\n#train model with the best hyperparameter\nrf = RandomForestClassifier(**grid_result.iloc[0,0])\n\n#train model with train data\nrf = rf.fit(X_train, y_train)\n\n#evaluate the model\nmb1_rf = my_scorer_2(rf, X_test, y_test, grid_result.iloc[0,1])\nmb1_rf = np.append(mb1_rf, grid_result.iloc[0,0])\n\n\n#XGBOOST MODELLING\n\n#define parameter for tuning\nparam_grid_xg = {\n    'learning_rate': [0.1, 0.01, 0.001],\n    'n_estimators': [100, 500],\n    'max_depth': [5, 10, 25],\n    'subsample': [0.8, 0.9, 1.0],\n    'colsample_bytree': [0.8, 0.9, 1.0]\n}\n\n#run grid search cv\nxg = GridSearchCV(estimator = XGBClassifier(),\n                  param_grid = param_grid_xg,\n                  cv=5,\n                  scoring = grid_scoring, refit=False, n_jobs = -1)\n\n#fit grid search cv with train data\nxg.fit(X_train, y_train)\n\n#selecting the best parameter and metrics\ngrid_result = pd.DataFrame(xg.cv_results_)\ngrid_result = grid_result[grid_result['mean_test_loss_score'] == grid_result['mean_test_loss_score'].min()][['params','mean_test_threshold','mean_test_loss_score','mean_test_precision','mean_test_recall','mean_test_f1']]\n\n#train model with the best hyperparameter\nxg = XGBClassifier(**grid_result.iloc[0,0])\n\n#train model with train data\nxg = xg.fit(X_train, y_train)\n\n#evaluate the model\nmb1_xg = my_scorer_2(xg, X_test, y_test, grid_result.iloc[0,1])\nmb1_xg = np.append(mb1_xg, grid_result.iloc[0,0])\n\n\n#LOGISTIC REGRESSION MODELLING\n\n#define parameter for tuning\nparam_grid_lg = {\n    'penalty': ['l1', 'l2'], \n    'C': [0.1, 1.0, 10.0],  \n    'solver': ['liblinear'],  \n    'max_iter': [50,100,200] \n}\n\n#run grid search cv\nlg = GridSearchCV(estimator = LogisticRegression(),\n                  param_grid = param_grid_lg,\n                  cv=5,\n                  scoring = grid_scoring, refit=False, n_jobs = -1)\n\n#fit grid search cv with train data\nlg.fit(X_train, y_train)\n\n#selecting the best parameter and metrics\ngrid_result = pd.DataFrame(lg.cv_results_)\ngrid_result = grid_result[grid_result['mean_test_loss_score'] == grid_result['mean_test_loss_score'].min()][['params','mean_test_threshold','mean_test_loss_score','mean_test_precision','mean_test_recall','mean_test_f1']]\n\n#train model with the best hyperparameter\nlg = LogisticRegression(**grid_result.iloc[0,0])\n\n#train model with train data\nlg = lg.fit(X_train, y_train)\n\n#evaluate the model\nmb1_lg = my_scorer_2(lg, X_test, y_test, grid_result.iloc[0,1])\nmb1_lg = np.append(mb1_lg, grid_result.iloc[0,0])\n\n\nresult_mb1 = pd.DataFrame([mb1_rf,mb1_xg,mb1_lg])\nresult_mb1.columns = ['threshold','score','precision','recall','f1','params']\nresult_mb1['model'] = ['MB1_RF','MB1_XG','MB1_Log_Reg']\nresult_mb1\n\n\n\n\n\n\n\n\nthreshold\nscore\nprecision\nrecall\nf1\nparams\nmodel\n\n\n\n\n0\n0.24\n539.0\n0.473101\n0.828255\n0.602216\n{'bootstrap': True, 'max_depth': 25, 'min_samp...\nMB1_RF\n\n\n1\n0.20\n558.0\n0.449190\n0.844875\n0.586538\n{'colsample_bytree': 0.8, 'learning_rate': 0.0...\nMB1_XG\n\n\n2\n0.22\n541.0\n0.467492\n0.836565\n0.599801\n{'C': 0.1, 'max_iter': 50, 'penalty': 'l2', 's...\nMB1_Log_Reg\n\n\n\n\n\n\n\n\n\nModel building 2 / Hyperparameter tuning + threshold tuning + SMOTE\n\n#RANDOM FOREST MODELLING\n\n#define parameter for tuning\nparam_grid_rf_smote = {\n    'class__n_estimators': [250 , 400],\n    'class__max_depth': [10, 25, 50],\n    'class__min_samples_split': [25, 50, 70, 120],\n    'class__min_samples_leaf': [50, 75, 120],\n    'class__bootstrap' : [True, False]\n}\n\n#create imbalanced pipeline to SMOTE \npipelinerf = Pipeline([\n        ('sampling', SMOTE()),\n        ('class', RandomForestClassifier())])\n\n#run grid search cv\nrf = GridSearchCV(estimator = pipelinerf,\n                  param_grid = param_grid_rf_smote,\n                  cv=5,\n                  scoring = grid_scoring, refit=False, n_jobs = -1)\n\n#fit grid search cv with train data\nrf.fit(X_train, y_train)\n\n#selecting the best parameter and metrics\ngrid_result = pd.DataFrame(rf.cv_results_)\ngrid_result = grid_result[grid_result['mean_test_loss_score'] == grid_result['mean_test_loss_score'].min()][['params','mean_test_threshold','mean_test_loss_score','mean_test_precision','mean_test_recall','mean_test_f1']]\nparams = grid_result.iloc[0,0]\n\nparams_update = {}\nfor key, value in params.items():\n    new_key = key.replace('class__', '')\n    params_update[new_key] = value\nparams = params_update\n    \n#train model with the best hyperparameter\nrf = RandomForestClassifier(**params)\n\n#train model with SMOTE train data\nX_train1, y_train1 = SMOTE().fit_resample(X_train, y_train)\nrf = rf.fit(X_train1, y_train1)\n\n#evaluate the model\nmb2_rf = my_scorer_2(rf, X_test, y_test, grid_result.iloc[0,1])\nmb2_rf = np.append(mb2_rf, params)\n\n\n#XGBOOST MODELLING\n\n#define parameter for tuning\nparam_grid_xg_smote = {\n    'class__learning_rate': [0.1, 0.01, 0.001],\n    'class__n_estimators': [100, 500],\n    'class__max_depth': [5, 10, 25],\n    'class__subsample': [0.8, 0.9, 1.0],\n    'class__colsample_bytree': [0.8, 0.9, 1.0]\n}\n\n#create imbalanced pipeline to SMOTE \npipelinexg = Pipeline([\n        ('sampling', SMOTE()),\n        ('class', XGBClassifier())])\n\n#run grid search cv\nxg = GridSearchCV(estimator = pipelinexg,\n                  param_grid = param_grid_xg_smote,\n                  cv=5,\n                  scoring = grid_scoring, refit=False, n_jobs = -1)\n\n#fit grid search cv with train data\nxg.fit(X_train, y_train)\n\n#selecting the best parameter and metrics\ngrid_result = pd.DataFrame(xg.cv_results_)\ngrid_result = grid_result[grid_result['mean_test_loss_score'] == grid_result['mean_test_loss_score'].min()][['params','mean_test_threshold','mean_test_loss_score','mean_test_precision','mean_test_recall','mean_test_f1']]\nparams = grid_result.iloc[0,0]\n\nparams_update = {}\nfor key, value in params.items():\n    new_key = key.replace('class__', '')\n    params_update[new_key] = value\nparams = params_update\n    \n#train model with the best hyperparameter\nxg = XGBClassifier(**params)\n\n#train model with SMOTE train data\nX_train1, y_train1 = SMOTE().fit_resample(X_train, y_train)\nxg = xg.fit(X_train1, y_train1)\n\n#evaluate the model\nmb2_xg = my_scorer_2(xg, X_test, y_test, grid_result.iloc[0,1])\nmb2_xg = np.append(mb2_xg, params)\n\n\n#LOGISTIC REGRESSION MODELLING\n\n#define parameter for tuning\nparam_grid_lg_smote = {\n    'class__penalty': ['l1', 'l2'], \n    'class__C': [0.1, 1.0, 10.0],  \n    'class__solver': ['liblinear'],  \n    'class__max_iter': [50,100,200] \n}\n\n\n#create imbalanced pipeline to SMOTE \npipelinelg = Pipeline([\n        ('sampling', SMOTE()),\n        ('class', LogisticRegression())])\n\n#run grid search cv\nlg = GridSearchCV(estimator = pipelinelg,\n                  param_grid = param_grid_lg_smote,\n                  cv=5,\n                  scoring = grid_scoring, refit=False, n_jobs = -1)\n\n#fit grid search cv with train data\nlg.fit(X_train, y_train)\n\n#selecting the best parameter and metrics\ngrid_result = pd.DataFrame(lg.cv_results_)\ngrid_result = grid_result[grid_result['mean_test_loss_score'] == grid_result['mean_test_loss_score'].min()][['params','mean_test_threshold','mean_test_loss_score','mean_test_precision','mean_test_recall','mean_test_f1']]\nparams = grid_result.iloc[0,0]\n\nparams_update = {}\nfor key, value in params.items():\n    new_key = key.replace('class__', '')\n    params_update[new_key] = value\nparams = params_update\n    \n#train model with the best hyperparameter\nlg = LogisticRegression(**params)\n\n#train model with SMOTE train data\nX_train1, y_train1 = SMOTE().fit_resample(X_train, y_train)\nlg = lg.fit(X_train1, y_train1)\n\n#evaluate the model\nmb2_lg = my_scorer_2(lg, X_test, y_test, grid_result.iloc[0,1])\nmb2_lg = np.append(mb2_lg, params)\n\n\nresult_mb2 = pd.DataFrame([mb2_rf,mb2_xg,mb2_lg])\nresult_mb2.columns = ['threshold','score','precision','recall','f1','params']\nresult_mb2['model'] = ['MB2_RF','MB2_XG','MB2_Log_Reg']\nresult_mb2\n\n\n\n\n\n\n\n\nthreshold\nscore\nprecision\nrecall\nf1\nparams\nmodel\n\n\n\n\n0\n0.42\n557.0\n0.454955\n0.839335\n0.590068\n{'bootstrap': False, 'max_depth': 25, 'min_sam...\nMB2_RF\n\n\n1\n0.44\n546.0\n0.472266\n0.825485\n0.600806\n{'colsample_bytree': 1.0, 'learning_rate': 0.0...\nMB2_XG\n\n\n2\n0.48\n546.0\n0.472178\n0.822715\n0.600000\n{'C': 1.0, 'max_iter': 50, 'penalty': 'l2', 's...\nMB2_Log_Reg\n\n\n\n\n\n\n\n\n\nModel building 3 / Hyperparameter tuning + threshold tuning + custom weight\n\n#RANDOM FOREST MODELLING\n\n#define parameter for tuning\nparam_grid_rf = {\n    'n_estimators': [250 , 400],\n    'max_depth': [10, 25, 50],\n    'min_samples_split': [25, 50, 70, 120],\n    'min_samples_leaf': [50, 75, 120],\n    'bootstrap' : [True, False]\n}\n\n#run grid search cv\nrf = GridSearchCV(estimator = RandomForestClassifier(),\n                  param_grid = param_grid_rf,\n                  cv=5,\n                  scoring = grid_scoring, refit=False, n_jobs = -1)\n\n#fit grid search cv with train data\nrf.fit(X_train, y_train, sample_weight = weight)\n\n#selecting the best parameter and metrics\ngrid_result = pd.DataFrame(rf.cv_results_)\ngrid_result = grid_result[grid_result['mean_test_loss_score'] == grid_result['mean_test_loss_score'].min()][['params','mean_test_threshold','mean_test_loss_score','mean_test_precision','mean_test_recall','mean_test_f1']]\n\n#train model with the best hyperparameter\nrf = RandomForestClassifier(**grid_result.iloc[0,0])\n\n#train model with train data\nrf = rf.fit(X_train, y_train, sample_weight = weight)\n\n#evaluate the model\nmb3_rf = my_scorer_2(rf, X_test, y_test, grid_result.iloc[0,1])\nmb3_rf = np.append(mb3_rf, grid_result.iloc[0,0])\n\n\n#XGBOOST MODELLING\n\n#define parameter for tuning\nparam_grid_xg = {\n    'learning_rate': [0.1, 0.01, 0.001],\n    'n_estimators': [100, 500],\n    'max_depth': [5, 10, 25],\n    'subsample': [0.8, 0.9, 1.0],\n    'colsample_bytree': [0.8, 0.9, 1.0]\n}\n\n#run grid search cv\nxg = GridSearchCV(estimator = XGBClassifier(),\n                  param_grid = param_grid_xg,\n                  cv=5,\n                  scoring = grid_scoring, refit=False, n_jobs = -1)\n\n#fit grid search cv with train data\nxg.fit(X_train, y_train, sample_weight = weight)\n\n#selecting the best parameter and metrics\ngrid_result = pd.DataFrame(xg.cv_results_)\ngrid_result = grid_result[grid_result['mean_test_loss_score'] == grid_result['mean_test_loss_score'].min()][['params','mean_test_threshold','mean_test_loss_score','mean_test_precision','mean_test_recall','mean_test_f1']]\n\n#train model with the best hyperparameter\nxg = XGBClassifier(**grid_result.iloc[0,0])\n\n#train model with train data\nxg = xg.fit(X_train, y_train, sample_weight = weight)\n\n#evaluate the model\nmb3_xg = my_scorer_2(xg, X_test, y_test, grid_result.iloc[0,1])\nmb3_xg = np.append(mb3_xg, grid_result.iloc[0,0])\n\n\n#LOGISTIC REGRESSION MODELLING\n\n#define parameter for tuning\nparam_grid_lg = {\n    'penalty': ['l1', 'l2'], \n    'C': [0.1, 1.0, 10.0],  \n    'solver': ['liblinear'],  \n    'max_iter': [50,100,200] \n}\n\n\n#run grid search cv\nlg = GridSearchCV(estimator = LogisticRegression(),\n                  param_grid = param_grid_lg,\n                  cv=5,\n                  scoring = grid_scoring, refit=False, n_jobs = -1)\n\n#fit grid search cv with train data\nlg.fit(X_train, y_train, sample_weight = weight)\n\n#selecting the best parameter and metrics\ngrid_result = pd.DataFrame(lg.cv_results_)\ngrid_result = grid_result[grid_result['mean_test_loss_score'] == grid_result['mean_test_loss_score'].min()][['params','mean_test_threshold','mean_test_loss_score','mean_test_precision','mean_test_recall','mean_test_f1']]\n\n#train model with the best hyperparameter\nlg = LogisticRegression(**grid_result.iloc[0,0])\n\n#train model with train data\nlg = lg.fit(X_train, y_train, sample_weight = weight)\n\n#evaluate the model\nmb3_lg = my_scorer_2(lg, X_test, y_test, grid_result.iloc[0,1])\nmb3_lg = np.append(mb3_lg, grid_result.iloc[0,0])\n\n\nresult_mb3 = pd.DataFrame([mb3_rf,mb3_xg,mb3_lg])\nresult_mb3.columns = ['threshold','score','precision','recall','f1','params']\nresult_mb3['model'] = ['MB3_RF','MB3_XG','MB3_Log_Reg']\nresult_mb3\n\n\n\n\n\n\n\n\nthreshold\nscore\nprecision\nrecall\nf1\nparams\nmodel\n\n\n\n\n0\n0.48\n549.0\n0.456193\n0.836565\n0.590420\n{'bootstrap': True, 'max_depth': 25, 'min_samp...\nMB3_RF\n\n\n1\n0.48\n551.0\n0.465409\n0.819945\n0.593781\n{'colsample_bytree': 0.8, 'learning_rate': 0.0...\nMB3_XG\n\n\n2\n0.44\n571.0\n0.433803\n0.853186\n0.575163\n{'C': 0.1, 'max_iter': 50, 'penalty': 'l2', 's...\nMB3_Log_Reg\n\n\n\n\n\n\n\n\nfinal_result = pd.concat([result_mb1, result_mb2, result_mb3]).sort_values('score')\nfinal_result\n\n\n\n\n\n\n\n\nthreshold\nscore\nprecision\nrecall\nf1\nparams\nmodel\n\n\n\n\n0\n0.24\n539.0\n0.473101\n0.828255\n0.602216\n{'bootstrap': True, 'max_depth': 25, 'min_samp...\nMB1_RF\n\n\n2\n0.22\n541.0\n0.467492\n0.836565\n0.599801\n{'C': 0.1, 'max_iter': 50, 'penalty': 'l2', 's...\nMB1_Log_Reg\n\n\n1\n0.44\n546.0\n0.472266\n0.825485\n0.600806\n{'colsample_bytree': 1.0, 'learning_rate': 0.0...\nMB2_XG\n\n\n2\n0.48\n546.0\n0.472178\n0.822715\n0.600000\n{'C': 1.0, 'max_iter': 50, 'penalty': 'l2', 's...\nMB2_Log_Reg\n\n\n0\n0.48\n549.0\n0.456193\n0.836565\n0.590420\n{'bootstrap': True, 'max_depth': 25, 'min_samp...\nMB3_RF\n\n\n1\n0.48\n551.0\n0.465409\n0.819945\n0.593781\n{'colsample_bytree': 0.8, 'learning_rate': 0.0...\nMB3_XG\n\n\n0\n0.42\n557.0\n0.454955\n0.839335\n0.590068\n{'bootstrap': False, 'max_depth': 25, 'min_sam...\nMB2_RF\n\n\n1\n0.20\n558.0\n0.449190\n0.844875\n0.586538\n{'colsample_bytree': 0.8, 'learning_rate': 0.0...\nMB1_XG\n\n\n2\n0.44\n571.0\n0.433803\n0.853186\n0.575163\n{'C': 0.1, 'max_iter': 50, 'penalty': 'l2', 's...\nMB3_Log_Reg\n\n\n\n\n\n\n\n\nAnalysis of how the models performed: 1.RandomForest performs best with hyperparameter tuning and a lower decision threshold. However, this model performs worst when using SMOTE. 2.LogisticRegression performs worst when using ‘sample weighting’. 3.XGBoost performs best when using SMOTE. 4.It’s important to note that all models produce similar results when using their best parameters and conditions. 5.In my opinion, the greatest impact is achieved by using hyperparameter tuning and decision threshold tuning, rather than using SMOTE and weighting techniques."
  },
  {
    "objectID": "ipynb/telco/final-project-original.html#model-interpretation",
    "href": "ipynb/telco/final-project-original.html#model-interpretation",
    "title": "Table Of Contents",
    "section": "Model Interpretation",
    "text": "Model Interpretation\n\nIn this section, I want to show you how to interpret a tree-based model, such as Random Forest, so we can have a better understanding of how the model actually works.\n\n#selecting the best parameter for random forest\nrf_param = final_result[final_result['model'] == 'MB1_RF']['params'][0]\nrf_param\n\n{'bootstrap': True,\n 'max_depth': 25,\n 'min_samples_leaf': 50,\n 'min_samples_split': 50,\n 'n_estimators': 250}\n\n\n\nrf = RandomForestClassifier(**rf_param)\nrf.fit(X_train, y_train)\n\nRandomForestClassifier(max_depth=25, min_samples_leaf=50, min_samples_split=50,\n                       n_estimators=250)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(max_depth=25, min_samples_leaf=50, min_samples_split=50,\n                       n_estimators=250)\n\n\n\nPartial dependence plot (PDP) & Individual conditional expectation (ICE)\nWhat is ICE? It is a plot that shows how a model makes predictions based on changing the value of one or more features, while keeping the values of other features constant. This provides us with more insights and understanding of how the model treats features to make predictions. ICE works per row (or per customer in this case), and PDP is simply the average of ICE.\n\ndef pdp_ice_plot(model, df_test, column, clusters=True):\n    df_test = df_test.copy()\n    pdp_isolate = pdp.PDPIsolate(model = model, df = df_test, \n                      num_grid_points=15,\n                      model_features = df_test.columns, \n                      feature = column, feature_name=column)\n    pdp_isolate.plot(\n                center=True, plot_lines=True,\n                cluster=clusters, n_cluster_centers=5,cluster_method='accurate',\n                plot_pts_dist=False, to_bins=True,\n                show_percentile=False, which_classes=None,\n                figsize=(10,6), dpi=300,\n                ncols=2, plot_params={\"pdp_hl\": True},\n                engine='matplotlib', template='plotly_white')\n    plt.show()\n    \n\n\npdp_ice_plot(rf, X_test, 'MonthlyCharges', False)\n\nobtain pred_func from the provided model.\n\n\n\n\n\n\n\n\n\nThe light blue lines represent ICE (Individual Conditional Expectation), and the yellowish blue line represents PDP (Partial Dependence Plot). The X-axis represents MonthlyCharges, while the Y-axis represents the change in prediction probability. At MonthlyCharges of 60.7 USD, you can observe that some customers experience a significant increase in churn probability as the MonthlyCharges increase. However, it is important to note that not all customers have the same response. Some customers are minimally affected, and some may not be affected at all.\n\n\npdp_ice_plot(rf, X_test, 'MonthlyCharges')\n\nobtain pred_func from the provided model.\n\n\n\n\n\n\n\n\n\nThis is the same plot as above, but I have grouped the ICE into 5 clusters for easier viewing and analysis. You can see that there are some customers who experience a significant increase in churn probability as the MonthlyCharges increase.\n\n\npdp_ice_plot(rf, X_test, 'tenure')\n\nobtain pred_func from the provided model.\n\n\n\n\n\n\n\n\n\nThe longer the tenure, the lower the churn probability. However, the effect is not the same for all customers. Some customers are greatly affected, while others are barely affected.\n\n\npdp_ice_plot(rf, X_test, 'Contract')\n\nobtain pred_func from the provided model.\n\n\n\n\n\n\n\n\n\nThe same also goes with Contract. Longer contract means lower churn probability.\n\n\nX_test_copy = X_test.copy()\npdp_interact = pdp.PDPInteract(model=rf, df=X_test_copy,\n                              num_grid_points=6,\n                              model_features = X_test.columns, \n                              features=['MonthlyCharges','tenure'], \n                              feature_names=['MonthlyCharges','tenure'])\npdp_interact.plot(\n            plot_type=\"grid\", plot_pdp=True,\n            to_bins=True, show_percentile=False,\n            which_classes=None, figsize=(10,6),\n            dpi=300, ncols=2, plot_params=None, \n            engine='matplotlib', template='plotly_white',\n)\nplt.show()\n\nobtain pred_func from the provided model.\nobtain pred_func from the provided model.\nobtain pred_func from the provided model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s analyze the combination of MonthlyCharges and tenure. We can observe a spike in churn probability for MonthlyCharges ranging from 56.3 USD to 79.2 USD, particularly for customers with a tenure of less than 7 months.\n\n\nX_test_copy = X_test.copy()\npdp_interact = pdp.PDPInteract(model=rf, df=X_test_copy,\n                              num_grid_points=10,\n                              model_features = X_test.columns, \n                              features=['tenure','Contract'], \n                              feature_names=['tenure','Contract'])\npdp_interact.plot(\n            plot_type=\"grid\", plot_pdp=True,\n            to_bins=True, show_percentile=False,\n            which_classes=None, figsize=(10,6),\n            dpi=300, ncols=2, plot_params=None, \n            engine='matplotlib', template='plotly_white',\n)\nplt.show()\n\nobtain pred_func from the provided model.\nobtain pred_func from the provided model.\nobtain pred_func from the provided model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe model produces similar churn probabilities for customers with a combination of a 2-year contract and low tenure compared to those with a month-to-month contract and medium tenure (24-36 months).\n\nYou can see how PDP and ICE plots can be very beneficial in understanding how the model utilizes features to make predictions. In the next section, I will demonstrate how to assess the model’s prediction confidence level.\n\n\nConfidence level based on trees’ standard deviation and confidence interval.\nTree-based models like RandomForest make predictions by using the mean of all the trees’ prediction probabilities. However, instead of solely relying on the mean, we can also calculate the standard deviation. A higher standard deviation indicates lower confidence in the predictions. Additionally, we can utilize confidence intervals, such as 95% or even more extreme at 99%.\n\n#extract all trees' prediction probability per row\npredict = np.stack([x.predict_proba(X_test)[:,1] for x in rf.estimators_])\n\n\n#assign mean and std. deviation of trees' prediction probability.\ndf_pred = X_test.copy()\ndf_pred['avg'] = np.round(np.mean(predict, axis = 0) * 100, 2)\ndf_pred['std_dev'] = np.round(np.std(predict, axis = 0) * 100, 2)\n\n\nplt.figure(figsize=(10,6))\nsns.histplot(df_pred['std_dev'],color='skyblue', kde=True, edgecolor='none')\nplt.title('Standard deviation distribution', weight='bold')\nplt.show()\n\n\n\n\n\nMost of predictions have std.deviation under 10%. Let’s calculate confidence interval with 99%.\n\n\ndf_pred['CI-99%'] = (2.576 * df_pred['std_dev'] / np.sqrt(len(predict))) * 100 / (df_pred['avg'])\n\n\ndf_pred[df_pred.avg &gt; 40].sort_values('CI-99%', ascending=False).head(5)\n\n\n\n\n\n\n\n\nContract\ntenure\nFiber_optic\nMonthlyCharges\nFO_LB\nNo_internet\navg\nstd_dev\nCI-99%\n\n\n\n\n852\n2\n7\n1\n94.05\n1\n0\n40.06\n25.08\n10.199818\n\n\n499\n1\n58\n1\n98.70\n1\n0\n41.05\n18.24\n7.239149\n\n\n1186\n1\n59\n1\n101.10\n1\n0\n40.37\n16.73\n6.751699\n\n\n1137\n1\n15\n1\n96.30\n0\n0\n48.14\n19.69\n6.663701\n\n\n1184\n1\n10\n1\n92.50\n0\n0\n48.17\n19.12\n6.466765\n\n\n\n\n\n\n\n\nLet’s consider the example of row 1. The model predicts a 40% probability of churn for the customer, with a confidence interval of +- 10%. By default, the model’s output indicates that the customer will not churn. However, due to the high confidence interval, it is safer to assume that the customer will churn.\n\nChecking the standard deviation and confidence interval of the trees is extremely useful, particularly when the cost of ‘False Negative’ is significant and can have severe consequences."
  },
  {
    "objectID": "portfolios/telco_churn/index.html",
    "href": "portfolios/telco_churn/index.html",
    "title": "Telco customer churn analysis",
    "section": "",
    "text": "On this occasion, I would like to discuss one of my projects that involves the telco customer churn dataset. Who doesn’t know about telco customer churn? This dataset is exceedingly common, and I’m confident that every data science practitioner has worked with it at some point. So, why am I still using this dataset? It’s because I want to demonstrate a different and more detailed approach on how to process it. Here, I will conduct a deeper Exploratory Data Analysis (EDA) and showcase how to create and fine-tune a machine learning model to align it with the intended business objectives, followed by interpreting the results.\nOf course, the goals of this project are to produce actionable insights and the most suitable predictive model aligned with the existing business concepts. On this particular blog post, I will show detailed step by step on how to do the projects (including the codes) so this will be very technical, but if you just want to see the big picture you can see the google slides presentation below : (go full screen by clicking 3-dot option button and choose ‘Full Screen’)\nBelow is the web application of this project that I made on shinyApps for Python: (click the image to open the web application)\nLet’s jump into the code."
  },
  {
    "objectID": "portfolios/telco_churn/index.html#table-of-contents",
    "href": "portfolios/telco_churn/index.html#table-of-contents",
    "title": "Telco customer churn analysis",
    "section": "Table Of Contents",
    "text": "Table Of Contents\n• Data Introduction  • Data Preparation (Import libraries, data cleaning & data wrangling) • Basic exploratory data analysis      • Descriptive statistic      • Univariate analysis      • Multivariate analysis  • Deep exploratory data analysis      • Services and internet services analysis      • Monthly charges analysis      • Customer analysis      • Benefits analysis      • Churn analysis  • Modelling     • Features selection and encoding      • Splits data and define custom metrics      • Model building combination 1      • Model building combination 2      • Model building combination 3\n• Model interpretation     • PDP and ICE plots      • Checking prediction’s confidence using confidence interval"
  },
  {
    "objectID": "portfolios/telco_churn/index.html#data-introduction",
    "href": "portfolios/telco_churn/index.html#data-introduction",
    "title": "Telco customer churn analysis",
    "section": "Data Introduction",
    "text": "Data Introduction\nThis dataset contain informations about customers that churn and not churned in a telco company. Below is the column informations: 1.customerID = customer unique ID. 2.gender = customer gender (M/F). 3.SeniorCitizen = old / young customer. 4.Partner = either a customer has partners or not. 5.Dependents = either a customer has dependents or not. 6.tenure = how long the customer subscribed (in month). 7.MultipleLines = either a customer using multiple lines or not (phone lines). 8.InternetService = either a customer using InternetService lines or not. 9.OnlineSecurity = either a customer has OnlineSecurity or not. 10.OnlineBackup = either a customer has OnlineBackup or not. 11.DeviceProtection = either a customer has DeviceProtection or not. 12.TechSupport = either a customer has TechSupport or not. 13.StreamingTV = either a customer has StreamingTV or not. 14.StreamingMovies = either a customer has StreamingMovie or not. 15.Contract = types of contract. 16.PaperlessBilling = either a customer has PaperlessBilling or not. 17.PaymentMethod = types of the payment method. 18.MonthlyCharges = how much charges per month. 19.TotalCharges = total charges of all time. 20.Churn = either a customer churn or not. 21.Hobby = customer hobby."
  },
  {
    "objectID": "portfolios/telco_churn/index.html#data-preparation",
    "href": "portfolios/telco_churn/index.html#data-preparation",
    "title": "Telco customer churn analysis",
    "section": "Data Preparation",
    "text": "Data Preparation\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import gaussian_kde\nimport matplotlib as mpl\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.metrics import confusion_matrix\nfrom pdpbox import pdp, info_plots\nsns.set_style('darkgrid')\nmpl.rcParams['figure.dpi'] = 300 #set figure dpi\nsns.set() #set figure styling\n\n\n\n\nCode\n#import dataset\ndf = pd.read_csv('telco.csv')\ndf.head()\n\n\n\n\n\n\n\n\n\ncustomerID\ngender\nSeniorCitizen\nPartner\nDependents\ntenure\nPhoneService\nMultipleLines\nInternetService\nOnlineSecurity\n...\nTechSupport\nStreamingTV\nStreamingMovies\nContract\nPaperlessBilling\nPaymentMethod\nMonthlyCharges\nTotalCharges\nChurn\nHobby\n\n\n\n\n0\n7590-VHVEG\nFemale\n0\nYes\nNo\n135\nNo\nNo phone service\nDSL\nNo\n...\nNo\nNo\nNo\nMonth-to-month\nYes\nElectronic check\n29.85\n29.85\nNo\nSwimming\n\n\n1\n5575-GNVDE\nMale\n0\nNo\nNo\n34\nYes\nNo\nDSL\nYes\n...\nNo\nNo\nNo\nOne year\nNo\nMailed check\n56.95\n1889.5\nNo\nRunning\n\n\n2\n3668-QPYBK\nMale\n0\nNo\nNo\n140\nYes\nNo\nDSL\nYes\n...\nNo\nNo\nNo\nMonth-to-month\nYes\nMailed check\n53.85\n7560\nYes\nHiking\n\n\n3\n7795-CFOCW\nMale\n0\nNo\nNo\n136\nNo\nNo phone service\nDSL\nYes\n...\nYes\nNo\nNo\nOne year\nNo\nBank transfer (automatic)\n42.45\n1840.75\nNo\nSwimming\n\n\n4\n9237-HQITU\nFemale\n0\nNo\nNo\n2\nYes\nNo\nFiber optic\nNo\n...\nNo\nNo\nNo\nMonth-to-month\nYes\nElectronic check\n70.70\n151.65\nYes\nRunning\n\n\n\n\n5 rows × 22 columns\n\n\n\n\n\nCode\n#take a look at the dataframe\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 7043 entries, 0 to 7042\nData columns (total 22 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   customerID        7043 non-null   object \n 1   gender            7043 non-null   object \n 2   SeniorCitizen     7043 non-null   int64  \n 3   Partner           7043 non-null   object \n 4   Dependents        7043 non-null   object \n 5   tenure            7043 non-null   int64  \n 6   PhoneService      7043 non-null   object \n 7   MultipleLines     7043 non-null   object \n 8   InternetService   7043 non-null   object \n 9   OnlineSecurity    7043 non-null   object \n 10  OnlineBackup      7043 non-null   object \n 11  DeviceProtection  6627 non-null   object \n 12  TechSupport       7043 non-null   object \n 13  StreamingTV       7043 non-null   object \n 14  StreamingMovies   7043 non-null   object \n 15  Contract          6798 non-null   object \n 16  PaperlessBilling  7043 non-null   object \n 17  PaymentMethod     7043 non-null   object \n 18  MonthlyCharges    7043 non-null   float64\n 19  TotalCharges      4859 non-null   object \n 20  Churn             7043 non-null   object \n 21  Hobby             4201 non-null   object \ndtypes: float64(1), int64(2), object(19)\nmemory usage: 1.2+ MB\n\n\nThere are 4 columns with missing values which are DeviceProtection, Contract, TotalCharges and Hobby.\n\n\nCode\n#checking percentage of missing values\nmissingkolom = ['DeviceProtection','Contract','TotalCharges','Hobby']\nfor x in missingkolom:\n    print(f'Missing value of column {x} (%) : {round(df[x].isna().sum()/len(df) * 100,2)}')\n\n\nMissing value of column DeviceProtection (%) : 5.91\nMissing value of column Contract (%) : 3.48\nMissing value of column TotalCharges (%) : 31.01\nMissing value of column Hobby (%) : 40.35\n\n\n\n\nCode\n#impute missing values with univariate imputation by value proportion\ndf['DeviceProtection'] = df['DeviceProtection'].fillna(\n                             pd.Series(np.random.choice(['No','Yes','No internet service'], \n                             p = list(df['DeviceProtection'].value_counts(normalize=True)), size=len(df))))\n\ndf['Contract'] = df['Contract'].fillna(\n                     pd.Series(np.random.choice(['Month-to-month','Two year','One year'], \n                     p = list(df['Contract'].value_counts(normalize=True)), size=len(df))))\n\n\nHere I impute DeviceProtection and Contract with univariate imputation by value proportion for the following reasons: 1.The missing values is not that much (&lt;10%). 2.The columns don’t have any relationship with other columns so that multivariate imputation is not possible. 3.Using proportion is more precise in this case rather than use ‘mode’.\n\n\nCode\n#delete column Hobby\ndf.drop(columns=['Hobby'],inplace=True)\n\n\nReasons to delete: 1.Missing values is too many. 2.By business context, Hobby doesn’t give enough useful informations. 3.Cannot be imputed by multivariate imputation.\n\n\nCode\n#impute TotalCharges from tenure and MonthlyCharges\ndf['TotalCharges'] = df['TotalCharges'].fillna(df['tenure'] * df['MonthlyCharges'])\n\n\nEven this column has so many missing values, I decided to impute it with multivariate imputation because: 1.By business context, TotalCharges is more or less tenure * MonthlyCharges. 2.So even the missing values are high, it can still be imputed with a strong justification.\n\n\nCode\n#there's a space in the total charges column.\nfor x in df.TotalCharges:\n    try:\n        float(x)\n    except:\n        print(f'Unable to convert to float with this value : {x}')\n\n\nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \nUnable to convert to float with this value :  \n\n\n\n\nCode\ndf[df.TotalCharges == ' '].head()\n\n\n\n\n\n\n\n\n\ncustomerID\ngender\nSeniorCitizen\nPartner\nDependents\ntenure\nPhoneService\nMultipleLines\nInternetService\nOnlineSecurity\n...\nDeviceProtection\nTechSupport\nStreamingTV\nStreamingMovies\nContract\nPaperlessBilling\nPaymentMethod\nMonthlyCharges\nTotalCharges\nChurn\n\n\n\n\n488\n4472-LVYGI\nFemale\n0\nYes\nYes\n0\nNo\nNo phone service\nDSL\nYes\n...\nYes\nYes\nYes\nNo\nOne year\nYes\nBank transfer (automatic)\n52.55\n\nNo\n\n\n753\n3115-CZMZD\nMale\n0\nNo\nYes\n0\nYes\nNo\nNo\nNo internet service\n...\nNo internet service\nNo internet service\nNo internet service\nNo internet service\nTwo year\nNo\nMailed check\n20.25\n\nNo\n\n\n936\n5709-LVOEQ\nFemale\n0\nYes\nYes\n0\nYes\nNo\nDSL\nYes\n...\nNo\nNo\nYes\nYes\nTwo year\nNo\nMailed check\n80.85\n\nNo\n\n\n1082\n4367-NUYAO\nMale\n0\nYes\nYes\n0\nYes\nYes\nNo\nNo internet service\n...\nNo internet service\nNo internet service\nNo internet service\nNo internet service\nTwo year\nNo\nMailed check\n25.75\n\nNo\n\n\n1334\n1768-ZAIFU\nFemale\n1\nNo\nNo\n0\nNo\nNo phone service\nDSL\nNo\n...\nNo\nNo\nNo\nNo\nMonth-to-month\nYes\nElectronic check\n25.20\n\nYes\n\n\n\n\n5 rows × 21 columns\n\n\n\nWhen the tenure value is 0, the TotalCharges value is empty (‘space’). This is because customers who have just joined (less than a month) have not been charged yet, resulting in a TotalCharges value of 0. Since there are only 12 rows with this condition, I will delete them.\n\n\nCode\n#drop rows that has empty TotalCharges.\ndf = df.drop(df.index[df.TotalCharges == ' ']).reset_index(drop=True)\n\n\n\n\nCode\n#check for duplicate data, if True then there's no duplicate.\ndf.customerID.nunique() == len(df) \n\n\nTrue\n\n\n\n\nCode\n#feature engineered 2 new features for the sake of easier analysis.\ndf['Services'] = df[['PhoneService','InternetService']].apply(\n                     lambda x: 'Both' if list(x).count('No') == 0 else\n                     'Internet Only' if x[0] == 'No' else 'Phone Only', axis=1)\n\ndf['TotalBenefits'] = df.loc[:,'OnlineSecurity':'StreamingMovies']\\\n                          .apply(lambda x: list(x).count('Yes'), axis=1)\n\n\nNew features explanation:  1.Services = Combined values of PhoneService and InternetService. 2.TotalBenefits = Sum of benefits taken on OnlineSecurity until StreamingMovies.\n\n\nCode\n#Change values of 1 and 0 to 'Yes' and 'No'\ndf['SeniorCitizen'] = df.SeniorCitizen.apply(lambda x: 'Yes' if x == 1 else 'No')\n\n\nChange numerical value to strings for simpler and consistent analysis.\n\n\nCode\n#drop useless column\ndf.drop(columns=['customerID'], inplace=True)\n\n\n\n\nCode\n#change columns object data type to numerical\ndf.tenure = df.tenure.astype('int64')\ndf.MonthlyCharges = df.MonthlyCharges.astype('float64')\ndf.TotalCharges = df.TotalCharges.astype('float64')\n\n\n\n\nCode\n#checking values of real totalcharges and calculated totalcharges\ndf['TotalChargesDiff'] = df[['tenure','MonthlyCharges','TotalCharges']].apply(\n                             lambda x: round(abs(1 - (x[0] * x[1] / x[2])) * 100, 3), axis=1)\n\n\nHere, I have created a new column called TotalChargesDiff to check the differences (%) between the actual TotalCharges value (from the dataset) and the calculated TotalCharges value (obtained by multiplying tenure with MonthlyCharges). If the difference is above 40%, I will consider those rows as invalid because the values of tenure and MonthlyCharges cannot be trusted.\n\n\nCode\ndf['TotalChargesDiff'].sort_values(ascending=False).head(10)\n\n\n0       13400.000\n5        1357.404\n18        788.048\n19        330.214\n3         213.633\n128        73.511\n47         72.615\n4631       64.286\n5802       63.380\n20         58.263\nName: TotalChargesDiff, dtype: float64\n\n\nYou can observe that some data points have a TotalChargesDiff that reaches hundreds or even thousands percent.\n\n\nCode\n#removing rows that have &gt; 40% TotalChargesDiff.\ndf = df[df.TotalChargesDiff &lt; 40].reset_index(drop=True)\ndf.drop(columns=['TotalChargesDiff'], inplace=True)\n\n\n\n\nCode\ndef numericategoric(df):\n    num = len(df._get_numeric_data().columns)\n    cat = len(df.columns) - num\n    print(\"TotalNumericalData = \" + str(num))\n    print(\"TotalCategoricalData = \" + str(cat))\n    print(\"Numerical = \" + str(list(df._get_numeric_data().columns )))\n    print(\"Categorical = \" + str(list(df.drop(df._get_numeric_data().columns, axis=1).columns)))\n\n\n\n\nCode\nnumericategoric(df)\n\n\nTotalNumericalData = 4\nTotalCategoricalData = 18\nNumerical = ['tenure', 'MonthlyCharges', 'TotalCharges', 'TotalBenefits']\nCategorical = ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'Churn', 'Services']\n\n\nShow numerical and categorical columns\n\n\nCode\n#assign categorical and numerical columns on different dataframe for easier analysis.\ndfnum = df._get_numeric_data()\ndfcat = df.drop(columns = dfnum.columns)"
  },
  {
    "objectID": "portfolios/telco_churn/index.html#basic-exploratory-data-analysis.",
    "href": "portfolios/telco_churn/index.html#basic-exploratory-data-analysis.",
    "title": "Telco customer churn analysis",
    "section": "Basic Exploratory Data Analysis.",
    "text": "Basic Exploratory Data Analysis.\n\nDescriptive Statistics\n\n\nCode\n#numerical columns describe.\ndfnum.describe()\n\n\n\n\n\n\n\n\n\ntenure\nMonthlyCharges\nTotalCharges\nTotalBenefits\n\n\n\n\ncount\n7012.000000\n7012.000000\n7012.000000\n7012.000000\n\n\nmean\n32.506560\n64.732760\n2286.410207\n2.042356\n\n\nstd\n24.564234\n30.109753\n2265.759401\n1.835886\n\n\nmin\n1.000000\n12.000000\n13.500000\n0.000000\n\n\n25%\n9.000000\n35.450000\n402.437500\n0.000000\n\n\n50%\n29.000000\n70.300000\n1397.250000\n2.000000\n\n\n75%\n56.000000\n89.850000\n3784.125000\n3.000000\n\n\nmax\n140.000000\n118.750000\n8684.800000\n6.000000\n\n\n\n\n\n\n\n1.All columns seems to have a normal min-max values. Nothing weird here. 2.Average tenure is about 30 months which is pretty low. 3.Average MonthlyCharge is about 65-70 USD which is pretty good. 4.Out of 6 benefits available, the average taken by customer is around 2, which is pretty low.\n\n\nCode\ndfcat.describe()\n\n\n\n\n\n\n\n\n\ngender\nSeniorCitizen\nPartner\nDependents\nPhoneService\nMultipleLines\nInternetService\nOnlineSecurity\nOnlineBackup\nDeviceProtection\nTechSupport\nStreamingTV\nStreamingMovies\nContract\nPaperlessBilling\nPaymentMethod\nChurn\nServices\n\n\n\n\ncount\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n7012\n\n\nunique\n2\n2\n2\n2\n2\n3\n3\n3\n3\n3\n3\n3\n3\n3\n2\n4\n2\n3\n\n\ntop\nMale\nNo\nNo\nNo\nYes\nNo\nFiber optic\nNo\nNo\nNo\nNo\nNo\nNo\nMonth-to-month\nYes\nElectronic check\nNo\nBoth\n\n\nfreq\n3542\n5874\n3624\n4920\n6336\n3372\n3087\n3486\n3075\n3066\n3459\n2798\n2769\n3850\n4155\n2354\n5155\n4821\n\n\n\n\n\n\n\n1.Male and Female has the same proportion. 2.Most of customers is non SeniorCitizen with no Partner and No Dependents. 3.Favorite InternetService is Fiber optic. 4.Majority of customers is subscribed with ‘Month-to-month’ Contract. 5.5155 out of 7012 is non-Churn customers which make this dataset is imbalanced. 6.All these columns have low cardiality values.\n\n\nUnivariate Analysis\n\n\nCode\nfig, axarr = plt.subplots(1,4, figsize=(10, 4))\nfor x in dfnum.columns:\n    axarr[dfnum.columns.get_loc(x)].boxplot(df[x],patch_artist=True)\n    axarr[dfnum.columns.get_loc(x)].set_xlabel(x)\nplt.suptitle('Outliers checking on numeric columns')\nfig.tight_layout(pad=1)\nplt.show()\n\n\n\n\n\nWill drop outlier in tenure.\n\n\nCode\n#drop outlier in tenure\ndf = df[df.tenure &lt; 125]\n\n\n\n\nCode\n#plot distribution for numerical columns\nfig, axarr = plt.subplots(1,4, figsize=(10, 4))\nfor x in dfnum.columns:\n    sns.histplot(data=dfnum[x], \n                 color='skyblue', \n                 kde=True, \n                 edgecolor='none', \n                 ax=axarr[dfnum.columns.get_loc(x)])\nplt.suptitle('Distribution plot', weight='bold')\nfig.tight_layout(pad=1)\n\n\n\n\n\nAbove plots are distribution plots on all numerical columns. 1.tenure and MonthlyCharges have a ‘U-shaped’ distribution. 2.TotalCharges has a positive-skew distribution.\n\n\nCode\n#count plot for categorical columns\nplt.figure(figsize=(15,12))\n\nfeatures = dfcat.columns\nfor i in np.arange(1, len(features)+1):\n    plt.subplot(5, len(features)//3 - 2, i)\n    sns.countplot(x=df[features[i-1]], color='green')\n    plt.xticks(rotation=10)\n    plt.xlabel(features[i-1])\nplt.suptitle('CountPlot', size=19, weight='bold')\nplt.tight_layout(pad = 1)\n\n\n\n\n\nHere I count plotted all categorical columns.\n\n\nMultivariate Analysis\n\n\nCode\n#count plots against 'churn'\nplt.figure(figsize=(15,12))\n\nfeatures = dfcat.columns\nfor i in np.arange(1, len(features)+1):\n    plt.subplot(5, len(features)//3 - 2, i)\n    sns.countplot(data=df, x=df[features[i-1]], hue='Churn')\n    plt.legend(prop={'size': 8})\n    plt.xticks(rotation=10)\n    plt.xlabel(features[i-1])\nplt.suptitle('CountPlot vs Churned', size=19, weight='bold')\nplt.tight_layout(pad = 1)\n\n\n\n\n\nI added Churn count into the categorical plots. 1.You can see for column Gender, the values and Churn count is pretty equal which make this column will have a very low predictive power. 2.For InternetService, fiber optic has way higher in churn probability compare to DSL. 3.Same ways also applied on Month-to-month Contract and Electronic-check PaymentMethod.\n\n\nCode\n#distribution plots against 'churn'\nfig, axarr = plt.subplots(1,4, figsize=(10, 4))\nfor x in dfnum.columns:\n    sns.histplot(data=df, \n                 x = dfnum[x],\n                 color='skyblue', \n                 kde=True, \n                 edgecolor='none', \n                 ax=axarr[dfnum.columns.get_loc(x)], \n                 hue='Churn')\n    \nplt.suptitle(\"Distribution plot\", weight='bold')\nfig.tight_layout(pad=1)\n\n\n\n\n\nLet’s also compare Churn distribution on numerical columns. Customers tend to churn when the tenure is low and not churn when the MonthlyCharges is very low. I will do further analysis about these columns later.\n\n\nCode\n#change binary column into numerical\nbinary = ['gender','SeniorCitizen','Partner','Dependents','PhoneService','PaperlessBilling','Churn']\nvalue_mapping = {\n    'No': 0,\n    'Yes' : 1,\n    'Male' : 1,\n    'Female' : 0\n}\ndfcorr = df[binary]\nfor col in binary:\n    dfcorr[col] = dfcorr[col].map(value_mapping).astype('int64')\n\n\n\n\nCode\nplt.figure(figsize=(10,8))\nsns.heatmap(dfcorr.corr(), annot=True, fmt='.2f')\nplt.show()\n\n\n\n\n\nCorrelation! 1.TotalCharges and tenure have a high positive correlation (causation : the longer the customers subscribed, the more they paid). 2.TotalBenefits also has a strong correlation with MonthlyCharges and TotalCharges (causation : more benefits taken also make the MonthlyCharges higher). Note : Correlation doesn’t indicate causation. Understanding the specific context, industry knowledge, and conducting further analysis or experiments can help determine if there is a causal relationship between the variables or if other factors are influencing the observed correlations."
  },
  {
    "objectID": "portfolios/telco_churn/index.html#deep-dive-exploratory-data-analysis",
    "href": "portfolios/telco_churn/index.html#deep-dive-exploratory-data-analysis",
    "title": "Telco customer churn analysis",
    "section": "Deep-Dive Exploratory Data Analysis",
    "text": "Deep-Dive Exploratory Data Analysis\n\n\nCode\n#create a function to plot churn probability for numerical columns.\ndef prob_plot(df,colom,x):\n    means = df[colom].mean()\n    medians = df[colom].median()\n    data = df[df.Churn == 'Yes'][colom].astype('float64')\n    data1 = df[df.Churn == 'No'][colom].astype('float64')\n    \n    kde = gaussian_kde(data)\n    kde1 = gaussian_kde(data1)\n    dist_space = np.linspace( min(data), max(data), 200)\n    dist_space1 = np.linspace( min(data1), max(data1), 200)\n    axarr[x].plot( dist_space, kde(dist_space), label='Churned', color='orange' )\n    axarr[x].plot( dist_space1, kde1(dist_space1), label='Not churn', color='blue')\n    axarr[x].axvline(x = means, linestyle = '--', color='g', label='Mean')\n    axarr[x].axvline(x = medians, linestyle = '--', color='r', label='Median')\n    axarr[x].set_title('Probability', fontweight='bold', size=12)\n    axarr[x].set(ylabel = 'Probability', xlabel = colom)\n    axarr[x].legend()\n\n\n\nServices & InternetService analysis\n\n\nCode\n#count plot for Services.\nfig, axarr = plt.subplots(1,2, figsize=(12, 6))\nsns.countplot(data=df[df.Services == 'Both'], \n              x='InternetService', \n              ax=axarr[0])\n\nsns.countplot(data=df[df.Services == 'Internet Only'], \n              x='InternetService', \n              ax=axarr[1])\n\naxarr[0].set_title(\"Both phone service & internet service\", weight='bold')\naxarr[1].set_title(\"Internet service only\", weight='bold')\n\nplt.suptitle(\"Comparison of internet services on product services\")\nplt.tight_layout(pad=1)\nplt.show()\n\n\n\n\n\nIt is observed that customers tend to prefer Fiber optic over DSL for ‘phone & internet service’. However, when considering ‘internet service only’ without phone, there is no option for Fiber optic available. This suggests that in order to utilize Fiber optic, a phone connection (or phone service) is required.\n\n\nCode\n#plot churn probability for 'services' & 'internet services'\nfig, axarr = plt.subplots(1,2, figsize=(12, 6))\n\ndf.groupby('Services')['Churn'].agg(lambda x: list(x).count('Yes') * 100 / len(list(x)))\\\n                               .plot(kind='bar', width=0.3,rot=True, ax=axarr[0])\n\ndf[df.Services != 'Phone Only'].groupby(['InternetService'])['Churn'].agg(lambda x: list(x).count('Yes') * 100 / len(x))\\\n                               .plot(kind='bar', width=0.3,rot=True, ax=axarr[1])\n\naxarr[0].set_title('Services churn probability', weight='bold')\naxarr[1].set_title('Internet service churn probability', weight='bold')\n\nplt.tight_layout(pad=1)\nplt.show()\n\n\n\n\n\nCustomers that used ‘Both’ services and InternetService fiber optic tends to churn more.\n\n\nMonthlyCharges analysis\n\n\nCode\n#checking InternetService price.\ndf_filtered = df[(df.Services == 'Both') & (df.TotalBenefits == 0) & (df.MultipleLines == 'No')].copy()\ndf_filtered.groupby('InternetService')['MonthlyCharges'].mean()\n\n\nInternetService\nDSL            44.965089\nFiber optic    70.074454\nName: MonthlyCharges, dtype: float64\n\n\n\nThe price of Fiber optic is higher, around 25 USD, compared to DSL. However, it’s important to keep in mind that these prices include a phone service with a single line.\n\n\n\nCode\ndf_filtered = df[(df.Services == 'Phone Only') & (df.TotalBenefits == 0)].copy()\ndf_filtered.groupby('MultipleLines')['MonthlyCharges'].mean()\n\n\nMultipleLines\nNo     19.958088\nYes    24.980060\nName: MonthlyCharges, dtype: float64\n\n\n\n\nCode\nplt.figure(figsize=(10,5))\nsns.histplot(data=df_filtered, \n             x='MonthlyCharges', \n             hue='MultipleLines', \n             multiple='stack')\n\nplt.title('Phone service distribution', weight='bold')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nWe can see that the price for a ‘phone service’ with a single line is around 20 USD, while the price for a ‘phone service’ with multiple lines is around 25 USD. This also means that the price for DSL is around 25 USD, while the price for Fiber optic is around 50 USD, which is twice as much as DSL.\n\n\nCode\nfig, axarr = plt.subplots(1,2, figsize=(12, 6))\nmc_dist = sns.histplot(data=df, \n                       x = 'MonthlyCharges',\n                       hue='Churn', \n                       ax=axarr[0], \n                       multiple='stack')\n\naxarr[0].set_title('Distribution', fontweight='bold', size=12)\n\nprob_plot(df,'MonthlyCharges',1)\naxarr[1].legend(loc='upper right')\nplt.show()\n\n\n\n\n\nAt a MonthlyCharges range of approximately +- 20 USD, the ratio of non-churn customers is very high. It is known that products within this price range are typically ‘phone service only’. However, between the price range of 60 - 100 USD, the churn probability increases significantly. I am planning to conduct further analysis specifically for customers within this price range.\n\n\nCode\ndf_filtered = df[(df.InternetService == 'Fiber optic') & (df.Services == 'Both') & (df.MultipleLines == 'No')]\ndf_agg = df_filtered.groupby('TotalBenefits')['MonthlyCharges'].agg('mean').reset_index()\ndf_agg['MonthlyCharges'] = df_agg.MonthlyCharges.round()\n\n\n\n\nCode\nfig, axarr = plt.subplots(1,2, figsize=(12, 6))\nsns.scatterplot(data=df_filtered, \n                x='MonthlyCharges', \n                y='TotalBenefits', \n                s=35, \n                ax=axarr[0])\n\nsns.barplot(df_agg, x = 'TotalBenefits', y = 'MonthlyCharges')\naxarr[0].set_title('MonthlyCharges vs Totalbenefits', weight='bold')\naxarr[1].set_title('Average MonthlyCharges vs Totalbenefits', weight='bold')\nfig.tight_layout(pad = 1)\nplt.show()\n\n\n\n\n\nHere, you can observe that as more TotalBenefits are taken, the MonthlyCharges also increase. On the left plot, you can see that there are 5 outlier data points, which will be removed later.\n\n\nCode\ndf_fil = df[(df.InternetService == 'Fiber optic') & (df.Services == 'Both')\\\n              & (df.MultipleLines == 'No') & (df.TotalBenefits == 1)]\n\nbenefits = ['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']\n\ndf_benefit = pd.DataFrame()\nfor x in benefits:\n    df_value = pd.DataFrame([x, \n                      df_fil[df_fil[x] == 'Yes']['MonthlyCharges'].min(), \n                      df_fil[df_fil[x] == 'Yes']['MonthlyCharges'].max(), \n                      df_fil[df_fil[x] == 'Yes']['MonthlyCharges'].mean(), \n                      df_fil[df_fil[x] == 'Yes']['MonthlyCharges'].median()\n                     ]).transpose()\n    \n    df_benefit = pd.concat([df_benefit, df_value])\ndf_benefit.columns = ['Benefit','MinCharges','MaxCharges','MeanCharges','MedianCharges']    \n\n\n\n\nCode\ndf_benefit\n\n\n\n\n\n\n\n\n\nBenefit\nMinCharges\nMaxCharges\nMeanCharges\nMedianCharges\n\n\n\n\n0\nOnlineSecurity\n73.2\n76.45\n74.795588\n74.95\n\n\n0\nOnlineBackup\n72.75\n76.65\n74.708511\n74.65\n\n\n0\nDeviceProtection\n69.55\n76.65\n74.604762\n74.9\n\n\n0\nTechSupport\n73.85\n76.55\n75.045455\n74.7\n\n\n0\nStreamingTV\n77.65\n81.9\n79.749242\n79.825\n\n\n0\nStreamingMovies\n12.0\n86.1\n79.045902\n80.15\n\n\n\n\n\n\n\nAbove are the prices of benefits with Fiber optic and a single line phone connection. You can see that StreamingTV and StreamingMovies are more expensive compared to other benefits, approximately +- 5 USD.\nNow that we know the prices of every product, here’s a recap:\nDSL = approximately 25 USD. Fiber optic = approximately 50 USD. Phone service (single line) = approximately 20 USD. Phone service (multiple lines) = approximately 25 USD. OnlineSecurity - TechSupport = approximately 5 USD. StreamingTV - StreamingMovies = approximately 10 USD.\nWith this data, we can perform a simple ‘anomaly detection’ by manually calculating the MonthlyCharges and comparing them with the actual MonthlyCharges, similar to how we calculated the TotalChargesDiff above.\n\n\nCode\n#checking for MonthlyCharges values with the calculated one (similar with checking TotalCharges difference).\ndef MonthlyChargesDiff(x):\n    estimation = 0\n    if x['PhoneService'] == 'Yes':\n        estimation += 20\n    if x['MultipleLines'] == 'Yes':\n        estimation += 5\n    if x['InternetService'] == 'DSL':\n        estimation += 25\n    if x['InternetService'] == 'Fiber optic':\n        estimation += 50\n        \n    if (x['StreamingTV'] == 'Yes') & (x['StreamingMovies'] == 'Yes'):\n        estimation += 20 + (x['TotalBenefits'] - 2) * 5\n    elif (x['StreamingTV'] == 'Yes') | (x['StreamingMovies'] == 'Yes'):\n        estimation += 10 + (x['TotalBenefits'] - 1) * 5\n    else:\n        estimation += x['TotalBenefits'] * 5\n        \n    return abs(1 - (estimation / x['MonthlyCharges'])) * 100\n    \n   \n\n\n\n\nCode\ndf['MonthlyChargesEstimationDifference'] = df.apply(MonthlyChargesDiff, axis=1)\n\n\n\n\nCode\ndf[df.MonthlyChargesEstimationDifference &gt; 40][['MonthlyCharges','MonthlyChargesEstimationDifference']]\n\n\n\n\n\n\n\n\n\nMonthlyCharges\nMonthlyChargesEstimationDifference\n\n\n\n\n12\n29.00\n296.551724\n\n\n389\n12.00\n733.333333\n\n\n666\n12.00\n566.666667\n\n\n859\n26.41\n278.644453\n\n\n1439\n18.26\n447.645126\n\n\n2185\n21.63\n362.320851\n\n\n4090\n31.26\n219.897633\n\n\n5848\n15.00\n466.666667\n\n\n6718\n21.00\n304.761905\n\n\n\n\n\n\n\nYou can see that there are 9 rows with extreme MonthlyCharges values. These are considered as ‘anomalies’, so let’s remove them.\n\n\nCode\n#remove MonthlyCharges extreme values.\ndf = df[df.MonthlyChargesEstimationDifference &lt; 40].reset_index(drop=True)\n\n\n\n\nCustomer analysis\n\n\nCode\n#creating a function to engineered a new feature.\ndef statuss(x):\n    x = list(x)\n    if (x[0] == 'Yes') & (x[1] == 'Yes'):\n        return 'Both'\n    elif (x[0] == 'Yes') & (x[1] == 'No'):\n        return 'Partner Only'\n    elif (x[0] == 'No') & (x[1] == 'Yes'):\n        return 'Dependent Only'\n    else:\n        return 'Single'\n\n\n\n\nCode\ndf['Status'] = df[['Partner','Dependents']].apply(statuss, axis=1)\n\n\nI have created a new feature called ‘Status’. This feature is derived from the columns Partner and Dependents. 1.If customers have both Partner and Dependents, it will be labeled as ‘Both’. 2.If customers have Partner but no Dependents, it will be labeled as ‘Partner Only’. 3.If customers have Dependents but no Partner, it will be labeled as ‘Dependent Only’. 4.If customers have neither Partner nor Dependents, it will be labeled as ‘Single’.\n\n\nCode\nplt.figure(figsize=(10,5))\nsns.countplot(df.sort_values('Status', ascending=True), x='Status')\nplt.title('Status Count', size=16, weight='bold')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nMajority of customers are Single.\n\n\nCode\nfig, axarr = plt.subplots(1,3, figsize=(15, 6))\nk = ['tenure','MonthlyCharges','TotalBenefits']\nfor x in k:\n    sns.barplot(data=df.groupby(['Status'])[[x]].mean().reset_index(), \n                x='Status', \n                y=x, \n                ax=axarr[k.index(x)],\n                palette=['grey', 'g','m','b'])\n    \n    axarr[k.index(x)].set_title(f'{x} average', weight='bold', size=15)\n    \nfig.tight_layout()\nplt.show()\n\n\n\n\n\nCustomers labeled as ‘Partner Only’ are considered the best since they have the longest tenure and the highest MonthlyCharges. The second-best group is ‘Both’, although these customers may not have MonthlyCharges as high as those in the ‘Single’ group, their tenure is almost double that of the ‘Single’ group.\n\n\nCode\nplt.figure(figsize=(10,5))\nsns.barplot(data=df.groupby('Status')[['Churn']].agg(lambda x: list(x).count('Yes') / len(x)).reset_index(), \n            x='Status', \n            y='Churn')\n\nplt.title('Churn Probability', weight='bold', size=16)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nSingle customers have the highest churn probability.\n\n\nCode\ndf.SeniorCitizen.value_counts(normalize=True)\n\n\nSeniorCitizen\nNo     0.837903\nYes    0.162097\nName: proportion, dtype: float64\n\n\nThe majority of customers are young people.\n\n\nCode\ndf_status = df.groupby('SeniorCitizen')[['Status']]\ndf_status = df_status.agg(Single = ('Status', lambda x: list(x).count('Single') * 100 / len(x)), \n                          PartnerOnly = ('Status', lambda x: list(x).count('Partner Only') * 100  / len(x)), \n                          Both = ('Status', lambda x: list(x).count('Both') * 100  / len(x)), \n                          DependentOnly = ('Status', lambda x: list(x).count('Dependent Only') * 100  / len(x)))\ndf_status = df_status.reset_index().melt(id_vars='SeniorCitizen')\ndf_status = df_status.rename(columns={'variable':'Status'})\n\n\n\n\nCode\nplt.figure(figsize=(10,5))\n\nsns.barplot(data=df_status, \n            x ='SeniorCitizen', \n            y='value', \n            hue='Status')\n\nplt.title(\"Status comparison between senior citizen\", size=15, weight='bold')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nFor non-Senior citizens, ‘Single’ customers have the highest frequency, followed by ‘Both’. For Senior citizens, ‘Single’ is also the highest category, but the difference with ‘PartnerOnly’ is not as significant. From the plots above, we can also conclude that young people tend to have dependents more than older people.\n\n\nCode\ndf_status = df.groupby('Status')[['OnlineSecurity','OnlineBackup',\n                                  'DeviceProtection','TechSupport','StreamingTV','StreamingMovies']]\\\n                                  .agg(lambda x: list(x).count('Yes'))\n    \ndf_status['total'] = df_status.apply('sum',axis=1)\n\nfor x in df_status.drop(columns='total').columns:\n    df_status[x] = (df_status[x] * 100 / df_status.total).round()\ndf_status.drop(columns='total', inplace=True)\n\n\n\n\nCode\ndf_status.plot(kind='bar', rot=0)\nplt.title('Benefit count comparison between status (%)', size=12, weight='bold')\nplt.legend(bbox_to_anchor=(1, 1))\nplt.show()\n\n\n\n\n\nSingle and PartnerOnly customers tend to prefer entertainment benefits such as StreamingTV and StreamingMovies compared to other customers. Additionally, these customers show a lower preference for using TechSupport and OnlineSecurity.\n\n\nBenefits analysis\n\n\nCode\nfig, axarr = plt.subplots(1,2, figsize=(12, 6))\nsns.countplot(df[df.Services != 'Phone Only'], \n              x = 'TotalBenefits', ax=axarr[0])\n\nsns.barplot(df[['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']]\\\n            .apply(lambda x: list(x).count('Yes')).reset_index(), x = 'index', y = 0, ax=axarr[1])\n\naxarr[0].set_title('TotalBenefits Count', size=12, weight='bold')\naxarr[0].set(ylabel=None)\n\naxarr[1].set(ylabel=None)\naxarr[1].set_title('Benefits Count', size=12, weight='bold')\naxarr[1].tick_params(axis='x', rotation=25)\nfig.tight_layout()\nplt.show()\n\n\n\n\n\nThe average number of TotalBenefits taken by customers is around 3, with StreamingTV and StreamingMovies being the most popular choices.\n\n\nCode\ndf_total = pd.DataFrame()\nfor x in ['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']:\n    total = list(df[df[x] == 'Yes']['Churn']).count('Yes') / len(df[df[x] == 'Yes']['Churn'])\n    df_total = pd.concat([df_total, pd.DataFrame([x],[total])])\n\n\n\n\nCode\nfig, axarr = plt.subplots(1,2, figsize=(12, 6))\nsns.barplot(df.groupby('TotalBenefits')[['Churn']]\\\n            .agg(lambda x: list(x).count('Yes') * 100 / len(x)).round().reset_index(), \n            x='TotalBenefits', \n            y='Churn', ax=axarr[0])\n\nsns.barplot(df_total.reset_index(), \n            x=0, \n            y='index', \n            ax=axarr[1])\n\naxarr[0].set_title('TotalBenefits Churn probability', size=12, weight='bold')\naxarr[0].set(ylabel=None)\n\naxarr[1].set(ylabel=None, xlabel='Benefits')\naxarr[1].set_title('Benefits Churn probability', size=12, weight='bold')\naxarr[1].tick_params(axis='x', rotation=25)\nfig.tight_layout()\nplt.show()\n\n\n\n\n\nWhile StreamingTV and StreamingMovies are the most favored choices, the churn probability associated with them is also the highest.\n\n\nChurn analysis\n\n\nCode\nfig, axarr = plt.subplots(1, figsize=(10, 6))\nsns.scatterplot(data=df, \n                x = 'MonthlyCharges', \n                y = 'tenure', \n                hue ='Churn', \n                s=20)\n\nplt.fill_between((68 , 97),20, alpha=0.2, color='green')\nplt.fill_between((68 , 97),52.5, 72.5, alpha=0.2, color='blue')\n\nplt.title('MonthlyCharges vs tenure vs Churn', size = 15, weight = 'bold')\nplt.show()\n\n\n\n\n\nHere, I have created two areas, denoted by green and blue, both focusing on MonthlyCharges in the range of 70 - 95 USD. This price range corresponds to the highest churn probability. The green area represents customers with low tenure and is predominantly occupied by churned customers, while the blue area represents customers with high tenure and is predominantly occupied by non-churned customers.\n\n\nCode\nplt.subplots(1, figsize=(15, 8))\nsns.scatterplot(data=df, \n                x = 'MonthlyCharges', \n                y = 'tenure', \n                s=35, \n                hue='TotalBenefits', \n                style='InternetService', \n                palette='coolwarm')\n\nplt.fill_between((68 , 97),20, alpha=0.15, color='green')\nplt.fill_between((68 , 97),52.5, 72.5, alpha=0.15, color='blue')\n\nplt.title('MonthlyCharges vs tenure vs TotalBenefits vs InternetService', size = 15, weight = 'bold')\nplt.show()\n\n\n\n\n\nStill on the same plot, I have added TotalBenefits and InternetService. It can be observed that in the green area, Fiber optic is the dominant InternetService with low TotalBenefits. On the other hand, the blue area is dominated by DSL with high TotalBenefits. This indicates that customers, at the same price point, tend to choose DSL with high TotalBenefits rather than Fiber optic with low TotalBenefits. Note that Fiber optic prices are doubled than DSL.\nWith the observed pattern above, we can create an important new feature, which we will refer to as ‘FO_LB’ (Fiber optic_Low benefit). I will assign a value of ‘1’ to indicate that the internet service is Fiber optic and the Totalbenefits taken are less than or equal to 3. For other cases, I will assign ‘0’.\n\n\nCode\ndf['FO_LB'] = df[['InternetService','TotalBenefits']].apply(\n    lambda x: 1 if (x['InternetService'] == 'Fiber optic') & (x['TotalBenefits'] &lt;= 3) else 0, axis=1)"
  },
  {
    "objectID": "portfolios/telco_churn/index.html#modelling",
    "href": "portfolios/telco_churn/index.html#modelling",
    "title": "Telco customer churn analysis",
    "section": "Modelling",
    "text": "Modelling\nOne important thing to address before we proceed is considering the types of errors to make this project as realistic as possible. Typically, there are two types of errors: false positive (FP) and false negative (FN). However, in this project, I will introduce three types of errors.\n1.FP: False positive  2.FN1: False negative for customers with MonthlyCharges below 95 USD  3.FN2: False negative for customers with MonthlyCharges above 95 USD (VIP customers)  Let’s agree on the misclassification ratio, which is FP:FN1:FN2 = 1:3:5 \nIt’s important to note that this dataset is imbalanced, meaning there is a significant difference in the number of samples between the classes.\nBased on these problems, we can set up our model’s parameters as follows: 1.Hyperparameter tuning. 2.Decision threshold tuning. 3.Oversampling data using SMOTE. 4.Applying weights to the models. I will be using Random Forest, XGBoost, and Logistic Regression.\nMetrics: Custom scoring based on sample misclassification. Precision. Recall. F1_score. Once the models are evaluated using these metrics, I will interpret the best model.\n\nFeature selection & encoding\n\n\nCode\ndf1 = df.copy()\n\n\n\n\nCode\ndf = df1.copy()\n\n\n\n\nCode\n#drop columns\ndf.drop(columns = ['Services','MonthlyChargesEstimationDifference','Status','PaperlessBilling','PaymentMethod'], inplace=True)\n\n\nI have dropped the columns from ‘Services’ to ‘Status’ as these columns were engineered features created for simpler exploratory data analysis (EDA). Additionally, I have also dropped the ‘PaperlessBilling’ and ‘PaymentMethod’ columns because, in the business context, these columns are considered irrelevant for determining customer churn since they represent optional ‘features’ for customers.\n\n\nCode\n#converting 'No internet service' to 'No' in benefit columns.\nKolomBenefit = ['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']\n\nfor x in KolomBenefit:\n    df[x] = df[x].apply(lambda x: 'No' if x == 'No internet service' else x)\n\n\n\n\nCode\n#converting 'No phone service' to 'No'\ndf['MultipleLines'] = df[x].apply(lambda x: 'No' if x == 'No phone service' else x)\n\n\n\n\nCode\n#dict to mapping string to numerical.\nvalue_mapping = {\n    'No': 0,\n    'Yes' : 1,\n    'Male' : 1,\n    'Female' : 0\n}\n\n\n\n\nCode\n#binary encoding\nbinary = list(df.drop(columns=['tenure','InternetService','MonthlyCharges','TotalCharges','TotalBenefits','Contract','FO_LB']).columns)\n\nfor col in binary:\n    df[col] = df[col].map(value_mapping).astype('int64')\n\n\n\n\nCode\n#label encoding\ndf['Contract'] = df['Contract'].apply(lambda x: 1 if x == 'Month-to-month' else 2 if x == 'One year' else 3) \n\n\n\n\nCode\n#one hot encoding\ndf = pd.get_dummies(df, columns=['InternetService'])\n\n\n\n\nCode\n#feature selection\ndf = df[['Contract','tenure','InternetService_Fiber optic','MonthlyCharges','FO_LB','InternetService_No','Churn']]\ndf = df.rename(columns={\n                'InternetService_Fiber optic':'Fiber_optic',\n                'InternetService_No':'No_internet'})\n\n\nHere I only choose a feature that have a strong predictive power (by using feature of importances)\n\n\nSplits data and define custom function\n\n\nCode\n#split train-test\nX_train, X_test, y_train, y_test = train_test_split(df.drop(columns='Churn'), df.Churn.to_numpy(), test_size = 0.2, random_state=123)\nX_train = X_train.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)\n\n\n\n\nCode\n#custom function to do threshold tuning and custom metrics (used in GridSearchCV)\ndef my_scorer_2(clf, X, y_true, thres = np.arange(0.1,1,0.1)):\n    result_dict = {}\n    for threshold in np.atleast_1d(thres):\n        y_pred = (clf.predict_proba(X)[:,1] &gt; threshold).astype(int)\n        X_segment = (X['MonthlyCharges'] &gt; 95).to_numpy().astype(int)\n        y_stack = np.column_stack((X_segment, y_pred, y_true))\n        y_stack_reg, y_stack_vip = y_stack[y_stack[:,0] == 0], y_stack[y_stack[:,0] == 1]\n        cm_reg = confusion_matrix(y_stack_reg[:,2], y_stack_reg[:,1])\n        cm_vip = confusion_matrix(y_stack_vip[:,2], y_stack_vip[:,1])\n        fn_reg, fn_vip = cm_reg[1][0], cm_vip[1][0]\n        fp = cm_reg[0][1] + cm_vip[0][1]\n        loss_score = (fp * 1) + (fn_reg * 3) + (fn_vip * 5)\n        result_dict[threshold] = np.array([loss_score, metrics.precision_score(y_true, y_pred, zero_division = 0), \n                                           metrics.recall_score(y_true, y_pred), metrics.f1_score(y_true, y_pred)])\n        \n    result_np = np.array([np.insert(value, 0, key) for key, value in result_dict.items()])\n    best_np = result_np[result_np[:,1] == np.min(result_np[:,1])][0]\n    return best_np\n\ndef my_scorer_threshold(clf, X, y_true):\n    return my_scorer_2(clf, X, y_true)[0]\n\ndef my_scorer_ls(clf, X, y_true):\n    return my_scorer_2(clf, X, y_true)[1]\n\ndef my_scorer_precision(clf, X, y_true):\n    return my_scorer_2(clf, X, y_true)[2]\n\ndef my_scorer_recall(clf, X, y_true):\n    return my_scorer_2(clf, X, y_true)[3]\n\ndef my_scorer_f1(clf, X, y_true):\n    return my_scorer_2(clf, X, y_true)[4]\n        \n        \n\n\n\n\nCode\n#Grid scoring parameter\ngrid_scoring = {\n    'threshold': my_scorer_threshold,\n    'loss_score': my_scorer_ls,\n    'precision': my_scorer_precision,\n    'recall': my_scorer_recall,\n    'f1': my_scorer_f1\n}\n\n\n\n\nCode\n#define weight by missclassification cost which is FP:FN1:FN2 = 1:3:5\nX_segment = (X_train['MonthlyCharges'] &gt; 95).to_numpy().astype(int)\narr_weight = np.column_stack((X_segment, y_train))\nweight = np.apply_along_axis(lambda x: 1 if x[1] == 0 else 5 if x[0] == 1 else 3 , axis=1, arr=arr_weight)\n\n\n\n\nModel building 1 / Hyperparameter tuning + threshold tuning\n\n\nCode\n#RANDOM FOREST MODELLING\n\n#define parameter for tuning\nparam_grid_rf = {\n    'n_estimators': [250 , 400],\n    'max_depth': [10, 25, 50],\n    'min_samples_split': [25, 50, 70, 120],\n    'min_samples_leaf': [50, 75, 120],\n    'bootstrap' : [True, False]\n}\n\n#run grid search cv\nrf = GridSearchCV(estimator = RandomForestClassifier(),\n                  param_grid = param_grid_rf,\n                  cv=5,\n                  scoring = grid_scoring, refit=False, n_jobs = -1)\n\n#fit grid search cv with train data\nrf.fit(X_train, y_train)\n\n#selecting the best parameter and metrics\ngrid_result = pd.DataFrame(rf.cv_results_)\ngrid_result = grid_result[grid_result['mean_test_loss_score'] == grid_result['mean_test_loss_score'].min()][['params','mean_test_threshold','mean_test_loss_score','mean_test_precision','mean_test_recall','mean_test_f1']]\n\n#train model with the best hyperparameter\nrf = RandomForestClassifier(**grid_result.iloc[0,0])\n\n#train model with train data\nrf = rf.fit(X_train, y_train)\n\n#evaluate the model\nmb1_rf = my_scorer_2(rf, X_test, y_test, grid_result.iloc[0,1])\nmb1_rf = np.append(mb1_rf, grid_result.iloc[0,0])\n\n\n\n\nCode\n#XGBOOST MODELLING\n\n#define parameter for tuning\nparam_grid_xg = {\n    'learning_rate': [0.1, 0.01, 0.001],\n    'n_estimators': [100, 500],\n    'max_depth': [5, 10, 25],\n    'subsample': [0.8, 0.9, 1.0],\n    'colsample_bytree': [0.8, 0.9, 1.0]\n}\n\n#run grid search cv\nxg = GridSearchCV(estimator = XGBClassifier(),\n                  param_grid = param_grid_xg,\n                  cv=5,\n                  scoring = grid_scoring, refit=False, n_jobs = -1)\n\n#fit grid search cv with train data\nxg.fit(X_train, y_train)\n\n#selecting the best parameter and metrics\ngrid_result = pd.DataFrame(xg.cv_results_)\ngrid_result = grid_result[grid_result['mean_test_loss_score'] == grid_result['mean_test_loss_score'].min()][['params','mean_test_threshold','mean_test_loss_score','mean_test_precision','mean_test_recall','mean_test_f1']]\n\n#train model with the best hyperparameter\nxg = XGBClassifier(**grid_result.iloc[0,0])\n\n#train model with train data\nxg = xg.fit(X_train, y_train)\n\n#evaluate the model\nmb1_xg = my_scorer_2(xg, X_test, y_test, grid_result.iloc[0,1])\nmb1_xg = np.append(mb1_xg, grid_result.iloc[0,0])\n\n\n\n\nCode\n#LOGISTIC REGRESSION MODELLING\n\n#define parameter for tuning\nparam_grid_lg = {\n    'penalty': ['l1', 'l2'], \n    'C': [0.1, 1.0, 10.0],  \n    'solver': ['liblinear'],  \n    'max_iter': [50,100,200] \n}\n\n#run grid search cv\nlg = GridSearchCV(estimator = LogisticRegression(),\n                  param_grid = param_grid_lg,\n                  cv=5,\n                  scoring = grid_scoring, refit=False, n_jobs = -1)\n\n#fit grid search cv with train data\nlg.fit(X_train, y_train)\n\n#selecting the best parameter and metrics\ngrid_result = pd.DataFrame(lg.cv_results_)\ngrid_result = grid_result[grid_result['mean_test_loss_score'] == grid_result['mean_test_loss_score'].min()][['params','mean_test_threshold','mean_test_loss_score','mean_test_precision','mean_test_recall','mean_test_f1']]\n\n#train model with the best hyperparameter\nlg = LogisticRegression(**grid_result.iloc[0,0])\n\n#train model with train data\nlg = lg.fit(X_train, y_train)\n\n#evaluate the model\nmb1_lg = my_scorer_2(lg, X_test, y_test, grid_result.iloc[0,1])\nmb1_lg = np.append(mb1_lg, grid_result.iloc[0,0])\n\n\n\n\nCode\nresult_mb1 = pd.DataFrame([mb1_rf,mb1_xg,mb1_lg])\nresult_mb1.columns = ['threshold','score','precision','recall','f1','params']\nresult_mb1['model'] = ['MB1_RF','MB1_XG','MB1_Log_Reg']\nresult_mb1\n\n\n\n\n\n\n\n\n\nthreshold\nscore\nprecision\nrecall\nf1\nparams\nmodel\n\n\n\n\n0\n0.24\n539.0\n0.473101\n0.828255\n0.602216\n{'bootstrap': True, 'max_depth': 25, 'min_samp...\nMB1_RF\n\n\n1\n0.20\n558.0\n0.449190\n0.844875\n0.586538\n{'colsample_bytree': 0.8, 'learning_rate': 0.0...\nMB1_XG\n\n\n2\n0.22\n541.0\n0.467492\n0.836565\n0.599801\n{'C': 0.1, 'max_iter': 50, 'penalty': 'l2', 's...\nMB1_Log_Reg\n\n\n\n\n\n\n\n\n\nModel building 2 / Hyperparameter tuning + threshold tuning + SMOTE\n\n\nCode\n#RANDOM FOREST MODELLING\n\n#define parameter for tuning\nparam_grid_rf_smote = {\n    'class__n_estimators': [250 , 400],\n    'class__max_depth': [10, 25, 50],\n    'class__min_samples_split': [25, 50, 70, 120],\n    'class__min_samples_leaf': [50, 75, 120],\n    'class__bootstrap' : [True, False]\n}\n\n#create imbalanced pipeline to SMOTE \npipelinerf = Pipeline([\n        ('sampling', SMOTE()),\n        ('class', RandomForestClassifier())])\n\n#run grid search cv\nrf = GridSearchCV(estimator = pipelinerf,\n                  param_grid = param_grid_rf_smote,\n                  cv=5,\n                  scoring = grid_scoring, refit=False, n_jobs = -1)\n\n#fit grid search cv with train data\nrf.fit(X_train, y_train)\n\n#selecting the best parameter and metrics\ngrid_result = pd.DataFrame(rf.cv_results_)\ngrid_result = grid_result[grid_result['mean_test_loss_score'] == grid_result['mean_test_loss_score'].min()][['params','mean_test_threshold','mean_test_loss_score','mean_test_precision','mean_test_recall','mean_test_f1']]\nparams = grid_result.iloc[0,0]\n\nparams_update = {}\nfor key, value in params.items():\n    new_key = key.replace('class__', '')\n    params_update[new_key] = value\nparams = params_update\n    \n#train model with the best hyperparameter\nrf = RandomForestClassifier(**params)\n\n#train model with SMOTE train data\nX_train1, y_train1 = SMOTE().fit_resample(X_train, y_train)\nrf = rf.fit(X_train1, y_train1)\n\n#evaluate the model\nmb2_rf = my_scorer_2(rf, X_test, y_test, grid_result.iloc[0,1])\nmb2_rf = np.append(mb2_rf, params)\n\n\n\n\nCode\n#XGBOOST MODELLING\n\n#define parameter for tuning\nparam_grid_xg_smote = {\n    'class__learning_rate': [0.1, 0.01, 0.001],\n    'class__n_estimators': [100, 500],\n    'class__max_depth': [5, 10, 25],\n    'class__subsample': [0.8, 0.9, 1.0],\n    'class__colsample_bytree': [0.8, 0.9, 1.0]\n}\n\n#create imbalanced pipeline to SMOTE \npipelinexg = Pipeline([\n        ('sampling', SMOTE()),\n        ('class', XGBClassifier())])\n\n#run grid search cv\nxg = GridSearchCV(estimator = pipelinexg,\n                  param_grid = param_grid_xg_smote,\n                  cv=5,\n                  scoring = grid_scoring, refit=False, n_jobs = -1)\n\n#fit grid search cv with train data\nxg.fit(X_train, y_train)\n\n#selecting the best parameter and metrics\ngrid_result = pd.DataFrame(xg.cv_results_)\ngrid_result = grid_result[grid_result['mean_test_loss_score'] == grid_result['mean_test_loss_score'].min()][['params','mean_test_threshold','mean_test_loss_score','mean_test_precision','mean_test_recall','mean_test_f1']]\nparams = grid_result.iloc[0,0]\n\nparams_update = {}\nfor key, value in params.items():\n    new_key = key.replace('class__', '')\n    params_update[new_key] = value\nparams = params_update\n    \n#train model with the best hyperparameter\nxg = XGBClassifier(**params)\n\n#train model with SMOTE train data\nX_train1, y_train1 = SMOTE().fit_resample(X_train, y_train)\nxg = xg.fit(X_train1, y_train1)\n\n#evaluate the model\nmb2_xg = my_scorer_2(xg, X_test, y_test, grid_result.iloc[0,1])\nmb2_xg = np.append(mb2_xg, params)\n\n\n\n\nCode\n#LOGISTIC REGRESSION MODELLING\n\n#define parameter for tuning\nparam_grid_lg_smote = {\n    'class__penalty': ['l1', 'l2'], \n    'class__C': [0.1, 1.0, 10.0],  \n    'class__solver': ['liblinear'],  \n    'class__max_iter': [50,100,200] \n}\n\n\n#create imbalanced pipeline to SMOTE \npipelinelg = Pipeline([\n        ('sampling', SMOTE()),\n        ('class', LogisticRegression())])\n\n#run grid search cv\nlg = GridSearchCV(estimator = pipelinelg,\n                  param_grid = param_grid_lg_smote,\n                  cv=5,\n                  scoring = grid_scoring, refit=False, n_jobs = -1)\n\n#fit grid search cv with train data\nlg.fit(X_train, y_train)\n\n#selecting the best parameter and metrics\ngrid_result = pd.DataFrame(lg.cv_results_)\ngrid_result = grid_result[grid_result['mean_test_loss_score'] == grid_result['mean_test_loss_score'].min()][['params','mean_test_threshold','mean_test_loss_score','mean_test_precision','mean_test_recall','mean_test_f1']]\nparams = grid_result.iloc[0,0]\n\nparams_update = {}\nfor key, value in params.items():\n    new_key = key.replace('class__', '')\n    params_update[new_key] = value\nparams = params_update\n    \n#train model with the best hyperparameter\nlg = LogisticRegression(**params)\n\n#train model with SMOTE train data\nX_train1, y_train1 = SMOTE().fit_resample(X_train, y_train)\nlg = lg.fit(X_train1, y_train1)\n\n#evaluate the model\nmb2_lg = my_scorer_2(lg, X_test, y_test, grid_result.iloc[0,1])\nmb2_lg = np.append(mb2_lg, params)\n\n\n\n\nCode\nresult_mb2 = pd.DataFrame([mb2_rf,mb2_xg,mb2_lg])\nresult_mb2.columns = ['threshold','score','precision','recall','f1','params']\nresult_mb2['model'] = ['MB2_RF','MB2_XG','MB2_Log_Reg']\nresult_mb2\n\n\n\n\n\n\n\n\n\nthreshold\nscore\nprecision\nrecall\nf1\nparams\nmodel\n\n\n\n\n0\n0.42\n557.0\n0.454955\n0.839335\n0.590068\n{'bootstrap': False, 'max_depth': 25, 'min_sam...\nMB2_RF\n\n\n1\n0.44\n546.0\n0.472266\n0.825485\n0.600806\n{'colsample_bytree': 1.0, 'learning_rate': 0.0...\nMB2_XG\n\n\n2\n0.48\n546.0\n0.472178\n0.822715\n0.600000\n{'C': 1.0, 'max_iter': 50, 'penalty': 'l2', 's...\nMB2_Log_Reg\n\n\n\n\n\n\n\nYou see that the threshold on all models are increasing than model building 1 because SMOTE make the models learn more on the positive class.\n\n\nModel building 3 / Hyperparameter tuning + threshold tuning + custom weight\n\n\nCode\n#RANDOM FOREST MODELLING\n\n#define parameter for tuning\nparam_grid_rf = {\n    'n_estimators': [250 , 400],\n    'max_depth': [10, 25, 50],\n    'min_samples_split': [25, 50, 70, 120],\n    'min_samples_leaf': [50, 75, 120],\n    'bootstrap' : [True, False]\n}\n\n#run grid search cv\nrf = GridSearchCV(estimator = RandomForestClassifier(),\n                  param_grid = param_grid_rf,\n                  cv=5,\n                  scoring = grid_scoring, refit=False, n_jobs = -1)\n\n#fit grid search cv with train data\nrf.fit(X_train, y_train, sample_weight = weight)\n\n#selecting the best parameter and metrics\ngrid_result = pd.DataFrame(rf.cv_results_)\ngrid_result = grid_result[grid_result['mean_test_loss_score'] == grid_result['mean_test_loss_score'].min()][['params','mean_test_threshold','mean_test_loss_score','mean_test_precision','mean_test_recall','mean_test_f1']]\n\n#train model with the best hyperparameter\nrf = RandomForestClassifier(**grid_result.iloc[0,0])\n\n#train model with train data\nrf = rf.fit(X_train, y_train, sample_weight = weight)\n\n#evaluate the model\nmb3_rf = my_scorer_2(rf, X_test, y_test, grid_result.iloc[0,1])\nmb3_rf = np.append(mb3_rf, grid_result.iloc[0,0])\n\n\n\n\nCode\n#XGBOOST MODELLING\n\n#define parameter for tuning\nparam_grid_xg = {\n    'learning_rate': [0.1, 0.01, 0.001],\n    'n_estimators': [100, 500],\n    'max_depth': [5, 10, 25],\n    'subsample': [0.8, 0.9, 1.0],\n    'colsample_bytree': [0.8, 0.9, 1.0]\n}\n\n#run grid search cv\nxg = GridSearchCV(estimator = XGBClassifier(),\n                  param_grid = param_grid_xg,\n                  cv=5,\n                  scoring = grid_scoring, refit=False, n_jobs = -1)\n\n#fit grid search cv with train data\nxg.fit(X_train, y_train, sample_weight = weight)\n\n#selecting the best parameter and metrics\ngrid_result = pd.DataFrame(xg.cv_results_)\ngrid_result = grid_result[grid_result['mean_test_loss_score'] == grid_result['mean_test_loss_score'].min()][['params','mean_test_threshold','mean_test_loss_score','mean_test_precision','mean_test_recall','mean_test_f1']]\n\n#train model with the best hyperparameter\nxg = XGBClassifier(**grid_result.iloc[0,0])\n\n#train model with train data\nxg = xg.fit(X_train, y_train, sample_weight = weight)\n\n#evaluate the model\nmb3_xg = my_scorer_2(xg, X_test, y_test, grid_result.iloc[0,1])\nmb3_xg = np.append(mb3_xg, grid_result.iloc[0,0])\n\n\n\n\nCode\n#LOGISTIC REGRESSION MODELLING\n\n#define parameter for tuning\nparam_grid_lg = {\n    'penalty': ['l1', 'l2'], \n    'C': [0.1, 1.0, 10.0],  \n    'solver': ['liblinear'],  \n    'max_iter': [50,100,200] \n}\n\n\n#run grid search cv\nlg = GridSearchCV(estimator = LogisticRegression(),\n                  param_grid = param_grid_lg,\n                  cv=5,\n                  scoring = grid_scoring, refit=False, n_jobs = -1)\n\n#fit grid search cv with train data\nlg.fit(X_train, y_train, sample_weight = weight)\n\n#selecting the best parameter and metrics\ngrid_result = pd.DataFrame(lg.cv_results_)\ngrid_result = grid_result[grid_result['mean_test_loss_score'] == grid_result['mean_test_loss_score'].min()][['params','mean_test_threshold','mean_test_loss_score','mean_test_precision','mean_test_recall','mean_test_f1']]\n\n#train model with the best hyperparameter\nlg = LogisticRegression(**grid_result.iloc[0,0])\n\n#train model with train data\nlg = lg.fit(X_train, y_train, sample_weight = weight)\n\n#evaluate the model\nmb3_lg = my_scorer_2(lg, X_test, y_test, grid_result.iloc[0,1])\nmb3_lg = np.append(mb3_lg, grid_result.iloc[0,0])\n\n\n\n\nCode\nresult_mb3 = pd.DataFrame([mb3_rf,mb3_xg,mb3_lg])\nresult_mb3.columns = ['threshold','score','precision','recall','f1','params']\nresult_mb3['model'] = ['MB3_RF','MB3_XG','MB3_Log_Reg']\nresult_mb3\n\n\n\n\n\n\n\n\n\nthreshold\nscore\nprecision\nrecall\nf1\nparams\nmodel\n\n\n\n\n0\n0.48\n549.0\n0.456193\n0.836565\n0.590420\n{'bootstrap': True, 'max_depth': 25, 'min_samp...\nMB3_RF\n\n\n1\n0.48\n551.0\n0.465409\n0.819945\n0.593781\n{'colsample_bytree': 0.8, 'learning_rate': 0.0...\nMB3_XG\n\n\n2\n0.44\n571.0\n0.433803\n0.853186\n0.575163\n{'C': 0.1, 'max_iter': 50, 'penalty': 'l2', 's...\nMB3_Log_Reg\n\n\n\n\n\n\n\n\n\nCode\nfinal_result = pd.concat([result_mb1, result_mb2, result_mb3]).sort_values('score')\nfinal_result\n\n\n\n\n\n\n\n\n\nthreshold\nscore\nprecision\nrecall\nf1\nparams\nmodel\n\n\n\n\n0\n0.24\n539.0\n0.473101\n0.828255\n0.602216\n{'bootstrap': True, 'max_depth': 25, 'min_samp...\nMB1_RF\n\n\n2\n0.22\n541.0\n0.467492\n0.836565\n0.599801\n{'C': 0.1, 'max_iter': 50, 'penalty': 'l2', 's...\nMB1_Log_Reg\n\n\n1\n0.44\n546.0\n0.472266\n0.825485\n0.600806\n{'colsample_bytree': 1.0, 'learning_rate': 0.0...\nMB2_XG\n\n\n2\n0.48\n546.0\n0.472178\n0.822715\n0.600000\n{'C': 1.0, 'max_iter': 50, 'penalty': 'l2', 's...\nMB2_Log_Reg\n\n\n0\n0.48\n549.0\n0.456193\n0.836565\n0.590420\n{'bootstrap': True, 'max_depth': 25, 'min_samp...\nMB3_RF\n\n\n1\n0.48\n551.0\n0.465409\n0.819945\n0.593781\n{'colsample_bytree': 0.8, 'learning_rate': 0.0...\nMB3_XG\n\n\n0\n0.42\n557.0\n0.454955\n0.839335\n0.590068\n{'bootstrap': False, 'max_depth': 25, 'min_sam...\nMB2_RF\n\n\n1\n0.20\n558.0\n0.449190\n0.844875\n0.586538\n{'colsample_bytree': 0.8, 'learning_rate': 0.0...\nMB1_XG\n\n\n2\n0.44\n571.0\n0.433803\n0.853186\n0.575163\n{'C': 0.1, 'max_iter': 50, 'penalty': 'l2', 's...\nMB3_Log_Reg\n\n\n\n\n\n\n\nAnalysis of how the models performed: 1.RandomForest performs best with hyperparameter tuning and a lower decision threshold. However, this model performs worst when using SMOTE. 2.LogisticRegression performs worst when using ‘sample weighting’. 3.XGBoost performs best when using SMOTE. 4.It’s important to note that all models produce similar results when using their best parameters and conditions. 5.In my opinion, the greatest impact is achieved by using hyperparameter tuning and decision threshold tuning, rather than using SMOTE and weighting techniques."
  },
  {
    "objectID": "portfolios/telco_churn/index.html#model-interpretation",
    "href": "portfolios/telco_churn/index.html#model-interpretation",
    "title": "Telco customer churn analysis",
    "section": "Model Interpretation",
    "text": "Model Interpretation\nIn this section, I want to show you how to interpret a tree-based model, such as Random Forest, so we can have a better understanding of how the model actually works.\n\n\nCode\n#selecting the best parameter for random forest\nrf_param = final_result[final_result['model'] == 'MB1_RF']['params'][0]\nrf_param\n\n\n{'bootstrap': True,\n 'max_depth': 25,\n 'min_samples_leaf': 50,\n 'min_samples_split': 50,\n 'n_estimators': 250}\n\n\n\n\nCode\nrf = RandomForestClassifier(**rf_param)\nrf.fit(X_train, y_train)\n\n\nRandomForestClassifier(max_depth=25, min_samples_leaf=50, min_samples_split=50,\n                       n_estimators=250)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(max_depth=25, min_samples_leaf=50, min_samples_split=50,\n                       n_estimators=250)\n\n\n\nPartial dependence plot (PDP) & Individual conditional expectation (ICE)\nWhat is ICE? It is a plot that shows how a model makes predictions based on changing the value of one or more features, while keeping the values of other features constant. This provides us with more insights and understanding of how the model treats features to make predictions. ICE works per row (or per customer in this case), and PDP is simply the average of ICE.\n\n\nCode\ndef pdp_ice_plot(model, df_test, column, clusters=True):\n    df_test = df_test.copy()\n    pdp_isolate = pdp.PDPIsolate(model = model, df = df_test, \n                      num_grid_points=15,\n                      model_features = df_test.columns, \n                      feature = column, feature_name=column)\n    pdp_isolate.plot(\n                center=True, plot_lines=True,\n                cluster=clusters, n_cluster_centers=5,cluster_method='accurate',\n                plot_pts_dist=False, to_bins=True,\n                show_percentile=False, which_classes=None,\n                figsize=(10,6), dpi=300,\n                ncols=2, plot_params={\"pdp_hl\": True},\n                engine='matplotlib', template='plotly_white')\n    plt.show()\n    \n\n\n\n\nCode\npdp_ice_plot(rf, X_test, 'MonthlyCharges', False)\n\n\nobtain pred_func from the provided model.\n\n\n\n\n\n\n\n\nThe light blue lines represent ICE (Individual Conditional Expectation), and the yellowish blue line represents PDP (Partial Dependence Plot). The X-axis represents MonthlyCharges, while the Y-axis represents the change in prediction probability. At MonthlyCharges of 60.7 USD, you can observe that some customers experience a significant increase in churn probability as the MonthlyCharges increase. However, it is important to note that not all customers have the same response. Some customers are minimally affected, and some may not be affected at all.\n\n\nCode\npdp_ice_plot(rf, X_test, 'MonthlyCharges')\n\n\nobtain pred_func from the provided model.\n\n\n\n\n\n\n\n\nThis is the same plot as above, but I have grouped the ICE into 5 clusters for easier viewing and analysis. You can see that there are some customers who experience a significant increase in churn probability as the MonthlyCharges increase.\n\n\nCode\npdp_ice_plot(rf, X_test, 'tenure')\n\n\nobtain pred_func from the provided model.\n\n\n\n\n\n\n\n\nThe longer the tenure, the lower the churn probability. However, the effect is not the same for all customers. Some customers are greatly affected, while others are barely affected.\n\n\nCode\npdp_ice_plot(rf, X_test, 'Contract')\n\n\nobtain pred_func from the provided model.\n\n\n\n\n\n\n\n\nThe same also goes with Contract. Longer contract means lower churn probability.\n\n\nCode\nX_test_copy = X_test.copy()\npdp_interact = pdp.PDPInteract(model=rf, df=X_test_copy,\n                              num_grid_points=6,\n                              model_features = X_test.columns, \n                              features=['MonthlyCharges','tenure'], \n                              feature_names=['MonthlyCharges','tenure'])\npdp_interact.plot(\n            plot_type=\"grid\", plot_pdp=True,\n            to_bins=True, show_percentile=False,\n            which_classes=None, figsize=(10,6),\n            dpi=300, ncols=2, plot_params=None, \n            engine='matplotlib', template='plotly_white',\n)\nplt.show()\n\n\nobtain pred_func from the provided model.\nobtain pred_func from the provided model.\nobtain pred_func from the provided model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s analyze the combination of MonthlyCharges and tenure. We can observe a spike in churn probability for MonthlyCharges ranging from 56.3 USD to 79.2 USD, particularly for customers with a tenure of less than 7 months.\n\n\nCode\nX_test_copy = X_test.copy()\npdp_interact = pdp.PDPInteract(model=rf, df=X_test_copy,\n                              num_grid_points=10,\n                              model_features = X_test.columns, \n                              features=['tenure','Contract'], \n                              feature_names=['tenure','Contract'])\npdp_interact.plot(\n            plot_type=\"grid\", plot_pdp=True,\n            to_bins=True, show_percentile=False,\n            which_classes=None, figsize=(10,6),\n            dpi=300, ncols=2, plot_params=None, \n            engine='matplotlib', template='plotly_white',\n)\nplt.show()\n\n\nobtain pred_func from the provided model.\nobtain pred_func from the provided model.\nobtain pred_func from the provided model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe model produces similar churn probabilities for customers with a combination of a 2-year contract and low tenure compared to those with a month-to-month contract and medium tenure (24-36 months).\nYou can see how PDP and ICE plots can be very beneficial in understanding how the model utilizes features to make predictions. In the next section, I will demonstrate how to assess the model’s prediction confidence level.\n\n\nConfidence level based on trees’ standard deviation and confidence interval.\nTree-based models like RandomForest make predictions by using the mean of all the trees’ prediction probabilities. However, instead of solely relying on the mean, we can also calculate the standard deviation. A higher standard deviation indicates lower confidence in the predictions. Additionally, we can utilize confidence intervals, such as 95% or even more extreme at 99%.\n\n\nCode\n#extract all trees' prediction probability per row\npredict = np.stack([x.predict_proba(X_test)[:,1] for x in rf.estimators_])\n\n\n\n\nCode\n#assign mean and std. deviation of trees' prediction probability.\ndf_pred = X_test.copy()\ndf_pred['avg'] = np.round(np.mean(predict, axis = 0) * 100, 2)\ndf_pred['std_dev'] = np.round(np.std(predict, axis = 0) * 100, 2)\n\n\n\n\nCode\nplt.figure(figsize=(10,6))\nsns.histplot(df_pred['std_dev'],color='skyblue', kde=True, edgecolor='none')\nplt.title('Standard deviation distribution', weight='bold')\nplt.show()\n\n\n\n\n\nMost of predictions have std.deviation under 10%. Let’s calculate confidence interval with 99%.\n\n\nCode\ndf_pred['CI-99%'] = (2.576 * df_pred['std_dev'] / np.sqrt(len(predict))) * 100 / (df_pred['avg'])\n\n\n\n\nCode\ndf_pred[df_pred.avg &gt; 40].sort_values('CI-99%', ascending=False).head(5)\n\n\n\n\n\n\n\n\n\nContract\ntenure\nFiber_optic\nMonthlyCharges\nFO_LB\nNo_internet\navg\nstd_dev\nCI-99%\n\n\n\n\n852\n2\n7\n1\n94.05\n1\n0\n40.06\n25.08\n10.199818\n\n\n499\n1\n58\n1\n98.70\n1\n0\n41.05\n18.24\n7.239149\n\n\n1186\n1\n59\n1\n101.10\n1\n0\n40.37\n16.73\n6.751699\n\n\n1137\n1\n15\n1\n96.30\n0\n0\n48.14\n19.69\n6.663701\n\n\n1184\n1\n10\n1\n92.50\n0\n0\n48.17\n19.12\n6.466765\n\n\n\n\n\n\n\nLet’s consider the example of row 1. The model predicts a 40% probability of churn for the customer, with a confidence interval of +- 10%. By default, the model’s output indicates that the customer will not churn. However, due to the high confidence interval, it is safer to assume that the customer will churn.\nChecking the standard deviation and confidence interval of the trees is extremely useful, particularly when the cost of ‘False Negative’ is significant and can have severe consequences."
  },
  {
    "objectID": "portfolios/pbi/index.html",
    "href": "portfolios/pbi/index.html",
    "title": "Advanced Power BI dashboard",
    "section": "",
    "text": "Live Dashboard\nThis is my Power BI dashboard about sales analysis on a company called Contoso. Power BI is very interactive dashboard, you can click anything on the dashboard to filter (bar, table, etc). Remember to view the dashboard in full screen by clicking the icon in the bottom right corner.\n\n\n\n\nExplanation\nI will explain every graphs that at the dashboard. But first, let’s take a look at the table being used and its relationships.\n \nThe Store, Customer, Product, and Date tables have a ‘1 to many’ relationship with the Sales table. A quick reminder that in Power BI, relationships also serve as filters (for example, in this case, filtering the Customer table will also filter the Sales table). The Day table is a custom-made table that will be used as a filter in the ‘Delivery Days by Online Store’ graph. Below is the plots explanation:\n\n1. LastYear Sales, Sales and Profit by Month\nThis plot is useful for comparing sales in the filtered year with sales in the same month of the previous year. There is also a profit line to display the profit in the filtered year.\n\n\n2. Sales by Store Country\nThis plot displays the total sales by store country.\n\n\n3. Online store customer order count by Country\nThis plot shows the number of customers based on the country who have purchased the product online.\n\n\n4. Customer buy more than once by Country\nThis plot displays how many customers have purchased the product more than once, or we can consider them as returning customers.\n\n\n5. Total stores\nThis plot shows the number of stores in each country.\n\n\n6. Delivery days by online store\nThis plot displays the performance of online store deliveries in each customer country. For example, a value of 67% for the country ‘Australia’ and on day ‘3’ means that 67% of all product deliveries to Australia were completed within a maximum of 3 days.\n\n\n7. Most sold product\nThis plot displays product sales and the corresponding profit, sorted in order of the most popular or highest-selling products.\n\n\n8. Most sold product by age category\nThis plot shows the sales of the top-selling products based on age categories.\n\n\n\nInsights\nNote: the data obtained for the months of 2017 and 2020 is not complete. In 2017, data for months 1-4 is missing, and in 2020, data for months 4-12 is also missing. Reliable analysis can only be based on the years 2018 and 2019.\n\nThe highest sales were recorded in 2018. The sales trend increased from 2017 to 2018, then decreased until 2020.\nThe highest sales were in the online store, and its sales contribution was 47.69% in 2017, 58.17% in 2018, 64.02% in 2019, and 61.9% in 2020.\nThe highest sales for offline stores are in the United States, and there is a significant sales gap between the United States and other offline stores.\nThe months with the highest sales are January, February, and December, while the lowest sales are in April.\nIn the online store, the highest number of customers with the highest purchases are in the United States, while the lowest number of customers is in Italy.\nThe best-selling product category is computers, with the top-performing subcategories being desktops, laptops, and projectors & screens.\nThe profit distribution across all product categories is fairly balanced, with an approximate range of around 50%.\nThe largest increase in sales from year to year occurred from 2017 to 2018. This is primarily due to the incomplete sales data in 2017, which makes the increase appear more significant in 2018.\nIn the year 2018-2019, all products experienced a decrease in sales except for the “Games and Toys” category.\nReturn customers (those who shop more than once a year) in all stores are very few, accounting for less than 2%.\nFor the online store, the delivery performance in each year is, on average, above 90%, with the products reaching customers in less than 5 days.\nPurchases from the online store have been increasing from year to year, except for 2020 (due to incomplete data). As online store sales continue to rise, reducing offline stores should be done gradually. However, for stores in the United States, considering the largest customer base from the company originates there, adding more stores might be a viable strategy.\nTo increase return customers, sales of short-term consumable products should be increased by lowering prices or offering discounts on those products. It can be observed that the profit margin for these products is around +-50%, and this profit can be reduced to lower the product prices.\nThe performance of the Online Store’s product delivery from year to year is quite stagnant, with 0-20% of items reaching customers on the first day, 60-80% on the third day, and &gt;90% on the fifth day. To improve this, enhancing courier services to ensure that over 90% of deliveries reach customers by the third day is crucial. Faster delivery to consumers can indeed lead to better sales by encouraging repeat/return customers.\nIn January, February, and December, sales are quite high. Create bundling purchase promotions to encourage customers to buy more items.\nIn March and April, sales are at their lowest point. Create discount promotions to attract customers.\n\nIn the future, I will try to explained in detail how I build this dashboard (including the DAX codes) and how to publish it live on website."
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Vertikal Willis",
    "section": "",
    "text": "Evaluate your regression model with residuals plot\n\n\n\n\n\n\n\nregression\n\n\ndata-science\n\n\nmape\n\n\nrmse\n\n\nr-squared\n\n\nresidual_plot\n\n\n\n\nHow a residual plot can help you to better assess your model’s performance\n\n\n\n\n\n\nOct 14, 2023\n\n\nVertikal\n\n\n\n\n\n\n  \n\n\n\n\nMissing values handling\n\n\n\n\n\n\n\ndata-cleaning\n\n\ndata-science\n\n\nimputation\n\n\n\n\nThe best way to handle missing values is by understanding the data.\n\n\n\n\n\n\nOct 13, 2023\n\n\nVertikal\n\n\n\n\n\n\n  \n\n\n\n\nThe importance of types of errors\n\n\n\n\n\n\n\nclassification\n\n\nmachine-learning\n\n\ndata-science\n\n\nhyphothesis testing\n\n\nimbalanced\n\n\n\n\nHow it is important to determine the cost of misclassification error before starting binary classification project especially with imbalanced dataset.\n\n\n\n\n\n\nAug 31, 2023\n\n\nVertikal\n\n\n\n\n\n\nNo matching items"
  }
]